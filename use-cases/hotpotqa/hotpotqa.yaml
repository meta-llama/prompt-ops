model:
  name: "openrouter/meta-llama/llama-3.1-8b-instruct"
  temperature: 0.0
  max_tokens: 40960

dataset:
  adapter_class: "src/prompt_ops/datasets/hotpotqa/adapter.py"
  path: "hotpot_dev_distractor_v1.json"
  train_size: 0.07
  validation_size: 0.07
  test_size: 0.07
  adapter_params:
    passages_per_hop: 3
    max_hops: 2
    retriever_url: null # Set to a valid ColBERT URL if available
    # Explicitly define input and output field mappings
    input_field: "question"  # Primary input field is the question
    context_field: "context"  # Field containing context passages
    supporting_facts_field: "supporting_facts"  # Field containing supporting facts
    golden_output_field: "answer"  # Field to use as ground truth/reference output

system_prompt:
  # alternatively you can also add the system prompt in the a seperate text file
  text: |
    You are an expert at answering complex questions that require multi-hop reasoning. Give a short factoid answer.
  inputs: ["question", "context"]
  outputs: ["answer"]

metric:
  class: "src/prompt_ops/datasets/hotpotqa/metric.py"
  output_field: "answer"
  passage_weight: 0.5

optimization:
  strategy: "llama"

# Output configuration
output:
  prefix: "hotpotqa_file_path_test"
