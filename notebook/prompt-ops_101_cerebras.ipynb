{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "All rights reserved.\n",
    "\n",
    "This source code is licensed under the terms described in the LICENSE file in\n",
    "the root directory of this source tree.\n",
    "\n",
    "<a aria-label=\"Meta home\" href=\"https://www.llama.com/docs\" tabindex=\"0\" target=\"_blank\" >![Meta---Logo@1x.jpg](data:image/jpeg;base64,/9j/4QAYRXhpZgAASUkqAAgAAAAAAAAAAAAAAP/sABFEdWNreQABAAQAAABkAAD/4QMxaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wLwA8P3hwYWNrZXQgYmVnaW49Iu+7vyIgaWQ9Ilc1TTBNcENlaGlIenJlU3pOVGN6a2M5ZCI/PiA8eDp4bXBtZXRhIHhtbG5zOng9ImFkb2JlOm5zOm1ldGEvIiB4OnhtcHRrPSJBZG9iZSBYTVAgQ29yZSA5LjAtYzAwMCA3OS5kYTRhN2U1ZWYsIDIwMjIvMTEvMjItMTM6NTA6MDcgICAgICAgICI+IDxyZGY6UkRGIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+IDxyZGY6RGVzY3JpcHRpb24gcmRmOmFib3V0PSIiIHhtbG5zOnhtcD0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wLyIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bXA6Q3JlYXRvclRvb2w9IkFkb2JlIFBob3Rvc2hvcCAyNC4xIChNYWNpbnRvc2gpIiB4bXBNTTpJbnN0YW5jZUlEPSJ4bXAuaWlkOjlDN0Y5QzBDNEIxRDExRUU5MjgwQUNGNjU1QzlDQjREIiB4bXBNTTpEb2N1bWVudElEPSJ4bXAuZGlkOjlDN0Y5QzBENEIxRDExRUU5MjgwQUNGNjU1QzlDQjREIj4gPHhtcE1NOkRlcml2ZWRGcm9tIHN0UmVmOmluc3RhbmNlSUQ9InhtcC5paWQ6OUM3RjlDMEE0QjFEMTFFRTkyODBBQ0Y2NTVDOUNCNEQiIHN0UmVmOmRvY3VtZW50SUQ9InhtcC5kaWQ6OUM3RjlDMEI0QjFEMTFFRTkyODBBQ0Y2NTVDOUNCNEQiLz4gPC9yZGY6RGVzY3JpcHRpb24+IDwvcmRmOlJERj4gPC94OnhtcG1ldGE+IDw/eHBhY2tldCBlbmQ9InIiPz7/7gAOQWRvYmUAZMAAAAAB/9sAhAABAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAgICAgICAgICAgIDAwMDAwMDAwMDAQEBAQEBAQIBAQICAgECAgMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwP/wAARCAA1APADAREAAhEBAxEB/8QAwQAAAgIDAQEBAAAAAAAAAAAACQoACwYHCAUDBAEAAQQDAQEBAAAAAAAAAAAABgAFCAkBAwQCBwoQAAAGAQEGBAMDCAYGCwAAAAECAwQFBgcIABESExQJIRUWFyIYCjEjJEFhMyW3eBkaUTK0djg5lLU2d9dYcYGhQkQ1JrY3RygRAAIBAgMEBAsGBAcAAwAAAAECAxEEABIFIRMGBzFBFAhRYXGBkbEiMnI0FaHB0UJSM/DhIxbxYqIkFxgJU3NU/9oADAMBAAIRAxEAPwB/jZYWNCaj9TWF9J2NZHK2cbi0qVXZqdGwR5aj6ds00oiqs0rtWhGwGezU09KiYSpkAE0kymVXOkgRRUhzy95ccYc0eIo+GOC7R7rUnGZjULHDGCA0s0h9mONaipO1iQiKzsqkU4y424a4B0V9e4ouVt7FTRR7zyPQkRxINruadA2AVZiqgsFTtS31DeerpPqIaZKohhmqslTJM5G1I1S8WSdQAxhK8lYuSrT+Jg3CoDu6ds5dETAP0xx3jtZ9y67g3A2j2IfmPdNrGqOKssBntoYz+lHSZXkA/U6IT+gdGIGca977ivUrsrwTANNsFNA0oinkcfqZWjZEJ/SrMB+o4zvSr9RJfa7JtYLVpRXOQYB84STd3+iBXIWwwCZlClM4JSmkFCRE42KQwioQHzZYALvIJx+AWTmf3AtD1C2a95WXq2F8ikra3O9kilNOjtDSSSRnwHduu3bTpDrwH3wdVs51teP7Vru0cis8G7SSPx7kIiOPCM6nwV6MNP4ZzXizUJjyCyphu6RF7oliTOaOnIhRTcRwgIFdxsmxcpt5GGmY9QeBwzdpIuUDeByF3htWTxfwdxNwFr8/DHF1nLY63bkZ45ANoPuujAlJI2G1JEZkYdBOJ2cN8TaFxfo8WvcOXMd1pUw9l0r0jpVlIDI69DI4DKekDGstVOrzC2j6heuMuTyiK7/qW9TpsMRJ9cLrJNkyHVYwEYos3TBFuChBcPHKiDJqBygoqU6iZDmXKLkvx1zq4h+gcGW4aOPKbi5lJS2tUY0DzSAE1NDkjRXlehyoQrFQ3mpze4L5P6D9c4unIkkqILeMBri5cCpWJCQKCozyOVjSozMCyhlocw98zVDbLctI4haQ2JqemsJWldeR9XvL5w1THhIq+l5qppqpOnBA4lCpBwEMYQKIgACNpnBXcC5TaPoy23Gjz6zrRX2plee1QMekJHFcEFVOwFtpAqaE0xWjxh35eaGraubjhBIdJ0cN7MLJBdMVHQWkkgBDHpIXYCaCo24710f98ah3V9D0DVDCHx3MvFE2TXLDN02fUx47VMQiQ2uNZxUWvUUTqGEvVJEdMybwMuLdMplAjzzp7g3EOhW8/EfKecalYoCzaeyslyqipPZ3aSQXBA27tjHIeiPeMQuPvXJ/vxaDrc8PD/NCA6deuQq36srWzMaU36LGhtwTszqHjHS+7UFsMAtXTZ82bvWThB4zeIIumjtqsm4bOmzhMqqDhuukY6S6C6RwMQ5REpiiAgIgO1cssUtvK0E6sk6MVZWBDKwNCrA7QQdhB2g7Dif8UsU8SzQsrwuoZWUgqykVBBGwgjaCNhG0Y++2vGzE2WFhVLN31UmDsJZny5hmU0m5Ym5LEmTr5jKQmWV+p7ZnLvaHaZWrOpRo2WjlFm7WQXijKppnMY5CHABHeA7OqaU7oHzjaAejw4ZZNZjjkaMo1VJHSOrBpu2z3F8Rdy/AC2b8XRMpTn8DbJalXzHFifsJCx0ueYgk9jercx4JoP4uwwDxu8aOiJkTOJ1UP0rdYC8VzbPbSZG2ilQfDhwtLuO7i3ibCDQjwYIPtz46sTZYWNN6hs7490xYQyhqAytKeUY/xNTpe42NynyjPHKEaj+DholFZVFN5PWGTUQYR7fjKLl85SSAd5w29xxtK4jT3ica5ZEhjMr+6orhWYfq88Abh3aOcwiPjuAci0oAH+jeIRQ7t/5ft3fn2dPpEn6x6Dhm+uxf/G3pGGwcWXpvlHGOOcmNI1zDNci0OoXptDvVkHLyKb26vx9gRjXbhqItl3LFOQBJQ6Y8BjEES+Ahs1MuVivgNMPaNnQP0VAPpxnm3nHrE2WFibLCxNlhY8iwT0TVoGbs888LHwVciJKemn501liMYmIZLSEi8Mi2TWcKlbM25ziVMhzmAu4oCO4NsgEmg6TjBIUFj0DAxcQd7DtkZ6ybRsO4o1PRlsyRkifZ1im1pPHOXotWXnX4HFow6+boEbFMjLCmIAdwukmBtwCYN+3S9lcxqXdaKOnaPxxxx6jZyuI0erk7Nh/DBUduXHbibLCxNlhYmywsTZYWJssLHiWWyQVNrlgt9olGkHWarCStjsU0/U5TGIgoNivJy0o9V3Dy2jBg1UVUNuHcQgjt2adp97q+oQaVpkTzajdTJFFGoq0kkjBERR1szEKB4Tjmvb2106zm1C+kWKygiaSR22KiIpZ2J6gqgk+IYrue4drdu2vDUNM358pJs8dwLp7WcL0RQ6gpVun9WUiDxZgkdREbbbzoJPJVUvMOZYU2xTmbtW5SX7cg+TWjckeAodChEb6/OqzahcilZZ8u1QxodxBUxwqaALmkKiSSQmn7m/zN1PmpxfJq0pddHiZo7ODqjhrsJUVG9loHlO0k0QEoiAG30QfT5Vuw49hciazrFdYiz2eOSkmOG6U7Y19zUWTxMirMl4sLxhKvHFkMgcDLx7RJsVgp92osspxkThvzm7+Wo6fr03D/ACgt7OXTbaQo1/cK0onZTRuzRKyKIqiiyuXMo9pURaM0muWPdGsrzSItY5kTXMd9OgZbOErGYgdo38hVyZKe9GoURnYzMagas1+9g59iSlzWXtINgtmRYSttXMracRWwrOTvDaGap853KUeYh2EcnaTMEimUUi1Wib4yJBFBV0sJUBJ+RXfmh4q1iHhTmxBa6fe3DBIb6DMlsZGNFS5jkZzDmNAJlcxhiM6xpVwxc2e6hLw/psvEPLya4vLWFS0tpLRpwgFS0Doq73KKkxFQ9B7DO1FwMft1dwTI2gnKnn8aWRteIbWok2yji8r3kt5xsmmZJpYoIXHG1jLjBiYDIL8IA5Q42yo8BynTkj3gOQ/D3PHhjsNyY7Xiu1qbO8y1aIk1aKQCjPBJ+ZK1VqSJ7QIb4hyd5t6zyp17tUGe44fuNlza5qLJsosiE7ElQ0o9KFao2wgr17Qa3qA7w+r99MTMspHQzoiUrP2BNNw/qWHMTt3igRUDX2ih0EnDw4LHRYteJJaTklFnLgxQ6twm365rfLXuYck4rbTIlnuKFbeOoSfU75lGeaZgCQuwNLJRlghVIYwSIY2CtL0LmP3tucs0mrO1vGrVuHoWh02zRiFhiUkAttKxJUGeVmmcgGWRWjMYdtTRRi6ltqY0wHQrkBWhW8nZ8jQMbdrbNr7gFd88mZlqudkquoHECTEjRskPgkkQA3bVP8Wd6Tntxbrr65NxFqNj7dY4LKV7W3iHUixRMAwA2ZpTI7fnZjizvhfu1clOF9FXRYtAsL32KPNeRJc3Ep62aSRTlJO3LEI0X8qqMBO7o/agrGHKhKajNMkY/ZUmEOLrJ2MRdO5YlXjnK4F9YVFw8O4kvTzJZUpZBkqosLJI3UJGK2IqRGd3dM74OrcbazDyy5qyxya9OMtjfZVjM7qPlrgKFTfMATDKqrvWG7cGVkLwn70fdQ0vg7SJeY3LKKRNEgOa9sszSCBCfmLcsS+6UkCWNi27U7xSIlYJtPsha45OWWU0cZNmln52ca+msGSsk4FV0mwi0TvbDjbnKGMqs3j2CaklFEHf07ZF2hxAkRqkQR7+nIK0s0HO3hSBY1eVItVjQUUvIQsN7QbAzuVhuD+d2hkpnaV2Ku5Dzxurtzyc4nmMjJG0mmSOasFQFpbOp2kIgM0A/IiypXKsSBkrar3FkmJssLFP5r4SUW14azkUUzqrK6s9QySSSRDKKKqKZetxSJpkKAmOc5hAAAAEREdi+D9hPgHqwC3XzUnxt68EJ7EHcEd9vrXFEwuRZNzAYKz05jsQ5uZSxlWLOpSgSayFGyJJtnAogzcY/sz1VB8osG9tDSMiPAKgEAOe+t+0QVXbIu0fePP66Y6tNuuy3NH2RtsPi8B83qriz62GMGGJssLCNv1UfcR9Q2ipduzGU4Iw9NWhcnajXEe4HgfWx4yK/wAaY3eGSMQToV6GfBPv0D81FVy+jDBwrMjAD5pdvQG4bpOwfefu9OBzWrqrC1ToG1vL1D7/AEYTgfR7+Lcizk2LyOdlSbODNXzZZo5BB62Res1hQcETVBJ2zcJrJG3blEjlMXeUwCLxWvRhhII6cXGGkz/Ctpn/AHfsNfs5rewfN+63xH14PIP2U+EerHQO2vG3Gj8mam9N+FnfQZh1AYUxVICRNQI/I2U6PSX5k1SlOkcjKyTka6OVQhwEogQd4CAh4be1ikf3FY+QE41vNFGaSMqnxkDHv41zhhbM7Vd9h/L2MMrMmpCqOneN79VLw2bEOJSlM4WrEtKJoFMYwAHGIeI7tsMjp74I8opjKSRybY2Vh4iD6sbR284940Rn+zVr2SzawGxQJHoYryQ1M1UmY1Ncjn0hMpigomo5KZNUqngIG3CA/btsjB3i+UY1ykbtto6D6sVdnZpWQbd0jRC4croNm6GdK8qs4crJN0Ek02siY51FljkTIUCh+UfEfD7die9+Vf4cBth85HX9WLWYblTygJjWutgUAEREZ2LAAAPERERdbgAA2FaHwYNcy+EYyFNRNVMiqRyKpKkKomomYp01EzlAxDkOURKchyiAgIDuENsYzj8UtLxUDGvJick4+GiI5AzmQlZZ62jo1i2Ju43Dx88URbNkCb/E5zFKH9O2QCTQdOMEgCp6Mc2sNcGi6VsAVOM1daY5G0GVK3LXmOecWO5o7gxgIDZONQtSjxRwJx3cspBPv/Jts3EwFSjU8hxqFxbk5RImb4h+OOlVZKOQYeaLv2SMZyU3PmKrpBNh06oFFJx1Z1Ab8lUDlEp+LhNvDcPjtqoejrxuqKV6sfiZ2SuyLgrSPnoV86OBjEbM5Ri6cHApTHMJUUFzqGApCiI7g8AAR2zQ4xUHYDgLfftz4+xXoySxhAvTM5/UJb2tMdnROKbktEryRbLcTInKIG4HrlCNjly7hA7WQVKPgO01O4rwBDxbzfbiS+TPYaBZtcCu0dplO5twfGoM0qnqeJT1Yit3u+Nn4X5aJolq+W91m5EJpsO4jG9mI8pEUbeFZGGAK9jjSbH5/wBY7O9W2NTkaHp2iW+S3rR0kVZlIXlV8DDG8c6IYogPSyqbiYIA/Cc8PwG3lMIDODvr8y7jl/ykbRdLkMet8QSmzVgaMtsFzXbqfGhSA9YFxUbRURU7qvBcHGvMZdUv0D6Vo0YuWB2q05bLbKfI4aYdRMNDsNMPUbUlYtVxNlhYr3e6OTA77WPl6w6b40jHHTuwqNJ5Rgo2NW3uSUTrEuc1TkGqZUmlSmpkqhm4FOoiq5Kss3ErZZBJO/zu66XzC0rkxoicyJN5rbW4MYYMJorVgDaxXJY1adYqZqhWUZY5Kyq7NTTze4k4D1zm1rNrwImTT4ptrKQYZ5lqLqS3A2CIS1oASrCssdI2VQSX6f3WRWsbZEtOky6toSJbZllC2bHNuFq3aSTi+xcaKDmjTUqbhO7j5yIbGVhklDlK3kiLIpFOrIlAsZO/byn1TiPh605naTJPMdGiMNzb5maNLaR83aYo+hWSRqXBAq8RR2IW3NZEd0rj/TdD1u64H1COCJ9VdZIZwoWR540yiCWTpZWjH9AE+zIHVatNhv3ap7Fh2PMmoaKscNLV6dj2stBz0Y/hpmKfJFXZScVKNVWMjHvEDgJFmrxoudNQg+BiGEB26rG+vNMvYdS0+R4b+3lSWKRDRkkjYMjqR0MrAMD1EA45r2ytNRs5tPv41lsZ4mjkRhVXjdSrow61ZSQR1g4QiyZCWbQprasEdWl3JZXAeY0JiqLrKnSWlK4wkm1grAPzgG86VhqTtuR0XcYh03ByjxFHx/Q9wtqOld4HkRbXOqKps+ItEMdwAARHM6NDPk8BhuFcxnYQUU7CMUJ8S6fqfIvnZcW+mswutA1kSQEkgvCrrLDm8UsDIHHQQ7DaDh8+o2eLu1UrFyg1RWhbbXoWzw6xgADKxc9GtpWPVMACIAKjR2QR3CIeO356dZ0q70LWLvRL8Zb6zuZYJB4JIXaNx5mU4vl0jU7XWtKtdZsTmsru3jmjPhSVA6HzqwxkOzbhwxT+69znT146zVEznTUJq01CnIoQwkOQ5cv24xTkOUQMU5TBvAQ8QHYvg/YT4B6sAt181J8bes4NN9SNoBd4IzpRtaNHhio4r1axkW6vPl7MjeNq+oRrXWz6zJKFRIVJsTKES2PPIcRjKOJJGXMPCQhA24tOuN4hhb3k6PJ/Lo9GHDVrXdyC4X3H6fi/n0+nDLf07vcPHWnoxYYvv86Mjn3SuhB44uSj5wZWVtuPDNV0cWX5U6xjrvXK8PGKw8ksc6q6sjFHdLCUXiYC2ahb7mbMv7b7R5esYdtKuu0W+Rj/AFU2HxjqP3ebBONf2sak6DNJ2W9TF06V4ekwRmtJrDhcUVLxkmdEYyi09uCZyujJSs6smZ6oiB1GcYi5dCUSIH3c1vC08oiXr6fEOvHZdTrbQNM3UNnjPUMVxfbN0mZH7unccbkyu+lbPXZm3zeoLVXdlTqIKOqp6iTlrDFpu0TJFYSmQrDJowrFNAQOzTeHcJJiizUApHcyraW3sbDSij+PB04FLSB7679vaCczHxfz6Mao7xBSp90DW42SImi2YZ3tMYxbIJJoN2cbFkZx0awaoJFIkg0YMGqaKSZQApEyFKAAAberP5VPhx4v/nJPiOLRPSZ/hW0z/u/Ya/ZzW9hib91viPrwYwfsp8I9WFMe/t33sjY+yNbdDeii5OKVJ0xRWB1AZ0rboE7W2tXCQX+LcbTDc4nrStaA3JnZZAxZIslxsW5motHB3LtYWKsonnFa9A+8/dhk1PUnVzbW5oR7xHTXwDweM4BhpN7HXcm19U1HPFXp8RW6PdTnloTJefbq9rS+QSOBMc9gh2gx1mu05FvB3GSlFWJWbwDcSK6oAYQ7pb62tzuyfaHUB0fdhtg067uV3qiinrY9P34wHU925u5F2j7TT8yW2MsOOG7eZatqdqFwZd3j+tMLIYy7ltCL2qCNFzdalHqTA502ko1ZlfpEOCQLFIqUnqK4trsFBQ+IjHma0u7EiRqjwMD9+HS+wj3eZXuLYqsuJs5uYpLVVhCKjn1hko5u3jW2XMdOXCUUyyS1h24Ebxs/FSqiLGwoNyEZFdOmjlAqRHvStWW/tBbuHT9pvsPg/DBBpl8btCkn7y/aPD+P88LTa2ewX3NrjqP1b55gML1I+MrRmnPGW4aXWzLi5ByvRpm7Wq4sJJSLcWhOTbrrwLkqotlEirJmHlmKAhu2coL+2EaRljmCgdB6aYaLjTLxpnkCjIWY9I6Kk+HABsE4SyJqQy/j/BeJoppOZIydYW1Xp8S+lo6DaP5l0mqqi3Xl5dy0jWBDEQMPMWUIQN27fvENnB3WNC7+6BhsijeaQRptcnZgxQ/TXd3IAEfYWljuD7Aznh7eP5g33EA3jtx/UrT9R9B/DHf9Jvv0D0j8cP6qZZqegPt7U7JOoxYlVidN2mvGcff46PeMpZ0e01ekVuqkpdddJuE4+am5+4FSiY0SqlQdO3CX3hUzCcGDIbi4Kx7SzGnp6fRgmzrbWoeXYEQV8oHR6dmK3HXB3HNafdgze2hptzcZOu2G0+VYX0tYwLOStbieseiSvxbKrxCQusgX1VPgBxLOmyrxwvxcgjVty2qRHBbQ2iVFK02sf42DAnc3dxeyUNaE7FH8bT48dT1D6ajuu2yoNbWvifHtRcvWZXren2/LVSj7eBFCcxJB0yj15WKjXihd29Fy8RUSEeFQCGAQDUdStA1Kk+OmzG9dIvWXNlA8RIrjRl5z13D+35hfUL20tVlQv8di7M1GjoyLxrk+Rdu46iyEHbIGyQGQsIWtstMwr2sjJVwWr1nEu1oR/wAagG5btLmE2LHb3DrcxEZlPSOvxH+K41NLdWsb2k4ORh0Hq21qD+GzG7fpqyFN3Z8GHEB4k6pmnhHeYADjwzfQNvKA8I7wD8oDu216l8o3lHrGNukfOr5D6jg4ff8AckvrhqSpOMxVMZjiivPToIcQimVe9w9JnHCok/qgocrUgb92/cUNraP/AD+4Ti0vlpe8UKv9bVrhQT4rWS5iA8gzH04rd77PFTX3MC04cZv6WmwMQPHcR28hPlIA9GCHfTz44b13TJl7IhkkiyV9zEaDMqUoc08PRarDHjyKH3bxAkna34gH2Bxfn2jn/wCh2vSXfM/SOGwT2ew0YS06t5dTyBiPKkEWPvHcc0lIeXep6+ab681Ux168lvDHl/1zSYYA2r+xNjAe+8BraHTXhUMTUOX6XM2bI2QjWbhmvwP6Zjw3Mj7JbCnSEVmclKiY8bFqfdmBUzhwkcFGW4Zq9yzkOOaPHX948Qw5+B9BlR2DCqXN5seC327GSPZPONoyiKN1yz1EPu+BztPLXgr+09BmycZ63E6KVNHtrTak0+zarybYYDsOYySI2aGmAydqLt31vV85yneszRDxxhiBrs1j+HKkdRqrNZGs0OZIJKLdEHcC+OIp8nIFEwbiyLpiYOMpFibTa75XeMv+UdhpnCnBsyDjO8njupagMIrKCUHK6n/9kqGLZt3Mc49ksjYh93P+Q9tzK1W+4w4ojf8AtWwikt4aVXe3k0ZUlSOkWsTiQg7N7JAfaCuuA5Z6w1k7RrqMteL5948hb9iK5NXletMSK8ed8kxctpylXyuLgcV2yEqxFrINTAbmtzHAh+FVM5Q+58EcXcOc3eX9rxLYok2h6raFZYXo+UsDHcW0o6CUbPE4plYCoqrAlm4q4c1vlzxhPol0zRarp9wDHKtVqFIeGeM9IDLlkXbVSaGjAjD3vbn1kw+trTPUsmmWYt8iwZU6fl+vteBEYm+xLVDrJFuyLuFvCWxoonJsQDjTTScGb8ZlW6u6krvAco7vk3zFuuHArtw/NWexlapz2zk5ULdckDAwydBJUSZQsi1tI5PcxrbmZwXBrdUGsRf0buMbMk6AVYDqSUUkTpADFKlkand+3xLH1PCcnfVp6Vd1rs7AggCQX3D1IsDtUAAOpfxchZKec5t3iYU4+ttSbx/IUA/Jtdl/5/60+pcin06Rq/TtauoVH6UkSG5A87zufPinfvz6Qmn86k1BFp2/R7aVj4XR5revmSFB5sMYdsu0r3DQdpnlnKhlVmmPgrHEcwmMCVJnJimtiCIiI/A1gSAH5gDas3vUaRHoveE4qs4gAj6lv/PdRR3LelpTixTuz6pJrHIjhm7lJLpp+581tLJbr/piGO69o/4+6Yp+9fX+O/Wf+9lqH/a9bti+D9hPgHqwC3XzMnxt6zi0Z1caP6Nrt0N2rTPeitmqd7xpBKVCyLN+etSMiw0Szk6Lc2nAUXAeSWBBEXSaRiHeR53DUxgTXOAi8UzQT71eo+kdYwYzwLc2xhbrGzxHqOK4rt/an8q9oPuNMZnI8RMQKWP7pP4L1P0EnGuvIURWcTh7qg3RQMCcu8rEjGt56HOkcEXrqOb8Kgt1jCYjuIkvLai9Yqp8fV+BwKWsz2N3V6ihow8XX6OkYIT9St3IorVhqMrGmfDtujrNgDTq3bTDyfrMuzmKxkbMFshG7uRsUfIxjlwwlYqj1mRTh2KgDxJPVpXhMZNYg7c+m2xijMrikjfYP5/hjq1e7E8ohjNYk8HQSfw6PThpDsF9u8ug/RTBTN4gxjtQeo4kPlLLfWN+TLVmKWYqGx1jJwByprIDTq/IKOXqCheYjNyj9MTGTIlwtd/cb+ai/trsH3nz+rDzplr2a3BYf1X2n7h5vWThDPvGf5o2uf8AeFu/9pS2fbP5WP4Rgav/AJyT4ziy0p2ST4a7blUy8mmksrivRFA5GSRXDeiurScENLKkiqXeXiTWUjAKIbw3gOw2y57kp4Xp6TguV93aCT9MdfQMVZWmSVw7fNX+K7RrLuj9jhqey80u+oG2OIydsclOQXmy9qtrV0xrTKQn3bq8O0jsFVWyCiqRnwrbtxBECiUOsJEI9ulB/HiwGQmN51Nwf6ZarH7T6cWHsd9RZ2dIiPYxMTqJkIyLjGbaOjY2OwFnBlHx0eyRI2ZsWLNtjZJu0ZtG6RU0kkylImQoFKAAABsPHTrwmpXb5R+OCkarYAUD7Phb8MaG1Zd7vssaqtNWbtPN01CP5WEyvjmzVUib7A+cVSx065j1V6pYWgr47IkhLVe0N2ciyWES8l21TPvDh22RWV7FKsirtB8I/HGqfUdPnhaJm2MP0nzdXUcKK9hfMU5hrur6UnkS8VQj8i22Tw5aGZDmIhMQeSoGSgWzN2UBLzEWVnPHSCZR8OoZJjuHdu2d79A9q9eoV9GGPTZDHepToJp6cWb2oD/4Gzb/ALo8k/8As2Z2GY/3F+IevBhL+23wn1Yq1uzJ/mm6HP8AfxWv7PIbFF78q/w4DdP+dj+LFsLsKYNcKR/VvZnm6vpk0xYMi3qrSMy5lu13SzJIH4BkY/ElcjG8ZGPADxUYnmcipO+AfAXDFI32kDZ20lAZWc9IFPT/AIYY9ckKwpGOhmJPm/xxzR9JRpRpc251F6zLLEspe302YisHYsdu0E1z1I8nAls2SpiPBYpwbS8zES8RHpOkuBZJkd6hxCm7VKO3VpWGWEdB2n7satDgU57g+8Ng8XWfu+3DuezJghwAv6kvBGM8pdrzLuSrdX27q96fpah3fFtoRSQJLwElZciU2hWaNB6KYuT1+xVyxKleMwOCKzls0XMUVGqIl79Ndlugo91qg+gnDZq0aPZs7D2loR6QDhSX6ar/ADZMH/3UzP8Asav2ztqXyjeUesYZNI+dXyH1HBk++HSZeI1pzdseoKJxV2rtXPCrHIIJuArtNqcTIckwhuMCTr4TbvsHa5buG6zZX/I6DSIGBu7G5nEo6131zcSJXyrtGKpu+tpl5Yc45tTmUi1vLeExnqO6t4EenkbYcF+7ClrhJTR9a6i0dJDO07MllUmWG8oOEWdkgq2/hpA6YCJumfi1cpJmHdxHaKAH9XaGf/oRot9Y857PWJkP0++0SERP1FoZZklSv6kzIxHUJFPXiWfcV1myv+Ul1pcTjt9nrE28TrCzRQtG9P0tR1B6yjDqwVvPec8facMU27MGTJUkZWapHqOOSQyYyU9LKFMSIrUE2UOTrZyde8KDdPeBAEwqKGIiRRQkRuXfAHEnM/i+y4L4VhMuq3koWprkijG2SeVgDliiWrudpIGVQzsqmUfHvHPD/LjhS74w4mlEWmWkZNBTPLIdkcMQJGaWVqKg2DbmYqiswRpu9tzT3DdWQyBWgymSM0W9pB1evJOHCkNU4MgCjFQ7dYUjGZVimwDcyztzygHlIOHiwCodUxr+NB0bgXu18nezF9zwvoVk0s8xAEtxKdskhFfanuZmCxpm95o4UIVUAo11vVuNe8NzZ7QE3vEmtXixQRAkxwRDZHGDT2YbeIFpHp7qyTOCxYl4jTfgeo6Z8KUDCtLIB4mlQqTR1JnRIg7sM86Od9YrK/IUx+F5OzLhZwYnEYqJTlSIIJpkAKD+Z/MLWeafHeo8da6aXl/OWWOtVhhUBIYEOz2YolVAaAsQXb2mJN4fLfgPSOWfBOn8FaKK2llAFZ6UaaViWmmcbfalkLORUhQQo9lQMB+76mhc+dcOtNTWO4UXeU8FRDklvaMUBO/tmHSrLSMn8JQEXDzHbxdeURDeX9XryH9c4IE2lP3JudK8FcXty44gmycM63KNwzGiwX9AieRbpQsLdP8AVWD3VznHwfvT8sH4n4aHG+jRZtd0qM74KPals6lm8ptyWlHR/TM3Scgwvd2wNbkjoh1Ex1kmHDxbDmQisajmGFbAsvwQguTmibkyZJcfPmqO9cncpgUh1VmSrtsTcZwByz/7yfJODnRy+k06zVF4vsM09hIaD+pT27dmPRHcqAhqQFkWKRqiOhhlyQ5tS8reNEvbpmPDV5lhvEFTRK+zMFHS8DEsNhLIZEFC9Q/dCTcPZYaJsVelGE3AT0YxmYSZi3SL6MlomTapPY6SjnrY6jd2xfM1yKpKkMYihDAYBEB2okvbK7028l07UIpIL+CRo5I3Uq8ciMVdHU0KsrAqykAggg4t1tLu2v7WO+spEls5o1eN0IZXRwGVlYVBVlIII2EGowo137LZETWrukV2PXTXfUzCVcj50E1CHFnIzFpuE+2YrlKYTpLhDyDZxwmABFNyQweA7XK/+eekXlhyZv8AUrlStvfa9M8VQRmSOC2hZx4RvEdKj8yMOrFSPfy1S1vubllp9uwaey0SFJaEey8k9xKFPgO7dHoepwevB7O1LCO4Ht/acmr0h013les02UhwEB6Sfv1rmY5Qu8AHgXjnySgfmNtXp3vb+HUe8ZxNNAQY0uYIqj9UNpbxOPM6MPNiePdUsZrDkDw5FOCHe3mkof0y3U8iHzoynz4IbtGzEhcU/evr/HfrP/ey1D/tet2xfB+wnwD1YBbr5mT429Zxbs0T/Yem/wB1K7/qhnsJN7x8uDhfdHkwn59SF2gcsZuynQtZGkPEdmyddbui0x7n2iY/hlJewO5KBjeCh5STimZTu3pFoBiaEllg3EblYRhgKIqrqA76beIiGGYgKNoJ+0ff6cMWrWLyOLiBSzHYwH2H7j5sDt7MnY61K3LWvR73rM07ZDxNgzBwtspvmOTqu6gWmTLpByDY1EorJrIFDzSP8+AknLEMkq1Ujo5Rotwi8T39F5fRCArCwLts2dQ6zjl0/TpmuA1whWNdu0dJ6h95/nixA2HsFOKmzvGf5o2uf94W7/2lLYrs/lY/hGAm/wDnJPjOLJVDHkll3tbNsVQqIuJrJWgdrQ4ZAo7jKy9t09pwMYmUd4eJnz9MNhzMEus56BJX7cFmQyWWQdJip6VxVoaT8eYhyNqgwvirUdabNjXEt2yRC0TIVwgDxUdPUptPvBgkJpZayMJCLjWULOOm6kio5bqAgyTXNw8RQ2KJWdYmeMAuBUePAbAkbzKkpIQmhPgw7p/KP6KP+ZHVL/peJv8AhtsyfVp/0p9v44Ivodv+t/s/DE/lH9FH/Mjql/0vE3/DbZfVp/0p9v44X0O3/W/2fhjdOnH6Y/SVpoz5h3UHUc+6jZuz4YyLVckQUNYHONDQcrJ1OWby7OPlgjaEwfjHO1mwEWBFZNQUxECmAfEPEmpyyxmMqtGFOv8AHGyLR4IZVlVnqpB6urzYPrn4pj4JzWQhRMc+JMjlKUobzGManTIFKUA8RERHw24I/wBxfKPXhzl/bb4T6sVZ3Zrct2ndK0NKuVk0Ez5/qLYp1DAUpnD3q2bREBH7VHDpciZA/KYwB+XYovPlX+HAZYbL2P4hi2M2FMG2E9Pq8sbTEphXRxlxo1WVhKXkzJ2P5p0QhzpNn2RaxW5+AKsYoCVIFk8avwAR3AJgAPt3bPGkMA7p1kA+j/HDFrqExxv1Aken/DHlfSKZvqrjFurHTcu/bNrvEX+tZvi4xVUpXk1VbHXY6hzr9gjvE6rasS9Wjk3ZtwAmaXbB48fgtXQ50k/LSn34xoci5Hh/NWvm6P48uHINmfD9gIn1FF1qdS7SOpiNss8xh5C+usUUylsnSnC6stqNlql2jyOKSDeZw9TrdYkX5wDwI1ZLKD4EHbt05SbtSOqpPoOG7VWVbFwTtNAPLUH7sJ2fTVf5smD/AO6mZ/2NX7Z41L5RvKPWMMWkfOr5D6jh1bvB6M7DqhwTDXPG0OpNZVwk9lZ6LgmLcV5a3U2abNUrbXYpFIAVeTaB4tm/Zo/GdbpFW6JDLOCAMqe5Xzv03lPzBn0PiiYQcIa9HHFJKzUjt7mJmNvNITsWI7ySKRtgXeJI7BI2OI6d8Dk5qPM/gSHWeGoTPxVojvKkSislxbyBRPDGBtaQZI5Y12lt28aAvIowqbp61O510lXWQtmGbc9p0y9b+T2WIeMW0lCTrVqsoJGFirssguydLR7gxxRUMQjpqc5+Uonxn4re+ZPKjl/zj0KPRuOLKO9sY23kEiuySxMwFXhmjIZQ4pmUExyALnVsq0ql5e8z+O+U2tyatwbePZ3rru5o2VXjlVSfZmhkBVihrlNA6EtlZatXI9Q2rvUprGn4BPLdzk7iZi7TaVKlQMW3i4BnJyBisyDEVaCbJIvZyQOoCQLqEcPVAMCQH4OEgNnLbkxyu5JadctwbYxWQkQtcXUshkmaNPaO8nlYlYkAzZAUiFM5WtThx5h83eZXOO/t14uvZbwxuFt7aJAkSu/sjdwRABpXrlzENIa5Q1KDDMfaW7dTvTDV1835jiE0c632IKzi4F0Qiq+Lqa8FJypFK+JiI3CwmTTPImAROzQIRoUSGF2ClWHfG7zEPNfVl4C4JmLcv9OmzSTLUC/uVqokHWbaGpEI6JHLTEECErZh3S+7rLyw0tuN+MYQvHV/DlSJtpsbdqExnqFxLQGY9MahYgQTKGNJtBjE0sfNVJJdJRFZNNZFZM6SqSpCqJKpKFEiiaiZwEp0zlEQEBAQEB29KzIwdCQ4NQRsII6CD1EYwyq6lWAKkUIPQR4DhJ3uxdsue0rZEl8yYhrTp7pqvMod8mnEtVHCeH7DIqmO5qUwmiU5mlScujiMK9MAJJkODFUQWSSUdXS91DvJafzT0CHg7i25VOZNlFlJcgG/iQUE8ZPvTquy4jFWJBnUZGdYqp+8lyLvuXesS8U8OQM/Ad3Jm9gE9ilY7YXp7sJP7Eh2AHdMcyqZOdNO3cx1j6ZKF7Z4xyiX0Q2Kp5FA2uvwtub1MzlVddwFXVm2blzFNVXLgyotOM7IFRMcEQMc4m+r8wO7FyZ5m66OJuKNLP1tqb2WCaW3M9AAN+ImUOwUBd5QSZaLnoFp8m4N7xHNfl9o50HhzUR9IWu7jmijnENSSdyZFJQEktkqY81TkqTXEcPY3znr11GtK83kJq7ZHyXYPOLveZgqr5GCiTLIEm7hZHCYJIMYSAYcJUkScog8KLNqTjOgkJtxbxZwD3fuWL6lLHBY8M6Xbbu1tY6IZZKExW0INS0sr1LMcx2vNK2VZHx884c4T44548x00+KSa94h1K43lzcyVYRR1AkuJiKBY4loAoyjYkMQqUTD92PKNA4xoVKxxV0BbVuh1Sv0+CRNwcwkTXIprEMOcKZCEOuZs0KKhgAOI4iP5dvzy8Sa/qHFXEN9xNqzZtT1C8muZTtoZJpGkelakDMxoK7BQYvd4e0Ox4Z0Gy4d0tcunWFrFbxDrCQosa1pTbRRU9ZqcZjsy4eMU/WvkQ+e7WgO8N3zZah/Hf4eGXrfv8fzbF8H7CfAPVgFuvmZPjb1nFu1RP8AYem/3Urv+qGewi3vHy4OF90eTGV7Yx6xNlhYmywsVNneLEB7o2ufcO//APQ14Dw/pB0kAh/1CGxXZ/Kx/CMBN/8AOSfGcWiOksQHStpnEB3gOn3DIgIeICA45re4QHYYm/db4j68GMH7KfCPVhFz6gfsyZDwBmDIWtTTrTZK16bcpTUleMnwtZjnD59gm+TbpaQtT2TjGRFlUMXWWVWUftJBMhGkS4cqMFit0iMjuXzT7xZEEMhpINg8Y/HA5qmntFIbiIViY1PiPX5vV0eDGv8AQr9TZqz0p41ruHsxY9reqak02NZQlQnbJaZSk5SiIJgQG7KGk7q3irSxtbGMZEKk1O9jRflIQCqO1CgUC+p9MhlYuhKMfOPRjzbaxPCgjkAdR0baH07a46Cz19WxqXuVYk4LT9ptxrhCafomboXi2WySzBMw4H4d72GhFq3SKySSS3CBBft5Nr47zIG+zbXHpMQNZGLDwdH442Sa5MwpEgU+Emv4Y6v+mhz33MsoZQzDMZXgr5lnSPlqWsd+tuccqzL5j6bzSLZMDOsWvZVqsa7pWnp0GEvDRxU42KTSQdFWaGRFpIatSjtlRQlBKNlB4PH4PL/A36RLeO7FwWgY1JPh8Xh8Y6vW5TIMGkowexj9AjljItHLB62UDem4aPETt3KCgflIqioYo/mHZm6NuH8iooejFSZrH01Zx7X+uGw0RUs5TbTiLJTPIuB8hJt1E0rFVIizDO4syRWnrhJRpIfDHodQUorFaSbZw0W+9QVKBbDKlzAG6QRQj1jAPPDJZ3BXaGU1B8XUcH2qv1dmoWOpkdGW/SJiSz3ttHJN39uichWyr1+SkE0gIaSGmKQdgdMyuFA4zoJy/CAiIEEhdwA3nSIy1VchfJ9+HNdclC0aNS3hqfV/PDVOoLTtW+6d23mGNspIx1UldQeEMbZJiJiITcSLLG+VZOrwl5rE/DA6OhIPomv2ZwVFZEVEVn8UddsZQnPMYGuOQ2tzmTaFYjyjow9SxC8tMj7C6g+Q9P8AHixWuScRra7QGsVE6pLFgzUJiWUcniJhJt11XulZeGWZHkIlV+1GCyHjK5MSHIPEmogsXiTVIk6RMRIkBgvIepoz9n4EYEiLiwn61lX0H8QcHxrX1depJjTm8datJWF7De0WSaC1rirrdK3XHT0iYEM+VpazSfepkVOHEZJOZIG8RApihuAOA6RHXY7ZfIPX/LDmNcmC0ZFLeGp9X88Ck1OZ37ifeFr+Z9VuXXLVLAGkeqqWB+zh2EpVcI44c2icgICNplKYnNNL2HJtvfS7PjO8dO5EzFHmOHKTVJAm3VFHb2ZWJP3HPnPjPixxTS3d+Gmf9pB5APEPGcbd+mrMUO7Lg0omADGqmaOEoiG827DN+37g+0d2/wAdvGpfKN5R6xjZpHzq+Q+o4sz9hrBdgC/ch/hWes1vfPqPdXqVPVny9e3fuD1/Efi9fdV+I803fb1X4nh4eLw3bWGd2D/t19DX+wMv9oZB2f6x2zseTZ8pl9nd/wD1+xWtNtcQM7yH/Vb603985v7qzHf/AEnsna8235rNtz/H7dKV6sZN20P4YfqFf5deP3X3H8k98/QHuvyOX+N9FdD+N4eV+n6X77l79/3fFs1d6f8A7XfTF/5Mp/Z+ze/Su2fT619ntWf2en3N57OalPaphz7tH/WH6i3/AB1X+69u7+p9l7dSntdmy+10e9k9qla+zXBwtoEYnBibLCxNlhY8ax+nvIJr1b5N6W8rfeovUfQ+QeS9Mp5n515n+rvK+j4+fz/uuXv4/h37dunfUfqEH0jffVN6u53Obe7yoybvJ7efNTLl9qtKbccl/wBh7FN9T3X07dtvd7l3e7oc+8z+zky1zZvZpWuzCpupH+CF7sS2/wB2+Z15ud8uHtj7V83nm5nlvH8HR8e/9H8HD/V8N21r/LT/ALxf2nFT6Rl3ez6x23t1KbM/+by7a9O3FaXML/p7/csub6pXPt+ldk7HWu3JX8vk2eDZg8Wgf5NPaJL5PPRHkn4X1d5P6W9feZ8KnR+5PkH4rzbp9/I6j4OXv5f/AHtoF94D/mn+7z/zL27tvtdn3m/7Jk2Zuxb32d3X3sm3N73ViaHJH/iT+1h/xR2Psns7/d7ntOfbl7Xuvaz093Nsp7vXjunb4Pj7RibLCwvHlb+XS9z8le7HyKe6fuBbvcr1J7U+pPX/AKhfesfPet/Geceoup6vnfe8/j4/i37OKfUcgyZ8lBTp6MNb/Ss5z7rPXb0Vr14YMifLvKozyfp/KfL2XlfScHS+XdMn0PTcv4On6bh4N3hw7t2zcenb04cxSmzox6GyxnE2WFibLCwAbUR/L8+9mW/mK+Sn3z9YzXur639r/WnrT4POvOvNP1l5v1G/m877zncXF47d8f1Ddjd58lNnThsl+mbxt7u95XbWla4OTjz0Z6Ao3tz5T7e+j6z6D8g6fyL0Z5Ky9L+S9J+E8p8k5HTcr7vk8PD4btuFs2Y5vert8uHFMuUZPcps8nVjKXXTdM463kdHyFur6rl9N03LNz+o5v3XI5W/j4vh4d+/w2xj1hNzuffy4vuPIe4HN9d9a59WfIf7IcHnnH+P9T9N+rvPOo4up4fvOdxcz49+zza/Ucvs+7/mrhgvPpOf2ve68lPtxiPbo/lqfcNh5J1/n/VN/JPn59kPTHmnGHQ9L1X6o6vquHl9T91zOHf4bZufqWXb0f5K4xafSc+zp/z5aYc9rfpz0/C+kPJPSvlbH076b6H0/wCS9On5b5L5X+rvK+k4eRyPuuXu4fDdszGtdvTh/FKDLTLj29sYzgbfc2/h3exh/wCIX7P+kN7/ANDev/Q3r7zvko9d7R+rfx/qLpuDndF4crdzvh3bdNr2jef7eubrpWnnxyXnZd3/ALrLl6q0r5sKP4q/llPdBpzPmm4PM/H3V9nPa/dzf/F9N+I8s/6PHg2dn+p5fy+atcMafSM/5/PSmHzMY+hPbbHvtd5N7Z+h6n7denOm9PehPIWHpHyHovwfk3p/p+l5X3XI4eH4d2zE2bMc3vV2+XBKmXIMlMlBTydWOJe5P/D39jHH8Qb2Z9Ebn3o/3K9CesvO+Wj1XtP6v/Hep+RwcfQfFyv0vwbbrbtG8/2+bN4q/bTHPd9l3f8AusuXqrSvmrhPjE38sr7zRvH83fL86/8Atn2Z9md3ON/5l0/4nyX/ALeDds8P9Tyfk81a4Yo/pG8/P56Uw5Faf4dfyMS/XfLZ8gfpyF8w9O+gfYLyL1LCeTb/ACv/ANHb/VvQ7uP7zzDg4/vtmcdo3+zN2ivjrh+bsvZtuTs1PFl/DpxyJoz/AIHvzB1L5Lfk++Yfy+zejvab259c9B6amPVXk/p/9a8r0v1fVcvw6bj4vh37bZu3bs77Pu/HWmNFv9O3o7Pu971UpXx4/9k=)</a>\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/meta-llama/prompt-ops/blob/main/notebook/prompt-ops_101_cerebras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "\n",
    "# Getting Started with [prompt-ops](https://github.com/meta-llama/prompt-ops)\n",
    "\n",
    "This notebook will guide you through the process of using [prompt-ops](https://github.com/meta-llama/prompt-ops) to optimize your prompts for Llama models. We'll cover:\n",
    "\n",
    "1. Introduction to prompt-ops\n",
    "2. Setting up your environment\n",
    "3. Creating a sample project\n",
    "4. Running prompt optimization\n",
    "5. Analyzing the results\n",
    "6. Advanced usage and customization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to prompt-ops\n",
    "\n",
    "### What is prompt-ops?\n",
    "\n",
    "prompt-ops is a Python package that **automatically optimizes prompts** for Llama models. It transforms prompts that work well with other LLMs into prompts that are optimized for Llama models, improving performance and reliability.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "prompt-ops takes three key inputs:\n",
    "1. Your existing system prompt\n",
    "2. A dataset of query-response pairs for evaluation and optimization\n",
    "3. A configuration file specifying model parameters and optimization details\n",
    "\n",
    "It then applies optimization techniques to transform your prompt into one that works better with Llama models, and provides metrics to measure the improvement.\n",
    "\n",
    "### Using Cerebras Inference\n",
    "\n",
    "This notebook demonstrates using prompt-ops with **Cerebras Inference API**, which provides ultra-fast inference for Llama models. Cerebras offers excellent performance and speed for prompt optimization workloads, making it ideal for rapid iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting up your environment\n",
    "\n",
    "Let's start by installing the Prompt ops package and setting up our environment. You can install it either from PyPI or directly from the source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/justinai/anaconda3/lib/python3.12/site-packages/conda/base/context.py:201: FutureWarning: Adding 'defaults' to channel list implicitly is deprecated and will be removed in 25.3. \n",
      "\n",
      "To remove this warning, please choose a default channel explicitly with conda's regular configuration system, e.g. by adding 'defaults' to the list of channels:\n",
      "\n",
      "  conda config --add channels defaults\n",
      "\n",
      "For more information see https://docs.conda.io/projects/conda/en/stable/user-guide/configuration/use-condarc.html\n",
      "\n",
      "  deprecated.topic(\n",
      "/Users/justinai/anaconda3/lib/python3.12/site-packages/conda/base/context.py:201: FutureWarning: Adding 'defaults' to channel list implicitly is deprecated and will be removed in 25.3. \n",
      "\n",
      "To remove this warning, please choose a default channel explicitly with conda's regular configuration system, e.g. by adding 'defaults' to the list of channels:\n",
      "\n",
      "  conda config --add channels defaults\n",
      "\n",
      "For more information see https://docs.conda.io/projects/conda/en/stable/user-guide/configuration/use-condarc.html\n",
      "\n",
      "  deprecated.topic(\n",
      "Channels:\n",
      " - defaults\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/justinai/anaconda3/envs/prompt-ops-test2\n",
      "\n",
      "  added / updated specs:\n",
      "    - python=3.10\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    libcxx-20.1.8              |       hd7fd590_1         306 KB\n",
      "    openssl-3.0.18             |       h9b4081a_0         3.1 MB\n",
      "    pip-25.2                   |     pyhc872135_1         1.1 MB\n",
      "    python-3.10.19             |       hf701271_0        11.5 MB\n",
      "    setuptools-80.9.0          |  py310hca03da5_0         1.4 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        17.4 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  bzip2              pkgs/main/osx-arm64::bzip2-1.0.8-h80987f9_6 \n",
      "  ca-certificates    pkgs/main/osx-arm64::ca-certificates-2025.9.9-hca03da5_0 \n",
      "  expat              pkgs/main/osx-arm64::expat-2.7.1-h313beb8_0 \n",
      "  libcxx             pkgs/main/osx-arm64::libcxx-20.1.8-hd7fd590_1 \n",
      "  libffi             pkgs/main/osx-arm64::libffi-3.4.4-hca03da5_1 \n",
      "  libzlib            pkgs/main/osx-arm64::libzlib-1.3.1-h5f15de7_0 \n",
      "  ncurses            pkgs/main/osx-arm64::ncurses-6.5-hee39554_0 \n",
      "  openssl            pkgs/main/osx-arm64::openssl-3.0.18-h9b4081a_0 \n",
      "  pip                pkgs/main/noarch::pip-25.2-pyhc872135_1 \n",
      "  python             pkgs/main/osx-arm64::python-3.10.19-hf701271_0 \n",
      "  readline           pkgs/main/osx-arm64::readline-8.3-h0b18652_0 \n",
      "  setuptools         pkgs/main/osx-arm64::setuptools-80.9.0-py310hca03da5_0 \n",
      "  sqlite             pkgs/main/osx-arm64::sqlite-3.50.2-h79febb2_1 \n",
      "  tk                 pkgs/main/osx-arm64::tk-8.6.15-hcd8a7d5_0 \n",
      "  tzdata             pkgs/main/noarch::tzdata-2025b-h04d1e81_0 \n",
      "  wheel              pkgs/main/osx-arm64::wheel-0.45.1-py310hca03da5_0 \n",
      "  xz                 pkgs/main/osx-arm64::xz-5.6.4-h80987f9_1 \n",
      "  zlib               pkgs/main/osx-arm64::zlib-1.3.1-h5f15de7_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "python-3.10.19       | 11.5 MB   |                                       |   0% \n",
      "openssl-3.0.18       | 3.1 MB    |                                       |   0% \u001b[A\n",
      "\n",
      "setuptools-80.9.0    | 1.4 MB    |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pip-25.2             | 1.1 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libcxx-20.1.8        | 306 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "python-3.10.19       | 11.5 MB   |                                       |   0% \u001b[A\n",
      "\n",
      "\n",
      "pip-25.2             | 1.1 MB    | 5                                     |   1% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "setuptools-80.9.0    | 1.4 MB    | 4                                     |   1% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libcxx-20.1.8        | 306 KB    | #9                                    |   5% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "openssl-3.0.18       | 3.1 MB    | ######################                |  60% \u001b[A\n",
      "\n",
      "\n",
      "python-3.10.19       | 11.5 MB   | ###3                                  |   9% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "setuptools-80.9.0    | 1.4 MB    | #################7                    |  48% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libcxx-20.1.8        | 306 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pip-25.2             | 1.1 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "openssl-3.0.18       | 3.1 MB    | ##################################### | 100% \u001b[A\n",
      "\n",
      "python-3.10.19       | 11.5 MB   | ##########4                           |  28% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libcxx-20.1.8        | 306 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "python-3.10.19       | 11.5 MB   | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "openssl-3.0.18       | 3.1 MB    | ##################################### | 100% \u001b[A\n",
      "openssl-3.0.18       | 3.1 MB    | ##################################### | 100% \u001b[A\n",
      "\n",
      "\n",
      "pip-25.2             | 1.1 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "setuptools-80.9.0    | 1.4 MB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "#\n",
      "# To activate this environment, use\n",
      "#\n",
      "#     $ conda activate prompt-ops-test2\n",
      "#\n",
      "# To deactivate an active environment, use\n",
      "#\n",
      "#     $ conda deactivate\n",
      "\n",
      "\n",
      "CondaError: Run 'conda init' before 'conda activate'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a virtual environment\n",
    "!conda create -n prompt-ops-test2 python=3.10 -y\n",
    "!conda activate prompt-ops-test2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change     /Users/justinai/anaconda3/condabin/conda\n",
      "no change     /Users/justinai/anaconda3/bin/conda\n",
      "no change     /Users/justinai/anaconda3/bin/conda-env\n",
      "no change     /Users/justinai/anaconda3/bin/activate\n",
      "no change     /Users/justinai/anaconda3/bin/deactivate\n",
      "no change     /Users/justinai/anaconda3/etc/profile.d/conda.sh\n",
      "no change     /Users/justinai/anaconda3/etc/fish/conf.d/conda.fish\n",
      "no change     /Users/justinai/anaconda3/shell/condabin/Conda.psm1\n",
      "no change     /Users/justinai/anaconda3/shell/condabin/conda-hook.ps1\n",
      "no change     /Users/justinai/anaconda3/lib/python3.12/site-packages/xontrib/conda.xsh\n",
      "no change     /Users/justinai/anaconda3/etc/profile.d/conda.csh\n",
      "no change     /Users/justinai/.bash_profile\n",
      "No action taken.\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CondaError: Run 'conda init' before 'conda activate'\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda activate prompt-ops-test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'prompt-ops'...\n",
      "remote: Enumerating objects: 1160, done.\u001b[K\n",
      "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
      "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
      "remote: Total 1160 (delta 43), reused 41 (delta 41), pack-reused 1110 (from 2)\u001b[K\n",
      "Receiving objects: 100% (1160/1160), 831.67 KiB | 6.40 MiB/s, done.\n",
      "Resolving deltas: 100% (557/557), done.\n"
     ]
    }
   ],
   "source": [
    "# Recommended: Install from source\n",
    "!git clone https://github.com/meta-llama/prompt-ops.git\n",
    "\n",
    "\n",
    "# Alternative: Install from PyPI (may have naming transition issues, still on version 0.0.7)\n",
    "# pip install llama-prompt-ops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///Users/justinai/Documents/Code/llama-prompt-ops\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: click>=8.0.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from prompt-ops==0.0.9) (8.1.8)\n",
      "Requirement already satisfied: requests>=2.25.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from prompt-ops==0.0.9) (2.32.3)\n",
      "Requirement already satisfied: dspy==2.6.13 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from prompt-ops==0.0.9) (2.6.13)\n",
      "Requirement already satisfied: numpy>=2.2.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from prompt-ops==0.0.9) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.15.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from prompt-ops==0.0.9) (1.15.2)\n",
      "Requirement already satisfied: pandas>=2.2.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from prompt-ops==0.0.9) (2.2.3)\n",
      "Requirement already satisfied: python-dotenv>=1.0.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from prompt-ops==0.0.9) (1.1.0)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from prompt-ops==0.0.9) (6.0.2)\n",
      "Requirement already satisfied: tiktoken>=0.9.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from prompt-ops==0.0.9) (0.9.0)\n",
      "Requirement already satisfied: openai>=1.65.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from prompt-ops==0.0.9) (1.72.0)\n",
      "Requirement already satisfied: litellm>=1.63.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from prompt-ops==0.0.9) (1.65.4.post1)\n",
      "Requirement already satisfied: huggingface-hub>=0.29.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from prompt-ops==0.0.9) (0.30.2)\n",
      "Requirement already satisfied: datasets>=2.21.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from prompt-ops==0.0.9) (3.5.0)\n",
      "Requirement already satisfied: propcache==0.3.1 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from prompt-ops==0.0.9) (0.3.1)\n",
      "Requirement already satisfied: backoff in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from dspy==2.6.13->prompt-ops==0.0.9) (2.2.1)\n",
      "Requirement already satisfied: joblib~=1.3 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from dspy==2.6.13->prompt-ops==0.0.9) (1.4.2)\n",
      "Requirement already satisfied: regex in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from dspy==2.6.13->prompt-ops==0.0.9) (2024.11.6)\n",
      "Requirement already satisfied: ujson in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from dspy==2.6.13->prompt-ops==0.0.9) (5.10.0)\n",
      "Requirement already satisfied: tqdm in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from dspy==2.6.13->prompt-ops==0.0.9) (4.67.1)\n",
      "Requirement already satisfied: optuna in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from dspy==2.6.13->prompt-ops==0.0.9) (4.2.1)\n",
      "Requirement already satisfied: pydantic~=2.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from dspy==2.6.13->prompt-ops==0.0.9) (2.11.3)\n",
      "Requirement already satisfied: magicattr~=0.1.6 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from dspy==2.6.13->prompt-ops==0.0.9) (0.1.6)\n",
      "Requirement already satisfied: diskcache in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from dspy==2.6.13->prompt-ops==0.0.9) (5.6.3)\n",
      "Requirement already satisfied: json-repair in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from dspy==2.6.13->prompt-ops==0.0.9) (0.40.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from dspy==2.6.13->prompt-ops==0.0.9) (9.1.2)\n",
      "Requirement already satisfied: anyio in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from dspy==2.6.13->prompt-ops==0.0.9) (4.9.0)\n",
      "Requirement already satisfied: asyncer==0.0.8 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from dspy==2.6.13->prompt-ops==0.0.9) (0.0.8)\n",
      "Requirement already satisfied: cachetools in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from dspy==2.6.13->prompt-ops==0.0.9) (5.5.2)\n",
      "Requirement already satisfied: cloudpickle in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from dspy==2.6.13->prompt-ops==0.0.9) (3.1.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from anyio->dspy==2.6.13->prompt-ops==0.0.9) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from anyio->dspy==2.6.13->prompt-ops==0.0.9) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from anyio->dspy==2.6.13->prompt-ops==0.0.9) (1.3.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from anyio->dspy==2.6.13->prompt-ops==0.0.9) (4.13.1)\n",
      "Requirement already satisfied: aiohttp in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from litellm>=1.63.0->prompt-ops==0.0.9) (3.11.16)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from litellm>=1.63.0->prompt-ops==0.0.9) (0.28.1)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from litellm>=1.63.0->prompt-ops==0.0.9) (8.6.1)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from litellm>=1.63.0->prompt-ops==0.0.9) (3.1.6)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from litellm>=1.63.0->prompt-ops==0.0.9) (4.23.0)\n",
      "Requirement already satisfied: tokenizers in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from litellm>=1.63.0->prompt-ops==0.0.9) (0.21.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.63.0->prompt-ops==0.0.9) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.63.0->prompt-ops==0.0.9) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.63.0->prompt-ops==0.0.9) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.63.0->prompt-ops==0.0.9) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.63.0->prompt-ops==0.0.9) (0.24.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from pydantic~=2.0->dspy==2.6.13->prompt-ops==0.0.9) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from pydantic~=2.0->dspy==2.6.13->prompt-ops==0.0.9) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from pydantic~=2.0->dspy==2.6.13->prompt-ops==0.0.9) (0.4.0)\n",
      "Requirement already satisfied: filelock in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from datasets>=2.21.0->prompt-ops==0.0.9) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from datasets>=2.21.0->prompt-ops==0.0.9) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from datasets>=2.21.0->prompt-ops==0.0.9) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from datasets>=2.21.0->prompt-ops==0.0.9) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from datasets>=2.21.0->prompt-ops==0.0.9) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.21.0->prompt-ops==0.0.9) (2024.12.0)\n",
      "Requirement already satisfied: packaging in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from datasets>=2.21.0->prompt-ops==0.0.9) (24.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from aiohttp->litellm>=1.63.0->prompt-ops==0.0.9) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from aiohttp->litellm>=1.63.0->prompt-ops==0.0.9) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from aiohttp->litellm>=1.63.0->prompt-ops==0.0.9) (5.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from aiohttp->litellm>=1.63.0->prompt-ops==0.0.9) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from aiohttp->litellm>=1.63.0->prompt-ops==0.0.9) (6.4.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from aiohttp->litellm>=1.63.0->prompt-ops==0.0.9) (1.19.0)\n",
      "Requirement already satisfied: certifi in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from httpx>=0.23.0->litellm>=1.63.0->prompt-ops==0.0.9) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from httpx>=0.23.0->litellm>=1.63.0->prompt-ops==0.0.9) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.23.0->litellm>=1.63.0->prompt-ops==0.0.9) (0.14.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from importlib-metadata>=6.8.0->litellm>=1.63.0->prompt-ops==0.0.9) (3.21.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from openai>=1.65.0->prompt-ops==0.0.9) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from openai>=1.65.0->prompt-ops==0.0.9) (0.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from pandas>=2.2.0->prompt-ops==0.0.9) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from pandas>=2.2.0->prompt-ops==0.0.9) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from pandas>=2.2.0->prompt-ops==0.0.9) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=2.2.0->prompt-ops==0.0.9) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from requests>=2.25.0->prompt-ops==0.0.9) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from requests>=2.25.0->prompt-ops==0.0.9) (2.3.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from optuna->dspy==2.6.13->prompt-ops==0.0.9) (1.15.2)\n",
      "Requirement already satisfied: colorlog in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from optuna->dspy==2.6.13->prompt-ops==0.0.9) (6.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from optuna->dspy==2.6.13->prompt-ops==0.0.9) (2.0.40)\n",
      "Requirement already satisfied: Mako in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from alembic>=1.5.0->optuna->dspy==2.6.13->prompt-ops==0.0.9) (1.3.9)\n",
      "Building wheels for collected packages: prompt-ops\n",
      "  Building editable for prompt-ops (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for prompt-ops: filename=prompt_ops-0.0.9-0.editable-py3-none-any.whl size=6952 sha256=785bb0f2125ab8fd3a084126bec44b8c126e69f1d9b5c9368d621d256771acb4\n",
      "  Stored in directory: /private/var/folders/s9/5_z22sq92_gf508gtrm9njm40000gn/T/pip-ephem-wheel-cache-nbspjqqy/wheels/8a/34/65/b5980476e16151db2c9dfa1d8434fb8917e868cf6264d07151\n",
      "Successfully built prompt-ops\n",
      "Installing collected packages: prompt-ops\n",
      "  Attempting uninstall: prompt-ops\n",
      "    Found existing installation: prompt-ops 0.0.9\n",
      "    Uninstalling prompt-ops-0.0.9:\n",
      "      Successfully uninstalled prompt-ops-0.0.9\n",
      "Successfully installed prompt-ops-0.0.9\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!cd prompt-ops\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating a Sample Project\n",
    "\n",
    "Prompt ops provides a convenient way to create a sample project with all the necessary files. Let's create a sample project to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/6] Creating project structure...\n",
      " Created project directory: my-cerebras-project-2\n",
      " Created data directory\n",
      " Created prompts directory\n",
      "\n",
      "[2/6] Generating configuration file...\n",
      " Created config.yaml\n",
      "\n",
      "[3/6] Creating prompt template...\n",
      " Created prompt.txt\n",
      "\n",
      "[4/6] Generating sample dataset...\n",
      " Created dataset.json with 200 examples\n",
      "\n",
      "[5/6] Setting up environment...\n",
      " Created .env file\n",
      "\n",
      "[6/6] Creating documentation...\n",
      " Created README.md\n",
      "\n",
      " Done! Project 'my-cerebras-project-2' created successfully!\n",
      "\n",
      "To get started:\n",
      "1. cd my-cerebras-project-2\n",
      "2. Edit the .env file to add your OPENROUTER_API_KEY\n",
      "   You can get an API key at: https://openrouter.ai/\n",
      "3. Run: prompt-ops migrate\n"
     ]
    }
   ],
   "source": [
    "# Create a sample project\n",
    "!prompt-ops create my-cerebras-project-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update config.yaml for Cerebras Inference\n",
    "\n",
    "Here's the configuration for using Cerebras Inference API with ultra-fast Llama 3.3 70B:\n",
    "\n",
    "```yaml\n",
    "system_prompt:\n",
    "  file: prompts/prompt.txt\n",
    "  inputs:\n",
    "  - question\n",
    "  outputs:\n",
    "  - answer\n",
    "\n",
    "dataset:\n",
    "  path: data/dataset.json\n",
    "  input_field:\n",
    "  - fields\n",
    "  - input\n",
    "  golden_output_field: answer\n",
    "\n",
    "model:\n",
    "  # Cerebras inference with Llama 3.3 70B\n",
    "  task_model: cerebras/llama3.3-70b\n",
    "  proposer_model: cerebras/llama3.3-70b\n",
    "  \n",
    "  # Cerebras API endpoint\n",
    "  api_base: https://api.cerebras.ai/v1\n",
    "  \n",
    "  # Temperature and generation settings\n",
    "  temperature: 0.0\n",
    "  max_tokens: 4096\n",
    "\n",
    "metric:\n",
    "  class: prompt_ops.core.metrics.FacilityMetric\n",
    "  strict_json: false\n",
    "  output_field: answer\n",
    "\n",
    "optimization:\n",
    "  strategy: basic\n",
    "```\n",
    "\n",
    "**Note**: Make sure to set your `CEREBRAS_API_KEY` environment variable before running the optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command creates a directory called `my-cerebras-project` with a sample configuration and dataset. Let's explore the files that were created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 24\n",
      "drwxr-xr-x@  7 justinai  staff   224 Oct 23 17:31 \u001b[34m.\u001b[m\u001b[m\n",
      "drwxr-xr-x@ 45 justinai  staff  1440 Oct 23 17:31 \u001b[34m..\u001b[m\u001b[m\n",
      "-rw-r--r--@  1 justinai  staff    81 Oct 23 17:32 .env\n",
      "-rw-r--r--@  1 justinai  staff   533 Oct 23 17:31 README.md\n",
      "-rw-r--r--@  1 justinai  staff   615 Oct 23 17:33 config.yaml\n",
      "drwxr-xr-x@  3 justinai  staff    96 Oct 23 17:31 \u001b[34mdata\u001b[m\u001b[m\n",
      "drwxr-xr-x@  3 justinai  staff    96 Oct 23 17:31 \u001b[34mprompts\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls -la my-cerebras-project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample project includes:\n",
    "- `.env`: A file for your Cerebras API key\n",
    "- `README.md`: Documentation for the project\n",
    "- `config.yaml`: Configuration file for prompt optimization\n",
    "- `data/dataset.json`: Sample dataset for evaluation and optimization\n",
    "- `prompts/prompt.txt`: Sample system prompt to optimize\n",
    "\n",
    "### Setting up your Cerebras API Key\n",
    "\n",
    "Before running optimization, you need to add your Cerebras API key to the `.env` file:\n",
    "\n",
    "```bash\n",
    "echo \"CEREBRAS_API_KEY=csk-your-api-key-here\" > my-cerebras-project/.env\n",
    "```\n",
    "\n",
    "Let's examine the configuration file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system_prompt:\n",
      "  file: prompts/prompt.txt\n",
      "  inputs:\n",
      "  - question\n",
      "  outputs:\n",
      "  - answer\n",
      "\n",
      "dataset:\n",
      "  path: data/dataset.json\n",
      "  input_field:\n",
      "  - fields\n",
      "  - input\n",
      "  golden_output_field: answer\n",
      "\n",
      "model:\n",
      "  # Cerebras inference with Llama 3.3 70B\n",
      "  task_model: cerebras/llama3.3-70b-instruct\n",
      "  proposer_model: cerebras/llama3.3-70b-instruct\n",
      "  \n",
      "  # Cerebras API endpoint\n",
      "  api_base: https://api.cerebras.ai/v1\n",
      "  \n",
      "  # Temperature and generation settings\n",
      "  temperature: 0.0\n",
      "  max_tokens: 4096\n",
      "\n",
      "metric:\n",
      "  class: prompt_ops.core.metrics.FacilityMetric\n",
      "  strict_json: false\n",
      "  output_field: answer\n",
      "\n",
      "optimization:\n",
      "  strategy: basic"
     ]
    }
   ],
   "source": [
    "!cat my-cerebras-project/config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration file specifies:\n",
    "- The system prompt to optimize\n",
    "- The dataset to use for evaluation and optimization\n",
    "- The model to use for optimization and evaluation\n",
    "- The metric to use for evaluation\n",
    "- The optimization strategy to use\n",
    "\n",
    "Let's also look at the sample prompt and dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant. Extract and return a json with the following keys and values:\n",
      "- \"urgency\" as one of `high`, `medium`, `low`\n",
      "- \"sentiment\" as one of `negative`, `neutral`, `positive`\n",
      "- \"categories\" Create a dictionary with categories as keys and boolean values (True/False), where the value indicates whether the category is one of the best matching support category tags from: `emergency_repair_services`, `routine_maintenance_requests`, `quality_and_safety_concerns`, `specialized_cleaning_services`, `general_inquiries`, `sustainability_and_environmental_practices`, `training_and_support_requests`, `cleaning_services_scheduling`, `customer_feedback_and_complaints`, `facility_management_issues`\n",
      "Your complete message should be a valid json string that can be read directly and only contain the keys mentioned in the list above. Never enclose it in ```json...```, no newlines, no unnessacary whitespaces.\n"
     ]
    }
   ],
   "source": [
    "!cat my-cerebras-project/prompts/prompt.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"fields\": {\n",
      "      \"input\": \"Subject: Urgent Assistance Required for Specialized Cleaning Services\\n\\nDear ProCare Facility Solutions Support Team,\\n\\nI hope this message finds you well. My name is [Sender], and my family and I have been availing your services for our home for the past year. We have always appreciated the high standards and professionalism your team brings to maintaining our living environment.\\n\\nHowever, we are currently facing an urgent issue that requires immediate attention. We recently hosted a large gathering at our home, and despite our best efforts, there are several areas that now require specialized cleaning. Specifically, we need deep cleaning for our carpets and upholstery, as well as thorough window washing. The situation is quite pressing as we have more guests arriving soon, and we want to ensure our home is in pristine condition to welcome them.\\n\\nWe have tried some basic cleaning ourselves, but the results have not been satisfactory. Given the high standards we have come to expect from ProCare, we are confident that your team can handle this situation efficiently and effectively.\\n\\nCould you please arrange for a specialized cleaning team to visit our home at the earliest convenience? We would greatly appreciate it if this could be prioritized due to the urgency of the situation.\\n\\nThank you for your prompt attention to this matter. We look forward to your swift response and assistance.\\n\\nBest regards,\\n[Sender]\"\n",
      "    },\n",
      "    \"answer\": \"{\\\"categories\\\": {\\\"routine_maintenance_requests\\\": false, \\\"customer_feedback_and_complaints\\\": false, \\\"training_and_support_requests\\\": false, \\\"quality_and_safety_concerns\\\": false, \\\"sustainability_and_environmental_practices\\\": false, \\\"cleaning_services_scheduling\\\": false, \\\"specialized_cleaning_services\\\": true, \\\"emergency_repair_services\\\": false, \\\"facility_management_issues\\\": false, \\\"general_inquiries\\\": false}, \\\"sentiment\\\": \\\"neutral\\\", \\\"urgency\\\": \\\"high\\\"}\"\n",
      "  },\n",
      "  {\n",
      "    \"fields\": {\n",
      "      \"input\": \"Subject: Inquiry About Specialized Cleaning Services\\n\\nHi ProCare Support Team,\\n\\nI hope this message finds you well. My name is Alex, and I've been a client of ProCare Facility Solutions for a few months now. I must say, your services have been quite satisfactory so far, especially the routine maintenance and cleaning schedules.\\n\\nI am reaching out to inquire about your specialized cleaning services. Specifically, I am interested in deep cleaning and carpet maintenance for my residential property. While the regular cleaning has been great, I feel that a more thorough cleaning would really help maintain the pristine condition of my home.\\n\\nI haven't taken any steps yet to address this, as I wanted to get more information from your team first. Could you please provide me with details on how these specialized services work, the scheduling options available, and any additional costs involved?\\n\\nLooking forward to your response.\\n\\nBest regards,\\nAlex\"\n",
      "    },\n",
      "    \"answer\": \"{\\\"categories\\\": {\\\"routine_maintenance_requests\\\": false, \\\"customer_feedback_and_complaints\\\": false, \\\"training_and_support_requests\\\": false, \\\"quality_and_safety_concerns\\\": false, \\\"sustainability_and_environmental_practices\\\": false, \\\"cleaning_services_scheduling\\\": false, \\\"specialized_cleaning_services\\\": true, \\\"emergency_repair_services\\\": false, \\\"facility_management_issues\\\": false, \\\"general_inquiries\\\": true}, \\\"sentiment\\\": \\\"neutral\\\", \\\"urgency\\\": \\\"low\\\"}\"\n",
      "  },\n",
      "  {\n",
      "    \"fields\": {\n",
      "      \"input\": \"Subject: Guidance Needed for Routine Plumbing Maintenance\\n\\nDear ProCare Support Team,\\n\\nI hope this message finds you well. My name is Dr. Samuel Thompson, and I have been a satisfied client of ProCare Facility Solutions for the past two years. Your commitment to quality and sustainability has always resonated deeply with my values, and I am grateful for the exceptional service your team consistently provides.\\n\\nI am writing to seek your assistance with a minor plumbing issue that has recently come to my attention. While it is not an urgent matter, I believe addressing it sooner rather than later would be beneficial. Specifically, there seems to be a small leak in the plumbing system of my office building. Although it has not caused any significant disruption, I would appreciate your expert guidance on how to proceed.\\n\\nIn an effort to mitigate the issue, I have already inspected the area and ensured that the immediate surroundings are dry and safe. However, given the importance of maintaining a well-functioning facility, I would like to request a professional assessment and any necessary routine maintenance at your earliest convenience.\\n\\nYour expertise and dedication to excellence have always been a source of reassurance for me, and I am confident that your team will handle this matter with the same level of care and professionalism that I have come to expect.\\n\\nThank you for your attention to this matter. I look forward to your prompt response and guidance.\\n\\nWarm regards,\\n\\nDr. Samuel Thompson\"\n",
      "    },\n",
      "    \"answer\": \"{\\\"categories\\\": {\\\"routine_maintenance_requests\\\": true, \\\"customer_feedback_and_complaints\\\": false, \\\"training_and_support_requests\\\": false, \\\"quality_and_safety_concerns\\\": false, \\\"sustainability_and_environmental_practices\\\": false, \\\"cleaning_services_scheduling\\\": false, \\\"specialized_cleaning_services\\\": false, \\\"emergency_repair_services\\\": false, \\\"facility_management_issues\\\": false, \\\"general_inquiries\\\": false}, \\\"sentiment\\\": \\\"positive\\\", \\\"urgency\\\": \\\"medium\\\"}\"\n",
      "  },\n",
      "  {\n"
     ]
    }
   ],
   "source": [
    "!head -n 20 my-cerebras-project/data/dataset.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running Prompt Optimization\n",
    "\n",
    "Now that we have our sample project set up, let's run the prompt optimization process. We'll use the `migrate` command, which takes a configuration file as input and outputs an optimized prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded environment variables from .env\n",
      "Loaded configuration from config.yaml\n",
      "2025-10-23 18:08:02,050 | INFO    |  Using model with DSPy: cerebras/llama3.3-70b\n",
      "Using the same model for task and proposer: cerebras/llama3.3-70b\n",
      "Using metric: FacilityMetric\n",
      "Resolved relative dataset path to: /Users/justinai/Documents/Code/llama-prompt-ops/my-cerebras-project/data/dataset.json\n",
      "Using dataset adapter: ConfigurableJSONAdapter\n",
      "Using BasicOptimizationStrategy from config for model: llama3.3-70b\n",
      "2025-10-23 18:08:02,058 - root - INFO - Loaded 200 examples from /Users/justinai/Documents/Code/llama-prompt-ops/my-cerebras-project/data/dataset.json\n",
      "2025-10-23 18:08:02,058 - root - INFO - Created dataset splits:\n",
      "2025-10-23 18:08:02,058 - root - INFO -   - Training:   50 examples (25.0% of total)\n",
      "2025-10-23 18:08:02,058 - root - INFO -   - Validation: 50 examples (25.0% of total)\n",
      "2025-10-23 18:08:02,058 - root - INFO -   - Testing:    100 examples (50.0% of total)\n",
      "Loaded prompt from file: /Users/justinai/Documents/Code/llama-prompt-ops/my-cerebras-project/prompts/prompt.txt\n",
      "Using 'system_prompt' from config\n",
      "Using config filename as output prefix: config\n",
      "Starting prompt optimization...\n",
      "2025-10-23 18:08:02,065 | INFO    | Applying BasicOptimizationStrategy to optimize prompt\n",
      "2025-10-23 18:08:02,065 | INFO    | Training set size: 50\n",
      "2025-10-23 18:08:02,065 | INFO    | Validation set size: 50\n",
      "2025-10-23 18:08:02,065 | INFO    | Test set size: 100\n",
      "\n",
      "Computing baseline score on 100 test examples using 2 threads...\n",
      "Average Metric: 78.77 / 100 (78.8%): 100%|| 100/100 [00:09<00:00, 10.08it/s]\n",
      "2025/10/23 18:08:12 INFO dspy.evaluate.evaluate: Average Metric: 78.76666666666671 / 100 (78.8%)\n",
      "Baseline Score: 78.770 in 11.96s\n",
      "\n",
      "2025-10-23 18:08:14,030 | INFO    | === Pre-Optimization Summary ===\n",
      "    Task Model       : cerebras/llama3.3-70b\n",
      "    Proposer Model   : cerebras/llama3.3-70b\n",
      "    Metric           : <prompt_ops.core.metrics.FacilityMetric object at 0x1382fc760>\n",
      "    Train / Val size : 50 / 50\n",
      "    MIPRO Params     : {\"auto_user\":\"basic\",\"auto_dspy\":\"light\",\"max_labeled_demos\":5,\"max_bootstrapped_demos\":4,\"num_candidates\":10,\"num_threads\":2,\"init_temperature\":0.5,\"seed\":9}\n",
      "    Baseline score   : 78.7700\n",
      "2025-10-23 18:08:14,030 - root - INFO - Optimization strategy using 5 labeled demos, 4 bootstrapped demos with 2 threads\n",
      "2025-10-23 18:08:14,030 - root - INFO - Compiling program with 50 training examples, 50 validation examples, and 100 test examples\n",
      "2025-10-23 18:08:14,044 - root - WARNING - Debug module not available, continuing without enhanced debugging\n",
      "2025-10-23 18:08:14,044 - root - INFO - Starting DSPy optimization with enhanced debugging\n",
      "2025-10-23 18:08:14,044 - root - INFO - Program type: <class 'dspy.predict.predict.Predict'>\n",
      "2025-10-23 18:08:14,044 - root - INFO - Trainset size: 50\n",
      "2025-10-23 18:08:14,044 - root - INFO - Valset size: 50\n",
      "2025-10-23 18:08:14,044 - root - INFO - First trainset example structure: <class 'dspy.primitives.example.Example'>\n",
      "2025-10-23 18:08:14,044 - root - WARNING - Example missing required 'inputs' or 'outputs' attributes\n",
      "2025-10-23 18:08:14,044 - root - WARNING - Example attributes: ['__class__', '__contains__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_demos', '_input_keys', '_output_keys', '_store', 'copy', 'get', 'inputs', 'items', 'keys', 'labels', 'toDict', 'values', 'with_inputs', 'without']\n",
      "2025-10-23 18:08:14,044 - root - INFO - Calling optimizer.compile\n",
      "2025/10/23 18:08:14 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "RUNNING WITH THE FOLLOWING LIGHT AUTO RUN SETTINGS:\n",
      "num_trials: 7\n",
      "minibatch: False\n",
      "num_candidates: 5\n",
      "valset size: 50\n",
      "\n",
      "2025/10/23 18:08:14 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "==> STEP 1: BOOTSTRAP FEWSHOT EXAMPLES <==\n",
      "2025/10/23 18:08:14 INFO dspy.teleprompt.mipro_optimizer_v2: These will be used as few-shot example candidates for our program and for creating instructions.\n",
      "\n",
      "2025/10/23 18:08:14 INFO dspy.teleprompt.mipro_optimizer_v2: Bootstrapping N=5 sets of demonstrations...\n",
      "Bootstrapping set 1/5\n",
      "Bootstrapping set 2/5\n",
      "Bootstrapping set 3/5\n",
      "  8%|                                        | 4/50 [00:03<00:35,  1.30it/s]\n",
      "Bootstrapped 4 full traces after 4 examples for up to 1 rounds, amounting to 4 attempts.\n",
      "Bootstrapping set 4/5\n",
      "  6%|                                         | 3/50 [00:00<00:12,  3.70it/s]\n",
      "Bootstrapped 3 full traces after 3 examples for up to 1 rounds, amounting to 3 attempts.\n",
      "Bootstrapping set 5/5\n",
      "  4%|                                          | 2/50 [00:00<00:14,  3.40it/s]\n",
      "Bootstrapped 2 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n",
      "2025/10/23 18:08:18 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "==> STEP 2: PROPOSE INSTRUCTION CANDIDATES <==\n",
      "2025/10/23 18:08:18 INFO dspy.teleprompt.mipro_optimizer_v2: We will use the few-shot examples from the previous step, a generated dataset summary, a summary of the program code, and a randomly selected prompting tip to propose instructions.\n",
      "2025/10/23 18:08:21 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "Proposing instructions...\n",
      "\n",
      "2025/10/23 18:08:25 INFO dspy.teleprompt.mipro_optimizer_v2: Proposed Instructions for Predictor 0:\n",
      "\n",
      "2025/10/23 18:08:25 INFO dspy.teleprompt.mipro_optimizer_v2: 0: You are a helpful assistant. Extract and return a json with the following keys and values:\n",
      "- \"urgency\" as one of `high`, `medium`, `low`\n",
      "- \"sentiment\" as one of `negative`, `neutral`, `positive`\n",
      "- \"categories\" Create a dictionary with categories as keys and boolean values (True/False), where the value indicates whether the category is one of the best matching support category tags from: `emergency_repair_services`, `routine_maintenance_requests`, `quality_and_safety_concerns`, `specialized_cleaning_services`, `general_inquiries`, `sustainability_and_environmental_practices`, `training_and_support_requests`, `cleaning_services_scheduling`, `customer_feedback_and_complaints`, `facility_management_issues`\n",
      "Your complete message should be a valid json string that can be read directly and only contain the keys mentioned in the list above. Never enclose it in ```json...```, no newlines, no unnessacary whitespaces.\n",
      "\n",
      "2025/10/23 18:08:25 INFO dspy.teleprompt.mipro_optimizer_v2: 1: You are a skilled customer support analyst for ProCare Facility Solutions, tasked with categorizing and analyzing client requests. Extract and return a json with the following keys and values: \"urgency\" as one of `high`, `medium`, `low`, \"sentiment\" as one of `negative`, `neutral`, `positive`, and \"categories\" as a dictionary with categories as keys and boolean values (True/False), where the value indicates whether the category is one of the best matching support category tags from: `emergency_repair_services`, `routine_maintenance_requests`, `quality_and_safety_concerns`, `specialized_cleaning_services`, `general_inquiries`, `sustainability_and_environmental_practices`, `training_and_support_requests`, `cleaning_services_scheduling`, `customer_feedback_and_complaints`, `facility_management_issues`. Your complete message should be a valid json string that can be read directly and only contain the keys mentioned in the list above.\n",
      "\n",
      "2025/10/23 18:08:25 INFO dspy.teleprompt.mipro_optimizer_v2: 2: Given a customer inquiry, analyze the message to determine the urgency level as high, medium, or low, and the sentiment as negative, neutral, or positive. Then, categorize the inquiry into relevant support categories such as emergency repair services, routine maintenance requests, quality and safety concerns, specialized cleaning services, general inquiries, sustainability and environmental practices, training and support requests, cleaning services scheduling, customer feedback and complaints, and facility management issues, marking each category as true or false based on relevance. Return the analysis as a json string with the keys \"urgency\", \"sentiment\", and \"categories\", ensuring the string is valid, concise, and without unnecessary whitespace.\n",
      "\n",
      "2025/10/23 18:08:25 INFO dspy.teleprompt.mipro_optimizer_v2: 3: Given a customer inquiry, analyze the message to determine the urgency level as high, medium, or low, the sentiment as negative, neutral, or positive, and identify the relevant categories from the provided list, marking each as true or false. Return the analysis as a JSON string with the keys \"urgency\", \"sentiment\", and \"categories\", ensuring the categories dictionary only includes the specified support category tags and boolean values indicating their relevance.\n",
      "\n",
      "2025/10/23 18:08:25 INFO dspy.teleprompt.mipro_optimizer_v2: 4: Given a customer inquiry or support request, analyze the text to determine the urgency, sentiment, and relevant categories, and return a JSON object with the keys \"urgency\", \"sentiment\", and \"categories\", where \"urgency\" is one of \"high\", \"medium\", \"low\", \"sentiment\" is one of \"negative\", \"neutral\", \"positive\", and \"categories\" is a dictionary with category names as keys and boolean values indicating whether the category is a best match, using the categories: \"emergency_repair_services\", \"routine_maintenance_requests\", \"quality_and_safety_concerns\", \"specialized_cleaning_services\", \"general_inquiries\", \"sustainability_and_environmental_practices\", \"training_and_support_requests\", \"cleaning_services_scheduling\", \"customer_feedback_and_complaints\", \"facility_management_issues\".\n",
      "\n",
      "2025/10/23 18:08:25 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "\n",
      "2025/10/23 18:08:25 INFO dspy.teleprompt.mipro_optimizer_v2: ==> STEP 3: FINDING OPTIMAL PROMPT PARAMETERS <==\n",
      "2025/10/23 18:08:25 INFO dspy.teleprompt.mipro_optimizer_v2: We will evaluate the program over a series of trials with different combinations of instructions and few-shot examples to find the optimal combination using Bayesian Optimization.\n",
      "\n",
      "2025/10/23 18:08:25 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 1 / 7 - Full Evaluation of Default Program ==\n",
      "Average Metric: 39.87 / 50 (79.7%): 100%|| 50/50 [00:05<00:00,  8.53it/s]\n",
      "2025/10/23 18:08:31 INFO dspy.evaluate.evaluate: Average Metric: 39.86666666666665 / 50 (79.7%)\n",
      "2025/10/23 18:08:31 INFO dspy.teleprompt.mipro_optimizer_v2: Default program score: 79.73\n",
      "\n",
      "/Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages/optuna/_experimental.py:31: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n",
      "2025/10/23 18:08:31 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 2 / 7 =====\n",
      "Average Metric: 42.73 / 50 (85.5%): 100%|| 50/50 [00:07<00:00,  6.28it/s]\n",
      "2025/10/23 18:08:39 INFO dspy.evaluate.evaluate: Average Metric: 42.733333333333306 / 50 (85.5%)\n",
      "2025/10/23 18:08:39 INFO dspy.teleprompt.mipro_optimizer_v2: \u001b[92mBest full score so far!\u001b[0m Score: 85.47\n",
      "2025/10/23 18:08:39 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 85.47 with parameters ['Predictor 0: Instruction 1', 'Predictor 0: Few-Shot Set 1'].\n",
      "2025/10/23 18:08:39 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [79.73, 85.47]\n",
      "2025/10/23 18:08:39 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 85.47\n",
      "2025/10/23 18:08:39 INFO dspy.teleprompt.mipro_optimizer_v2: =======================\n",
      "\n",
      "\n",
      "2025/10/23 18:08:39 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 3 / 7 =====\n",
      "Average Metric: 41.83 / 50 (83.7%): 100%|| 50/50 [01:08<00:00,  1.36s/it]\n",
      "2025/10/23 18:09:47 INFO dspy.evaluate.evaluate: Average Metric: 41.83333333333331 / 50 (83.7%)\n",
      "2025/10/23 18:09:47 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 83.67 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 1'].\n",
      "2025/10/23 18:09:47 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [79.73, 85.47, 83.67]\n",
      "2025/10/23 18:09:47 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 85.47\n",
      "2025/10/23 18:09:47 INFO dspy.teleprompt.mipro_optimizer_v2: =======================\n",
      "\n",
      "\n",
      "2025/10/23 18:09:47 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 4 / 7 =====\n",
      "Average Metric: 41.93 / 50 (83.9%): 100%|| 50/50 [00:09<00:00,  5.19it/s]\n",
      "2025/10/23 18:09:57 INFO dspy.evaluate.evaluate: Average Metric: 41.933333333333316 / 50 (83.9%)\n",
      "2025/10/23 18:09:57 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 83.87 with parameters ['Predictor 0: Instruction 4', 'Predictor 0: Few-Shot Set 1'].\n",
      "2025/10/23 18:09:57 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [79.73, 85.47, 83.67, 83.87]\n",
      "2025/10/23 18:09:57 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 85.47\n",
      "2025/10/23 18:09:57 INFO dspy.teleprompt.mipro_optimizer_v2: =======================\n",
      "\n",
      "\n",
      "2025/10/23 18:09:57 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 5 / 7 =====\n",
      "Average Metric: 41.83 / 50 (83.7%): 100%|| 50/50 [00:10<00:00,  4.77it/s]\n",
      "2025/10/23 18:10:07 INFO dspy.evaluate.evaluate: Average Metric: 41.83333333333331 / 50 (83.7%)\n",
      "2025/10/23 18:10:07 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 83.67 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 1'].\n",
      "2025/10/23 18:10:07 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [79.73, 85.47, 83.67, 83.87, 83.67]\n",
      "2025/10/23 18:10:07 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 85.47\n",
      "2025/10/23 18:10:07 INFO dspy.teleprompt.mipro_optimizer_v2: =======================\n",
      "\n",
      "\n",
      "2025/10/23 18:10:07 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 6 / 7 =====\n",
      "Average Metric: 39.87 / 50 (79.7%): 100%|| 50/50 [00:08<00:00,  6.08it/s]\n",
      "2025/10/23 18:10:16 INFO dspy.evaluate.evaluate: Average Metric: 39.86666666666665 / 50 (79.7%)\n",
      "2025/10/23 18:10:16 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 79.73 with parameters ['Predictor 0: Instruction 4', 'Predictor 0: Few-Shot Set 3'].\n",
      "2025/10/23 18:10:16 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [79.73, 85.47, 83.67, 83.87, 83.67, 79.73]\n",
      "2025/10/23 18:10:16 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 85.47\n",
      "2025/10/23 18:10:16 INFO dspy.teleprompt.mipro_optimizer_v2: =======================\n",
      "\n",
      "\n",
      "2025/10/23 18:10:16 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 7 / 7 =====\n",
      "Average Metric: 42.30 / 50 (84.6%): 100%|| 50/50 [01:08<00:00,  1.37s/it]\n",
      "2025/10/23 18:11:24 INFO dspy.evaluate.evaluate: Average Metric: 42.29999999999998 / 50 (84.6%)\n",
      "2025/10/23 18:11:24 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 84.6 with parameters ['Predictor 0: Instruction 0', 'Predictor 0: Few-Shot Set 1'].\n",
      "2025/10/23 18:11:24 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [79.73, 85.47, 83.67, 83.87, 83.67, 79.73, 84.6]\n",
      "2025/10/23 18:11:24 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 85.47\n",
      "2025/10/23 18:11:24 INFO dspy.teleprompt.mipro_optimizer_v2: =======================\n",
      "\n",
      "\n",
      "2025/10/23 18:11:24 INFO dspy.teleprompt.mipro_optimizer_v2: Returning best identified program with score 85.47!\n",
      "2025-10-23 18:11:24,869 - root - INFO - Optimizer.compile completed successfully\n",
      "2025-10-23 18:11:24,869 - root - INFO - Optimized program type: <class 'dspy.predict.predict.Predict'>\n",
      "2025-10-23 18:11:24,870 - root - INFO - Optimized program attributes: ['__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__subclasshook__', '__weakref__', '_base_init', 'batch', 'callbacks', 'candidate_programs', 'config', 'deepcopy', 'demos', 'dump_state', 'forward', 'get_config', 'get_lm', 'lm', 'load', 'load_state', 'map_named_predictors', 'mb_candidate_programs', 'model_family', 'named_parameters', 'named_predictors', 'named_sub_modules', 'parameters', 'predictors', 'prompt_model_total_calls', 'reset', 'reset_copy', 'save', 'score', 'set_lm', 'signature', 'stage', 'total_calls', 'traces', 'train', 'trial_logs', 'update_config']\n",
      "2025-10-23 18:11:24,870 | INFO    | [Running optimization strategy] completed in 202.80s\n",
      "2025-10-23 18:11:24,871 | INFO    | Optimized prompt:\n",
      "2025-10-23 18:11:24,871 | INFO    | ----------------------------------------\n",
      "2025-10-23 18:11:24,871 | INFO    | You are a skilled customer support analyst for ProCare Facility Solutions, tasked with categorizing and analyzing client requests. Extract and return a json with the following keys and values: \"urgency\" as one of `high`, `medium`, `low`, \"sentiment\" as one of `negative`, `neutral`, `positive`, and \"categories\" as a dictionary with categories as keys and boolean values (True/False), where the value indicates whether the category is one of the best matching support category tags from: `emergency_repair_services`, `routine_maintenance_requests`, `quality_and_safety_concerns`, `specialized_cleaning_services`, `general_inquiries`, `sustainability_and_environmental_practices`, `training_and_support_requests`, `cleaning_services_scheduling`, `customer_feedback_and_complaints`, `facility_management_issues`. Your complete message should be a valid json string that can be read directly and only contain the keys mentioned in the list above.\n",
      "2025-10-23 18:11:24,874 | INFO    | ----------------------------------------\n",
      "2025-10-23 18:11:24,878 | INFO    | Saved optimized prompt to results/config_20251023_180802.json\n",
      "2025-10-23 18:11:24,905 | INFO    | Saved YAML prompt to results/config_20251023_180802.yaml\n",
      "2025-10-23 18:11:24,905 | INFO    | [Saving optimized prompt] completed in 0.03s\n",
      "\n",
      "=== Optimization Complete ===\n",
      "Results saved to: /Users/justinai/Documents/Code/llama-prompt-ops/my-cerebras-project/results/config_20251023_180802.json\n",
      "Results also saved to: /Users/justinai/Documents/Code/llama-prompt-ops/my-cerebras-project/results/config_20251023_180802.yaml\n",
      "\n",
      "Optimized prompt:\n",
      "================================================================================\n",
      "You are a skilled customer support analyst for ProCare Facility Solutions, tasked with categorizing and analyzing client requests. Extract and return a json with the following keys and values: \"urgency\" as one of `high`, `medium`, `low`, \"sentiment\" as one of `negative`, `neutral`, `positive`, and \"categories\" as a dictionary with categories as keys and boolean values (True/False), where the value indicates whether the category is one of the best matching support category tags from: `emergency_repair_services`, `routine_maintenance_requests`, `quality_and_safety_concerns`, `specialized_cleaning_services`, `general_inquiries`, `sustainability_and_environmental_practices`, `training_and_support_requests`, `cleaning_services_scheduling`, `customer_feedback_and_complaints`, `facility_management_issues`. Your complete message should be a valid json string that can be read directly and only contain the keys mentioned in the list above.\n",
      "================================================================================\n",
      "2025-10-23 18:11:24,908 | INFO    | === Timings summary ===\n",
      "2025-10-23 18:11:24,908 | INFO    | Running optimization strategy 202.80s\n",
      "2025-10-23 18:11:24,908 | INFO    | Saving optimized prompt     0.03s\n"
     ]
    }
   ],
   "source": [
    "# Run prompt optimization with Cerebras\n",
    "!cd my-cerebras-project && prompt-ops migrate --config config.yaml --api-key-env CEREBRAS_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization process will take a few minutes to complete. It involves:\n",
    "1. Loading your system prompt and dataset\n",
    "2. Analyzing the prompt structure and content\n",
    "3. Applying optimization techniques specific to Llama models\n",
    "4. Evaluating the optimized prompt against the original prompt\n",
    "5. Saving the optimized prompt to the `results/` directory\n",
    "\n",
    "Let's check the results directory to see the optimized prompt. If the optimizer successfully found a better prompt, it will be saved in the `results/` directory. You may need to run the optimization process again with different parameters or a larger dataset if the prompt is the same as the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: my-notebook-project/results/: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls -la my-cerebras-project/results/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimized prompt is saved as a YAML file with a timestamp. Let's examine the contents of the optimized prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No result files found. Make sure the optimization process completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "# Find the most recent result file\n",
    "result_files = glob.glob('my-cerebras-project/results/*.yaml')\n",
    "if result_files:\n",
    "    latest_result = max(result_files, key=os.path.getctime)\n",
    "    print(f\"Latest result file: {latest_result}\")\n",
    "    \n",
    "    # Load and display the optimized prompt\n",
    "    with open(latest_result, 'r') as f:\n",
    "        result = yaml.safe_load(f)\n",
    "        print(\"\\nOptimized System Prompt:\")\n",
    "        print(result.get('system', 'No system prompt found'))\n",
    "else:\n",
    "    print(\"No result files found. Make sure the optimization process completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyzing the Results\n",
    "\n",
    "Let's compare the original prompt with the optimized prompt to understand the changes made during optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No result files found. Make sure the optimization process completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the original prompt\n",
    "with open('my-cerebras-project/prompts/prompt.txt', 'r') as f:\n",
    "    original_prompt = f.read()\n",
    "\n",
    "# Find the most recent result file again\n",
    "result_files = glob.glob('my-cerebras-project/results/*.yaml')\n",
    "if result_files:\n",
    "    latest_result = max(result_files, key=os.path.getctime)\n",
    "    \n",
    "    # Load the optimized prompt\n",
    "    with open(latest_result, 'r') as f:\n",
    "        result = yaml.safe_load(f)\n",
    "        optimized_prompt = result.get('system', 'No system prompt found')\n",
    "    \n",
    "    # Print the comparison\n",
    "    print(\"Original Prompt:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(original_prompt)\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"\\nOptimized Prompt:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(optimized_prompt)\n",
    "    print(\"-\" * 80)\n",
    "else:\n",
    "    print(\"No result files found. Make sure the optimization process completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Differences in the Optimized Prompt\n",
    "\n",
    "The optimized prompt typically includes several improvements:\n",
    "\n",
    "1. **Better Structure**: Llama models respond better to clear, structured instructions\n",
    "2. **Llama-Specific Formatting**: Formatting that works better with Llama's training patterns\n",
    "3. **Few-Shot Examples**: Examples that help the model understand the expected output format\n",
    "4. **Clear Output Expectations**: More explicit instructions about what the output should look like\n",
    "\n",
    "These changes can significantly improve the model's performance on your specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Usage and Customization\n",
    "\n",
    "### Using Your Own Data\n",
    "\n",
    "To use your own data with Prompt ops, you'll need to:\n",
    "\n",
    "1. Prepare your dataset in JSON format\n",
    "2. Create a system prompt file\n",
    "3. Create a configuration file\n",
    "\n",
    "Check out the comprehensive guide [here](https://github.com/meta-llama/prompt-ops/tree/main/docs) to learn more.\n",
    "\n",
    "Now, let's see how to create a custom configuration file:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom_config.yaml\n",
    "system_prompt:\n",
    "  file: \"path/to/your/prompt.txt\"\n",
    "  inputs: [\"question\"]\n",
    "  outputs: [\"answer\"]\n",
    "\n",
    "# Dataset configuration\n",
    "dataset:\n",
    "  path: \"path/to/your/dataset.json\"\n",
    "  input_field: \"question\"  # or [\"fields\", \"input\"] for nested fields\n",
    "  golden_output_field: \"answer\"\n",
    "\n",
    "# Model configuration\n",
    "model:\n",
    "  # Cerebras Inference with Llama 3.3 70B\n",
    "  task_model: \"cerebras/llama3.3-70b-instruct\"\n",
    "  proposer_model: \"cerebras/llama3.3-70b-instruct\"\n",
    "  \n",
    "  # Cerebras API endpoint\n",
    "  api_base: \"https://api.cerebras.ai/v1\"\n",
    "  \n",
    "  # Generation settings\n",
    "  temperature: 0.0\n",
    "  max_tokens: 4096\n",
    "\n",
    "# Metric configuration\n",
    "metric:\n",
    "  class: \"prompt_ops.core.metrics.StandardJSONMetric\"\n",
    "  strict_json: false\n",
    "  output_field: \"answer\"\n",
    "\n",
    "# Optimization settings\n",
    "optimization:\n",
    "  strategy: \"llama\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Different Metrics\n",
    "\n",
    "Prompt ops supports different metrics for evaluating prompt performance. The default is `StandardJSONMetric`, but you can use other metrics like `FacilityMetric` for specific use cases.\n",
    "\n",
    "Here's an example of using the `FacilityMetric` for the facility support analyzer use case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing facility_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile facility_config.yaml\n",
    "system_prompt:\n",
    "  file: \"prompts/facility_prompt.txt\"\n",
    "  inputs: [\"question\"]\n",
    "  outputs: [\"answer\"]\n",
    "\n",
    "# Dataset configuration\n",
    "dataset:\n",
    "  path: \"data/facility_dataset.json\"\n",
    "  input_field: [\"fields\", \"input\"]\n",
    "  golden_output_field: \"answer\"\n",
    "\n",
    "# Model configuration\n",
    "model:\n",
    "  # Cerebras Inference with Llama 3.3 70B\n",
    "  task_model: \"cerebras/llama3.3-70b-instruct\"\n",
    "  proposer_model: \"cerebras/llama3.3-70b-instruct\"\n",
    "  \n",
    "  # Cerebras API endpoint\n",
    "  api_base: \"https://api.cerebras.ai/v1\"\n",
    "  \n",
    "  # Generation settings\n",
    "  temperature: 0.0\n",
    "  max_tokens: 4096\n",
    "\n",
    "# Metric configuration\n",
    "metric:\n",
    "  class: \"prompt_ops.core.metrics.FacilityMetric\"\n",
    "  strict_json: false\n",
    "  output_field: \"answer\"\n",
    "\n",
    "# Optimization settings\n",
    "optimization:\n",
    "  strategy: \"llama\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Different Models\n",
    "\n",
    "Prompt ops supports different models through various inference providers. You can use Cerebras, OpenRouter, vLLM, or NVIDIA NIMs depending on your infrastructure needs.\n",
    "\n",
    "With Cerebras, you have access to ultra-fast Llama models. Here's an example configuration using a different Llama model size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing different_model_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile different_model_config.yaml\n",
    "system_prompt:\n",
    "  file: \"prompts/prompt.txt\"\n",
    "  inputs: [\"question\"]\n",
    "  outputs: [\"answer\"]\n",
    "\n",
    "# Dataset configuration\n",
    "dataset:\n",
    "  path: \"data/dataset.json\"\n",
    "  input_field: \"question\"\n",
    "  golden_output_field: \"answer\"\n",
    "\n",
    "# Model configuration\n",
    "model:\n",
    "  # Using Llama 3.1 8B for faster, cost-effective optimization\n",
    "  task_model: \"cerebras/llama3.1-8b-instruct\"\n",
    "  proposer_model: \"cerebras/llama3.1-8b-instruct\"\n",
    "  \n",
    "  # Cerebras API endpoint\n",
    "  api_base: \"https://api.cerebras.ai/v1\"\n",
    "  \n",
    "  # Generation settings\n",
    "  temperature: 0.0\n",
    "  max_tokens: 4096\n",
    "\n",
    "# Metric configuration\n",
    "metric:\n",
    "  class: \"prompt_ops.core.metrics.StandardJSONMetric\"\n",
    "  strict_json: false\n",
    "  output_field: \"answer\"\n",
    "\n",
    "# Optimization settings\n",
    "optimization:\n",
    "  strategy: \"llama\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've covered:\n",
    "\n",
    "1. Introduction to Prompt ops and its benefits\n",
    "2. Creating a sample project\n",
    "3. Setting up your environment and Cerebras API key\n",
    "4. Running prompt optimization with Cerebras Inference\n",
    "5. Analyzing the results\n",
    "6. Advanced usage and customization options\n",
    "\n",
    "Prompt ops provides a powerful way to optimize your prompts for Llama models, improving performance and reliability. By using Cerebras Inference API, you get ultra-fast inference speeds that make prompt optimization rapid and efficient.\n",
    "\n",
    "### Key Takeaways with Cerebras\n",
    "\n",
    "- **Ultra-Fast Inference**: Cerebras provides exceptional inference speed for Llama models\n",
    "- **Easy Integration**: Simply configure the API endpoint and model name\n",
    "- **Production Ready**: Cerebras scales from development to production seamlessly\n",
    "\n",
    "By following the steps in this notebook, you can start optimizing your own prompts with Cerebras and building more effective LLM applications.\n",
    "\n",
    "For more information:\n",
    "- [prompt-ops documentation](https://github.com/meta-llama/prompt-ops/tree/main/docs)\n",
    "- [Cerebras Inference API](https://cerebras.ai/)\n",
    "- Explore example use cases in the prompt-ops repository"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-prompt-ops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
