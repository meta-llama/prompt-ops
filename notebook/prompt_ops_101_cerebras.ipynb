{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QppShkRBS47w"
      },
      "source": [
        "Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "All rights reserved.\n",
        "\n",
        "This source code is licensed under the terms described in the LICENSE file in\n",
        "the root directory of this source tree.\n",
        "\n",
        "<a aria-label=\"Meta home\" href=\"https://www.llama.com/docs\" tabindex=\"0\" target=\"_blank\" >![Meta---Logo@1x.jpg](data:image/jpeg;base64,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)</a>\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/meta-llama/prompt-ops/blob/main/notebook/prompt-ops_101_cerebras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "\n",
        "# Getting Started with [prompt-ops](https://github.com/meta-llama/prompt-ops)\n",
        "\n",
        "This notebook will guide you through the process of using [prompt-ops](https://github.com/meta-llama/prompt-ops) to optimize your prompts for Llama models. We'll cover:\n",
        "\n",
        "1. Introduction to prompt-ops\n",
        "2. Setting up your environment\n",
        "3. Creating a sample project\n",
        "4. Running prompt optimization\n",
        "5. Analyzing the results\n",
        "6. Advanced usage and customization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKjxPrP4S47y"
      },
      "source": [
        "## 1. Introduction to prompt-ops\n",
        "\n",
        "### What is prompt-ops?\n",
        "\n",
        "prompt-ops is a Python package that **automatically optimizes prompts** for Llama models. It transforms prompts that work well with other LLMs into prompts that are optimized for Llama models, improving performance and reliability.\n",
        "\n",
        "### How It Works\n",
        "\n",
        "prompt-ops takes three key inputs:\n",
        "1. Your existing system prompt\n",
        "2. A dataset of query-response pairs for evaluation and optimization\n",
        "3. A configuration file specifying model parameters and optimization details\n",
        "\n",
        "It then applies optimization techniques to transform your prompt into one that works better with Llama models, and provides metrics to measure the improvement.\n",
        "\n",
        "### Using Cerebras Inference\n",
        "\n",
        "This notebook demonstrates using prompt-ops with **Cerebras Inference API**, which provides ultra-fast inference for Llama models. Cerebras offers excellent performance and speed for prompt optimization workloads, making it ideal for rapid iteration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTYCdotaS47y"
      },
      "source": [
        "## 2. Setting up your environment\n",
        "\n",
        "Let's start by installing the Prompt ops package and setting up our environment. You can install it either from PyPI or directly from the source code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpVDbuZ-S47z",
        "outputId": "1014c786-a22d-4541-f81d-34695be33547"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'prompt-ops'...\n",
            "remote: Enumerating objects: 1160, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 1160 (delta 43), reused 41 (delta 41), pack-reused 1110 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1160/1160), 831.67 KiB | 3.60 MiB/s, done.\n",
            "Resolving deltas: 100% (557/557), done.\n"
          ]
        }
      ],
      "source": [
        "# Recommended: Install from source\n",
        "!git clone https://github.com/meta-llama/prompt-ops.git\n",
        "\n",
        "# Alternative: Install from PyPI (may have naming transition issues, still on version 0.0.7)\n",
        "# pip install llama-prompt-ops\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6frRw9zuS47z",
        "outputId": "d7874932-c9dd-402a-c34c-e0626c5b9316"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtaining file:///content/prompt-ops\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from prompt-ops==0.0.9) (8.3.0)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.12/dist-packages (from prompt-ops==0.0.9) (2.32.4)\n",
            "Collecting dspy==2.6.13 (from prompt-ops==0.0.9)\n",
            "  Downloading dspy-2.6.13-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting numpy>=2.2.0 (from prompt-ops==0.0.9)\n",
            "  Downloading numpy-2.3.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.15.0 in /usr/local/lib/python3.12/dist-packages (from prompt-ops==0.0.9) (1.16.2)\n",
            "Requirement already satisfied: pandas>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from prompt-ops==0.0.9) (2.2.2)\n",
            "Requirement already satisfied: python-dotenv>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from prompt-ops==0.0.9) (1.1.1)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from prompt-ops==0.0.9) (6.0.3)\n",
            "Requirement already satisfied: tiktoken>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from prompt-ops==0.0.9) (0.12.0)\n",
            "Requirement already satisfied: openai>=1.65.0 in /usr/local/lib/python3.12/dist-packages (from prompt-ops==0.0.9) (1.109.1)\n",
            "Collecting litellm>=1.63.0 (from prompt-ops==0.0.9)\n",
            "  Downloading litellm-1.79.0-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.29.0 in /usr/local/lib/python3.12/dist-packages (from prompt-ops==0.0.9) (0.35.3)\n",
            "Requirement already satisfied: datasets>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from prompt-ops==0.0.9) (4.0.0)\n",
            "Collecting propcache==0.3.1 (from prompt-ops==0.0.9)\n",
            "  Downloading propcache-0.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting backoff (from dspy==2.6.13->prompt-ops==0.0.9)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: joblib~=1.3 in /usr/local/lib/python3.12/dist-packages (from dspy==2.6.13->prompt-ops==0.0.9) (1.5.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from dspy==2.6.13->prompt-ops==0.0.9) (2024.11.6)\n",
            "Collecting ujson (from dspy==2.6.13->prompt-ops==0.0.9)\n",
            "  Downloading ujson-5.11.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from dspy==2.6.13->prompt-ops==0.0.9) (4.67.1)\n",
            "Collecting optuna (from dspy==2.6.13->prompt-ops==0.0.9)\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: pydantic~=2.0 in /usr/local/lib/python3.12/dist-packages (from dspy==2.6.13->prompt-ops==0.0.9) (2.11.10)\n",
            "Collecting magicattr~=0.1.6 (from dspy==2.6.13->prompt-ops==0.0.9)\n",
            "  Downloading magicattr-0.1.6-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting diskcache (from dspy==2.6.13->prompt-ops==0.0.9)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting json-repair (from dspy==2.6.13->prompt-ops==0.0.9)\n",
            "  Downloading json_repair-0.52.3-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from dspy==2.6.13->prompt-ops==0.0.9) (8.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from dspy==2.6.13->prompt-ops==0.0.9) (4.11.0)\n",
            "Collecting asyncer==0.0.8 (from dspy==2.6.13->prompt-ops==0.0.9)\n",
            "  Downloading asyncer-0.0.8-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from dspy==2.6.13->prompt-ops==0.0.9) (5.5.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from dspy==2.6.13->prompt-ops==0.0.9) (3.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.21.0->prompt-ops==0.0.9) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.21.0->prompt-ops==0.0.9) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.21.0->prompt-ops==0.0.9) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.21.0->prompt-ops==0.0.9) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.21.0->prompt-ops==0.0.9) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21.0->prompt-ops==0.0.9) (2025.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets>=2.21.0->prompt-ops==0.0.9) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.29.0->prompt-ops==0.0.9) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.29.0->prompt-ops==0.0.9) (1.1.10)\n",
            "Requirement already satisfied: aiohttp>=3.10 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.63.0->prompt-ops==0.0.9) (3.13.1)\n",
            "Collecting fastuuid>=0.13.0 (from litellm>=1.63.0->prompt-ops==0.0.9)\n",
            "  Downloading fastuuid-0.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.63.0->prompt-ops==0.0.9) (0.28.1)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.63.0->prompt-ops==0.0.9) (8.7.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.63.0->prompt-ops==0.0.9) (3.1.6)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.63.0->prompt-ops==0.0.9) (4.25.1)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (from litellm>=1.63.0->prompt-ops==0.0.9) (0.22.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.65.0->prompt-ops==0.0.9) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.65.0->prompt-ops==0.0.9) (0.11.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.65.0->prompt-ops==0.0.9) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->prompt-ops==0.0.9) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->prompt-ops==0.0.9) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->prompt-ops==0.0.9) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->prompt-ops==0.0.9) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->prompt-ops==0.0.9) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->prompt-ops==0.0.9) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->prompt-ops==0.0.9) (2025.10.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.63.0->prompt-ops==0.0.9) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.63.0->prompt-ops==0.0.9) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.63.0->prompt-ops==0.0.9) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.63.0->prompt-ops==0.0.9) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.63.0->prompt-ops==0.0.9) (6.7.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.63.0->prompt-ops==0.0.9) (1.22.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->litellm>=1.63.0->prompt-ops==0.0.9) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.23.0->litellm>=1.63.0->prompt-ops==0.0.9) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata>=6.8.0->litellm>=1.63.0->prompt-ops==0.0.9) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.63.0->prompt-ops==0.0.9) (3.0.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.63.0->prompt-ops==0.0.9) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.63.0->prompt-ops==0.0.9) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.63.0->prompt-ops==0.0.9) (0.27.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic~=2.0->dspy==2.6.13->prompt-ops==0.0.9) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic~=2.0->dspy==2.6.13->prompt-ops==0.0.9) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic~=2.0->dspy==2.6.13->prompt-ops==0.0.9) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.0->prompt-ops==0.0.9) (1.17.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna->dspy==2.6.13->prompt-ops==0.0.9) (1.17.0)\n",
            "Collecting colorlog (from optuna->dspy==2.6.13->prompt-ops==0.0.9)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna->dspy==2.6.13->prompt-ops==0.0.9) (2.0.44)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna->dspy==2.6.13->prompt-ops==0.0.9) (1.3.10)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna->dspy==2.6.13->prompt-ops==0.0.9) (3.2.4)\n",
            "Downloading dspy-2.6.13-py3-none-any.whl (247 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.6/247.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading propcache-0.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.0/245.0 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asyncer-0.0.8-py3-none-any.whl (9.2 kB)\n",
            "Downloading litellm-1.79.0-py3-none-any.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.3.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastuuid-0.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.1/278.1 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading magicattr-0.1.6-py2.py3-none-any.whl (4.7 kB)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json_repair-0.52.3-py3-none-any.whl (26 kB)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ujson-5.11.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Building wheels for collected packages: prompt-ops\n",
            "  Building editable for prompt-ops (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for prompt-ops: filename=prompt_ops-0.0.9-0.editable-py3-none-any.whl size=6923 sha256=04dac8c74758cc590370b89f1fb6e460c2876589196671e3e563b108d9dd017a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vwgdsod9/wheels/23/bf/0f/5b277f8707afba6838b902d00fa3765cb2b3d9c9dffb5dcd55\n",
            "Successfully built prompt-ops\n",
            "Installing collected packages: magicattr, ujson, propcache, numpy, json-repair, fastuuid, diskcache, colorlog, backoff, asyncer, optuna, litellm, dspy, prompt-ops\n",
            "  Attempting uninstall: propcache\n",
            "    Found existing installation: propcache 0.4.1\n",
            "    Uninstalling propcache-0.4.1:\n",
            "      Successfully uninstalled propcache-0.4.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asyncer-0.0.8 backoff-2.2.1 colorlog-6.10.1 diskcache-5.6.3 dspy-2.6.13 fastuuid-0.14.0 json-repair-0.52.3 litellm-1.79.0 magicattr-0.1.6 numpy-2.3.4 optuna-4.5.0 prompt-ops-0.0.9 propcache-0.3.1 ujson-5.11.0\n"
          ]
        }
      ],
      "source": [
        "!cd prompt-ops && pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCtE-YJyS47z"
      },
      "source": [
        "## 3. Creating a Sample Project\n",
        "\n",
        "Prompt ops provides a convenient way to create a sample project with all the necessary files. Let's create a sample project to get started."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P81Y2ZdmS47z",
        "outputId": "fd88f18a-d157-43a9-c768-fffc6da22f15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/6] Creating project structure...\n",
            "✓ Created project directory: my-cerebras-project\n",
            "✓ Created data directory\n",
            "✓ Created prompts directory\n",
            "\n",
            "[2/6] Generating configuration file...\n",
            "✓ Created config.yaml\n",
            "\n",
            "[3/6] Creating prompt template...\n",
            "✓ Created prompt.txt\n",
            "\n",
            "[4/6] Generating sample dataset...\n",
            "✓ Created dataset.json with 200 examples\n",
            "\n",
            "[5/6] Setting up environment...\n",
            "✓ Created .env file\n",
            "\n",
            "[6/6] Creating documentation...\n",
            "✓ Created README.md\n",
            "\n",
            "✨ Done! Project 'my-cerebras-project' created successfully!\n",
            "\n",
            "To get started:\n",
            "1. cd my-cerebras-project\n",
            "2. Edit the .env file to add your OPENROUTER_API_KEY\n",
            "   You can get an API key at: https://openrouter.ai/\n",
            "3. Run: prompt-ops migrate\n"
          ]
        }
      ],
      "source": [
        "# Create a sample project\n",
        "!prompt-ops create my-cerebras-project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-KlbRusS47z"
      },
      "source": [
        "### Update config.yaml for Cerebras Inference\n",
        "\n",
        "Here's the configuration for using Cerebras Inference API with ultra-fast Llama 3.3 70B:\n",
        "\n",
        "```yaml\n",
        "system_prompt:\n",
        "  file: prompts/prompt.txt\n",
        "  inputs:\n",
        "  - question\n",
        "  outputs:\n",
        "  - answer\n",
        "\n",
        "dataset:\n",
        "  path: data/dataset.json\n",
        "  input_field:\n",
        "  - fields\n",
        "  - input\n",
        "  golden_output_field: answer\n",
        "\n",
        "model:\n",
        "  # Cerebras inference with Llama 3.3 70B\n",
        "  task_model: cerebras/llama3.3-70b\n",
        "  proposer_model: cerebras/llama3.3-70b\n",
        "  \n",
        "  # Cerebras API endpoint\n",
        "  api_base: https://api.cerebras.ai/v1\n",
        "  \n",
        "  # Temperature and generation settings\n",
        "  temperature: 0.0\n",
        "  max_tokens: 4096\n",
        "\n",
        "metric:\n",
        "  class: prompt_ops.core.metrics.FacilityMetric\n",
        "  strict_json: false\n",
        "  output_field: answer\n",
        "\n",
        "optimization:\n",
        "  strategy: basic\n",
        "```\n",
        "\n",
        "**Note**: Make sure to set your `CEREBRAS_API_KEY` environment variable before running the optimization. Create a secrets and allow notebook access in the left sidebar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTQZKz2yUFbd",
        "outputId": "83bce219-98f0-4d9f-e0c5-d1111da99189"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API Key set: csk-nyd6pc...\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('CEREBRAS_API_KEY')\n",
        "import os\n",
        "os.environ['CEREBRAS_API_KEY'] = api_key\n",
        "\n",
        "# Verify it's set\n",
        "print(f\"API Key set: {api_key[:10]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2_ppYizS470"
      },
      "source": [
        "This command creates a directory called `my-cerebras-project` with a sample configuration and dataset. Let's explore the files that were created:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjfopGNdS470",
        "outputId": "9abf03e1-490e-46c1-8d56-864eb2cfda6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 32\n",
            "drwxr-xr-x 5 root root 4096 Oct 28 10:15 .\n",
            "drwxr-xr-x 1 root root 4096 Oct 28 10:12 ..\n",
            "-rw-r--r-- 1 root root  597 Oct 28 10:13 config.yaml\n",
            "drwxr-xr-x 2 root root 4096 Oct 28 10:12 data\n",
            "-rw-r--r-- 1 root root   42 Oct 28 10:15 .env\n",
            "drwxr-xr-x 2 root root 4096 Oct 28 10:15 .ipynb_checkpoints\n",
            "drwxr-xr-x 2 root root 4096 Oct 28 10:12 prompts\n",
            "-rw-r--r-- 1 root root  533 Oct 28 10:12 README.md\n"
          ]
        }
      ],
      "source": [
        "!ls -la my-cerebras-project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-za9sVicS470"
      },
      "source": [
        "The sample project includes:\n",
        "- `.env`: A file for your Cerebras API key\n",
        "- `README.md`: Documentation for the project\n",
        "- `config.yaml`: Configuration file for prompt optimization\n",
        "- `data/dataset.json`: Sample dataset for evaluation and optimization\n",
        "- `prompts/prompt.txt`: Sample system prompt to optimize\n",
        "\n",
        "\n",
        "Let's examine the configuration file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5l3WP3y_S470",
        "outputId": "a9bf7c0a-35dc-40e5-8cb1-e2b1dc9b6e4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "system_prompt:\n",
            "  file: prompts/prompt.txt\n",
            "  inputs:\n",
            "  - question\n",
            "  outputs:\n",
            "  - answer\n",
            "\n",
            "dataset:\n",
            "  path: data/dataset.json\n",
            "  input_field:\n",
            "  - fields\n",
            "  - input\n",
            "  golden_output_field: answer\n",
            "\n",
            "model:\n",
            "  # Cerebras inference with Llama 3.3 70B\n",
            "  task_model: cerebras/llama3.3-70b\n",
            "  proposer_model: cerebras/llama3.3-70b\n",
            "  \n",
            "  # Cerebras API endpoint\n",
            "  api_base: https://api.cerebras.ai/v1\n",
            "  \n",
            "  # Temperature and generation settings\n",
            "  temperature: 0.0\n",
            "  max_tokens: 4096\n",
            "\n",
            "metric:\n",
            "  class: prompt_ops.core.metrics.FacilityMetric\n",
            "  strict_json: false\n",
            "  output_field: answer\n",
            "\n",
            "optimization:\n",
            "  strategy: basic"
          ]
        }
      ],
      "source": [
        "!cat my-cerebras-project/config.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxjFjiiIS470"
      },
      "source": [
        "The configuration file specifies:\n",
        "- The system prompt to optimize\n",
        "- The dataset to use for evaluation and optimization\n",
        "- The model to use for optimization and evaluation\n",
        "- The metric to use for evaluation\n",
        "- The optimization strategy to use\n",
        "\n",
        "Let's also look at the sample prompt and dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqNUGMmhS470",
        "outputId": "792b3c0e-888d-4de7-a253-28f057953d2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are a helpful assistant. Extract and return a json with the following keys and values:\n",
            "- \"urgency\" as one of `high`, `medium`, `low`\n",
            "- \"sentiment\" as one of `negative`, `neutral`, `positive`\n",
            "- \"categories\" Create a dictionary with categories as keys and boolean values (True/False), where the value indicates whether the category is one of the best matching support category tags from: `emergency_repair_services`, `routine_maintenance_requests`, `quality_and_safety_concerns`, `specialized_cleaning_services`, `general_inquiries`, `sustainability_and_environmental_practices`, `training_and_support_requests`, `cleaning_services_scheduling`, `customer_feedback_and_complaints`, `facility_management_issues`\n",
            "Your complete message should be a valid json string that can be read directly and only contain the keys mentioned in the list above. Never enclose it in ```json...```, no newlines, no unnessacary whitespaces.\n"
          ]
        }
      ],
      "source": [
        "!cat my-cerebras-project/prompts/prompt.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUhP2h_eS470",
        "outputId": "25fa00e1-18ca-44ee-a46e-5c77b17146b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "  {\n",
            "    \"fields\": {\n",
            "      \"input\": \"Subject: Urgent Assistance Required for Specialized Cleaning Services\\n\\nDear ProCare Facility Solutions Support Team,\\n\\nI hope this message finds you well. My name is [Sender], and my family and I have been availing your services for our home for the past year. We have always appreciated the high standards and professionalism your team brings to maintaining our living environment.\\n\\nHowever, we are currently facing an urgent issue that requires immediate attention. We recently hosted a large gathering at our home, and despite our best efforts, there are several areas that now require specialized cleaning. Specifically, we need deep cleaning for our carpets and upholstery, as well as thorough window washing. The situation is quite pressing as we have more guests arriving soon, and we want to ensure our home is in pristine condition to welcome them.\\n\\nWe have tried some basic cleaning ourselves, but the results have not been satisfactory. Given the high standards we have come to expect from ProCare, we are confident that your team can handle this situation efficiently and effectively.\\n\\nCould you please arrange for a specialized cleaning team to visit our home at the earliest convenience? We would greatly appreciate it if this could be prioritized due to the urgency of the situation.\\n\\nThank you for your prompt attention to this matter. We look forward to your swift response and assistance.\\n\\nBest regards,\\n[Sender]\"\n",
            "    },\n",
            "    \"answer\": \"{\\\"categories\\\": {\\\"routine_maintenance_requests\\\": false, \\\"customer_feedback_and_complaints\\\": false, \\\"training_and_support_requests\\\": false, \\\"quality_and_safety_concerns\\\": false, \\\"sustainability_and_environmental_practices\\\": false, \\\"cleaning_services_scheduling\\\": false, \\\"specialized_cleaning_services\\\": true, \\\"emergency_repair_services\\\": false, \\\"facility_management_issues\\\": false, \\\"general_inquiries\\\": false}, \\\"sentiment\\\": \\\"neutral\\\", \\\"urgency\\\": \\\"high\\\"}\"\n",
            "  },\n",
            "  {\n",
            "    \"fields\": {\n",
            "      \"input\": \"Subject: Inquiry About Specialized Cleaning Services\\n\\nHi ProCare Support Team,\\n\\nI hope this message finds you well. My name is Alex, and I've been a client of ProCare Facility Solutions for a few months now. I must say, your services have been quite satisfactory so far, especially the routine maintenance and cleaning schedules.\\n\\nI am reaching out to inquire about your specialized cleaning services. Specifically, I am interested in deep cleaning and carpet maintenance for my residential property. While the regular cleaning has been great, I feel that a more thorough cleaning would really help maintain the pristine condition of my home.\\n\\nI haven't taken any steps yet to address this, as I wanted to get more information from your team first. Could you please provide me with details on how these specialized services work, the scheduling options available, and any additional costs involved?\\n\\nLooking forward to your response.\\n\\nBest regards,\\nAlex\"\n",
            "    },\n",
            "    \"answer\": \"{\\\"categories\\\": {\\\"routine_maintenance_requests\\\": false, \\\"customer_feedback_and_complaints\\\": false, \\\"training_and_support_requests\\\": false, \\\"quality_and_safety_concerns\\\": false, \\\"sustainability_and_environmental_practices\\\": false, \\\"cleaning_services_scheduling\\\": false, \\\"specialized_cleaning_services\\\": true, \\\"emergency_repair_services\\\": false, \\\"facility_management_issues\\\": false, \\\"general_inquiries\\\": true}, \\\"sentiment\\\": \\\"neutral\\\", \\\"urgency\\\": \\\"low\\\"}\"\n",
            "  },\n",
            "  {\n",
            "    \"fields\": {\n",
            "      \"input\": \"Subject: Guidance Needed for Routine Plumbing Maintenance\\n\\nDear ProCare Support Team,\\n\\nI hope this message finds you well. My name is Dr. Samuel Thompson, and I have been a satisfied client of ProCare Facility Solutions for the past two years. Your commitment to quality and sustainability has always resonated deeply with my values, and I am grateful for the exceptional service your team consistently provides.\\n\\nI am writing to seek your assistance with a minor plumbing issue that has recently come to my attention. While it is not an urgent matter, I believe addressing it sooner rather than later would be beneficial. Specifically, there seems to be a small leak in the plumbing system of my office building. Although it has not caused any significant disruption, I would appreciate your expert guidance on how to proceed.\\n\\nIn an effort to mitigate the issue, I have already inspected the area and ensured that the immediate surroundings are dry and safe. However, given the importance of maintaining a well-functioning facility, I would like to request a professional assessment and any necessary routine maintenance at your earliest convenience.\\n\\nYour expertise and dedication to excellence have always been a source of reassurance for me, and I am confident that your team will handle this matter with the same level of care and professionalism that I have come to expect.\\n\\nThank you for your attention to this matter. I look forward to your prompt response and guidance.\\n\\nWarm regards,\\n\\nDr. Samuel Thompson\"\n",
            "    },\n",
            "    \"answer\": \"{\\\"categories\\\": {\\\"routine_maintenance_requests\\\": true, \\\"customer_feedback_and_complaints\\\": false, \\\"training_and_support_requests\\\": false, \\\"quality_and_safety_concerns\\\": false, \\\"sustainability_and_environmental_practices\\\": false, \\\"cleaning_services_scheduling\\\": false, \\\"specialized_cleaning_services\\\": false, \\\"emergency_repair_services\\\": false, \\\"facility_management_issues\\\": false, \\\"general_inquiries\\\": false}, \\\"sentiment\\\": \\\"positive\\\", \\\"urgency\\\": \\\"medium\\\"}\"\n",
            "  },\n",
            "  {\n"
          ]
        }
      ],
      "source": [
        "!head -n 20 my-cerebras-project/data/dataset.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fijfzuOgS470"
      },
      "source": [
        "## 4. Running Prompt Optimization\n",
        "\n",
        "Now that we have our sample project set up, let's run the prompt optimization process. We'll use the `migrate` command, which takes a configuration file as input and outputs an optimized prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-cuj8piS470",
        "outputId": "2250c3e5-f367-4c34-904a-d684aca159b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded environment variables from .env\n",
            "Loaded configuration from config.yaml\n",
            "2025-10-28 10:20:05,260 | INFO    |  Using model with DSPy: cerebras/llama3.3-70b\n",
            "Using the same model for task and proposer: cerebras/llama3.3-70b\n",
            "Using metric: FacilityMetric\n",
            "Resolved relative dataset path to: /content/my-cerebras-project/data/dataset.json\n",
            "Using dataset adapter: ConfigurableJSONAdapter\n",
            "Using BasicOptimizationStrategy from config for model: llama3.3-70b\n",
            "2025-10-28 10:20:05,265 - root - INFO - Loaded 200 examples from /content/my-cerebras-project/data/dataset.json\n",
            "2025-10-28 10:20:05,266 - root - INFO - Created dataset splits:\n",
            "2025-10-28 10:20:05,266 - root - INFO -   - Training:   50 examples (25.0% of total)\n",
            "2025-10-28 10:20:05,266 - root - INFO -   - Validation: 50 examples (25.0% of total)\n",
            "2025-10-28 10:20:05,266 - root - INFO -   - Testing:    100 examples (50.0% of total)\n",
            "Loaded prompt from file: /content/my-cerebras-project/prompts/prompt.txt\n",
            "Using 'system_prompt' from config\n",
            "Using config filename as output prefix: config\n",
            "Starting prompt optimization...\n",
            "2025-10-28 10:20:05,267 | INFO    | Applying BasicOptimizationStrategy to optimize prompt\n",
            "2025-10-28 10:20:05,267 | INFO    | Training set size: 50\n",
            "2025-10-28 10:20:05,267 | INFO    | Validation set size: 50\n",
            "2025-10-28 10:20:05,267 | INFO    | Test set size: 100\n",
            "\n",
            "Computing baseline score on 100 test examples using 18 threads...\n",
            "Average Metric: 79.07 / 100 (79.1%): 100% 100/100 [00:09<00:00, 10.53it/s]\n",
            "2025/10/28 10:20:15 INFO dspy.evaluate.evaluate: Average Metric: 79.06666666666666 / 100 (79.1%)\n",
            "2025-10-28 10:20:15,226 - numexpr.utils - INFO - NumExpr defaulting to 2 threads.\n",
            "Baseline Score: 79.070 in 10.16s\n",
            "\n",
            "2025-10-28 10:20:15,427 | INFO    | === Pre-Optimization Summary ===\n",
            "    Task Model       : cerebras/llama3.3-70b\n",
            "    Proposer Model   : cerebras/llama3.3-70b\n",
            "    Metric           : <prompt_ops.core.metrics.FacilityMetric object at 0x7d3066abbec0>\n",
            "    Train / Val size : 50 / 50\n",
            "    MIPRO Params     : {\"auto_user\":\"basic\",\"auto_dspy\":\"light\",\"max_labeled_demos\":5,\"max_bootstrapped_demos\":4,\"num_candidates\":10,\"num_threads\":18,\"init_temperature\":0.5,\"seed\":9}\n",
            "    Baseline score   : 79.0700\n",
            "2025-10-28 10:20:15,429 - root - INFO - Optimization strategy using 5 labeled demos, 4 bootstrapped demos with 18 threads\n",
            "2025-10-28 10:20:15,429 - root - INFO - Compiling program with 50 training examples, 50 validation examples, and 100 test examples\n",
            "2025-10-28 10:20:15,431 - root - WARNING - Debug module not available, continuing without enhanced debugging\n",
            "2025-10-28 10:20:15,431 - root - INFO - Starting DSPy optimization with enhanced debugging\n",
            "2025-10-28 10:20:15,431 - root - INFO - Program type: <class 'dspy.predict.predict.Predict'>\n",
            "2025-10-28 10:20:15,431 - root - INFO - Trainset size: 50\n",
            "2025-10-28 10:20:15,431 - root - INFO - Valset size: 50\n",
            "2025-10-28 10:20:15,431 - root - INFO - First trainset example structure: <class 'dspy.primitives.example.Example'>\n",
            "2025-10-28 10:20:15,431 - root - WARNING - Example missing required 'inputs' or 'outputs' attributes\n",
            "2025-10-28 10:20:15,431 - root - WARNING - Example attributes: ['__class__', '__contains__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_demos', '_input_keys', '_output_keys', '_store', 'copy', 'get', 'inputs', 'items', 'keys', 'labels', 'toDict', 'values', 'with_inputs', 'without']\n",
            "2025-10-28 10:20:15,431 - root - INFO - Calling optimizer.compile\n",
            "2025/10/28 10:20:15 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
            "RUNNING WITH THE FOLLOWING LIGHT AUTO RUN SETTINGS:\n",
            "num_trials: 7\n",
            "minibatch: False\n",
            "num_candidates: 5\n",
            "valset size: 50\n",
            "\n",
            "2025/10/28 10:20:15 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
            "==> STEP 1: BOOTSTRAP FEWSHOT EXAMPLES <==\n",
            "2025/10/28 10:20:15 INFO dspy.teleprompt.mipro_optimizer_v2: These will be used as few-shot example candidates for our program and for creating instructions.\n",
            "\n",
            "2025/10/28 10:20:15 INFO dspy.teleprompt.mipro_optimizer_v2: Bootstrapping N=5 sets of demonstrations...\n",
            "Bootstrapping set 1/5\n",
            "Bootstrapping set 2/5\n",
            "Bootstrapping set 3/5\n",
            "  0% 0/50 [00:00<?, ?it/s]2025-10-28 10:20:16,270 - datasets - INFO - TensorFlow version 2.19.0 available.\n",
            "2025-10-28 10:20:16,271 - datasets - INFO - JAX version 0.7.2 available.\n",
            "  8% 4/50 [00:02<00:27,  1.65it/s]\n",
            "Bootstrapped 4 full traces after 4 examples for up to 1 rounds, amounting to 4 attempts.\n",
            "Bootstrapping set 4/5\n",
            "  6% 3/50 [00:00<00:12,  3.68it/s]\n",
            "Bootstrapped 3 full traces after 3 examples for up to 1 rounds, amounting to 3 attempts.\n",
            "Bootstrapping set 5/5\n",
            "  4% 2/50 [00:00<00:12,  3.72it/s]\n",
            "Bootstrapped 2 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n",
            "2025/10/28 10:20:19 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
            "==> STEP 2: PROPOSE INSTRUCTION CANDIDATES <==\n",
            "2025/10/28 10:20:19 INFO dspy.teleprompt.mipro_optimizer_v2: We will use the few-shot examples from the previous step, a generated dataset summary, a summary of the program code, and a randomly selected prompting tip to propose instructions.\n",
            "2025/10/28 10:20:22 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
            "Proposing instructions...\n",
            "\n",
            "2025/10/28 10:20:27 INFO dspy.teleprompt.mipro_optimizer_v2: Proposed Instructions for Predictor 0:\n",
            "\n",
            "2025/10/28 10:20:27 INFO dspy.teleprompt.mipro_optimizer_v2: 0: You are a helpful assistant. Extract and return a json with the following keys and values:\n",
            "- \"urgency\" as one of `high`, `medium`, `low`\n",
            "- \"sentiment\" as one of `negative`, `neutral`, `positive`\n",
            "- \"categories\" Create a dictionary with categories as keys and boolean values (True/False), where the value indicates whether the category is one of the best matching support category tags from: `emergency_repair_services`, `routine_maintenance_requests`, `quality_and_safety_concerns`, `specialized_cleaning_services`, `general_inquiries`, `sustainability_and_environmental_practices`, `training_and_support_requests`, `cleaning_services_scheduling`, `customer_feedback_and_complaints`, `facility_management_issues`\n",
            "Your complete message should be a valid json string that can be read directly and only contain the keys mentioned in the list above. Never enclose it in ```json...```, no newlines, no unnessacary whitespaces.\n",
            "\n",
            "2025/10/28 10:20:27 INFO dspy.teleprompt.mipro_optimizer_v2: 1: You are a skilled customer support analyst for ProCare Facility Solutions. Read the customer's message carefully and extract the relevant information to provide a response in the following format: {\"categories\": {\"emergency_repair_services\": false, \"routine_maintenance_requests\": false, \"quality_and_safety_concerns\": false, \"specialized_cleaning_services\": false, \"general_inquiries\": false, \"sustainability_and_environmental_practices\": false, \"training_and_support_requests\": false, \"cleaning_services_scheduling\": false, \"customer_feedback_and_complaints\": false, \"facility_management_issues\": false}, \"sentiment\": \"neutral\", \"urgency\": \"low\"} where the values for categories are set to true or false based on the content of the message, sentiment is one of \"negative\", \"neutral\", \"positive\", and urgency is one of \"high\", \"medium\", \"low\". Your response should be a valid json string.\n",
            "\n",
            "2025/10/28 10:20:27 INFO dspy.teleprompt.mipro_optimizer_v2: 2: Given a customer inquiry, analyze the text to determine the urgency, sentiment, and relevant categories, then return a json string with the keys \"urgency\", \"sentiment\", and \"categories\", where \"urgency\" is one of \"high\", \"medium\", \"low\", \"sentiment\" is one of \"negative\", \"neutral\", \"positive\", and \"categories\" is a dictionary with the specified support category tags as keys and boolean values indicating whether each category is a best match.\n",
            "\n",
            "2025/10/28 10:20:27 INFO dspy.teleprompt.mipro_optimizer_v2: 3: Given a customer inquiry, analyze the message to determine the urgency level as high, medium, or low, and the sentiment as negative, neutral, or positive. Then, categorize the inquiry into relevant support categories such as emergency repair services, routine maintenance requests, quality and safety concerns, specialized cleaning services, general inquiries, sustainability and environmental practices, training and support requests, cleaning services scheduling, customer feedback and complaints, and facility management issues. Return a json string with the keys \"urgency\", \"sentiment\", and \"categories\", where \"categories\" is a dictionary with the support categories as keys and boolean values indicating whether each category is a best match for the inquiry.\n",
            "\n",
            "2025/10/28 10:20:27 INFO dspy.teleprompt.mipro_optimizer_v2: 4: You are a skilled customer support analyst working for ProCare Facility Solutions. Given a customer inquiry, analyze the message to determine the urgency, sentiment, and relevant categories. The urgency can be high, medium, or low, and the sentiment can be negative, neutral, or positive. Categorize the inquiry into one or more of the following categories: emergency_repair_services, routine_maintenance_requests, quality_and_safety_concerns, specialized_cleaning_services, general_inquiries, sustainability_and_environmental_practices, training_and_support_requests, cleaning_services_scheduling, customer_feedback_and_complaints, facility_management_issues. Provide your analysis as a JSON string with the keys \"urgency\", \"sentiment\", and \"categories\", where \"categories\" is a dictionary with the category names as keys and boolean values indicating whether each category is relevant. Ensure the JSON string is valid, concise, and directly readable.\n",
            "\n",
            "2025/10/28 10:20:27 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
            "\n",
            "2025/10/28 10:20:27 INFO dspy.teleprompt.mipro_optimizer_v2: ==> STEP 3: FINDING OPTIMAL PROMPT PARAMETERS <==\n",
            "2025/10/28 10:20:27 INFO dspy.teleprompt.mipro_optimizer_v2: We will evaluate the program over a series of trials with different combinations of instructions and few-shot examples to find the optimal combination using Bayesian Optimization.\n",
            "\n",
            "2025/10/28 10:20:27 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 1 / 7 - Full Evaluation of Default Program ==\n",
            "Average Metric: 40.23 / 50 (80.5%): 100% 50/50 [00:03<00:00, 15.25it/s]\n",
            "2025/10/28 10:20:30 INFO dspy.evaluate.evaluate: Average Metric: 40.233333333333334 / 50 (80.5%)\n",
            "2025/10/28 10:20:30 INFO dspy.teleprompt.mipro_optimizer_v2: Default program score: 80.47\n",
            "\n",
            "/usr/local/lib/python3.12/dist-packages/optuna/_experimental.py:32: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.\n",
            "  warnings.warn(\n",
            "2025/10/28 10:20:30 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 2 / 7 =====\n",
            "Average Metric: 42.90 / 50 (85.8%): 100% 50/50 [00:04<00:00, 11.55it/s]\n",
            "2025/10/28 10:20:35 INFO dspy.evaluate.evaluate: Average Metric: 42.9 / 50 (85.8%)\n",
            "2025/10/28 10:20:35 INFO dspy.teleprompt.mipro_optimizer_v2: \u001b[92mBest full score so far!\u001b[0m Score: 85.8\n",
            "2025/10/28 10:20:35 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 85.8 with parameters ['Predictor 0: Instruction 1', 'Predictor 0: Few-Shot Set 1'].\n",
            "2025/10/28 10:20:35 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [80.47, 85.8]\n",
            "2025/10/28 10:20:35 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 85.8\n",
            "2025/10/28 10:20:35 INFO dspy.teleprompt.mipro_optimizer_v2: =======================\n",
            "\n",
            "\n",
            "2025/10/28 10:20:35 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 3 / 7 =====\n",
            "Average Metric: 41.47 / 50 (82.9%): 100% 50/50 [01:03<00:00,  1.26s/it]\n",
            "2025/10/28 10:21:38 INFO dspy.evaluate.evaluate: Average Metric: 41.46666666666667 / 50 (82.9%)\n",
            "2025/10/28 10:21:38 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 82.93 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 1'].\n",
            "2025/10/28 10:21:38 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [80.47, 85.8, 82.93]\n",
            "2025/10/28 10:21:38 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 85.8\n",
            "2025/10/28 10:21:38 INFO dspy.teleprompt.mipro_optimizer_v2: =======================\n",
            "\n",
            "\n",
            "2025/10/28 10:21:38 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 4 / 7 =====\n",
            "Average Metric: 42.23 / 50 (84.5%): 100% 50/50 [00:03<00:00, 15.04it/s]\n",
            "2025/10/28 10:21:42 INFO dspy.evaluate.evaluate: Average Metric: 42.233333333333334 / 50 (84.5%)\n",
            "2025/10/28 10:21:42 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 84.47 with parameters ['Predictor 0: Instruction 4', 'Predictor 0: Few-Shot Set 1'].\n",
            "2025/10/28 10:21:42 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [80.47, 85.8, 82.93, 84.47]\n",
            "2025/10/28 10:21:42 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 85.8\n",
            "2025/10/28 10:21:42 INFO dspy.teleprompt.mipro_optimizer_v2: =======================\n",
            "\n",
            "\n",
            "2025/10/28 10:21:42 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 5 / 7 =====\n",
            "Average Metric: 41.47 / 50 (82.9%): 100% 50/50 [01:04<00:00,  1.28s/it]\n",
            "2025/10/28 10:22:46 INFO dspy.evaluate.evaluate: Average Metric: 41.46666666666667 / 50 (82.9%)\n",
            "2025/10/28 10:22:46 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 82.93 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 1'].\n",
            "2025/10/28 10:22:46 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [80.47, 85.8, 82.93, 84.47, 82.93]\n",
            "2025/10/28 10:22:46 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 85.8\n",
            "2025/10/28 10:22:46 INFO dspy.teleprompt.mipro_optimizer_v2: =======================\n",
            "\n",
            "\n",
            "2025/10/28 10:22:46 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 6 / 7 =====\n",
            "Average Metric: 40.60 / 50 (81.2%): 100% 50/50 [00:03<00:00, 13.70it/s]\n",
            "2025/10/28 10:22:50 INFO dspy.evaluate.evaluate: Average Metric: 40.6 / 50 (81.2%)\n",
            "2025/10/28 10:22:50 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 81.2 with parameters ['Predictor 0: Instruction 4', 'Predictor 0: Few-Shot Set 3'].\n",
            "2025/10/28 10:22:50 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [80.47, 85.8, 82.93, 84.47, 82.93, 81.2]\n",
            "2025/10/28 10:22:50 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 85.8\n",
            "2025/10/28 10:22:50 INFO dspy.teleprompt.mipro_optimizer_v2: =======================\n",
            "\n",
            "\n",
            "2025/10/28 10:22:50 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 7 / 7 =====\n",
            "Average Metric: 42.30 / 50 (84.6%): 100% 50/50 [00:03<00:00, 13.35it/s]\n",
            "2025/10/28 10:22:54 INFO dspy.evaluate.evaluate: Average Metric: 42.3 / 50 (84.6%)\n",
            "2025/10/28 10:22:54 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 84.6 with parameters ['Predictor 0: Instruction 0', 'Predictor 0: Few-Shot Set 1'].\n",
            "2025/10/28 10:22:54 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [80.47, 85.8, 82.93, 84.47, 82.93, 81.2, 84.6]\n",
            "2025/10/28 10:22:54 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 85.8\n",
            "2025/10/28 10:22:54 INFO dspy.teleprompt.mipro_optimizer_v2: =======================\n",
            "\n",
            "\n",
            "2025/10/28 10:22:54 INFO dspy.teleprompt.mipro_optimizer_v2: Returning best identified program with score 85.8!\n",
            "2025-10-28 10:22:54,244 - root - INFO - Optimizer.compile completed successfully\n",
            "2025-10-28 10:22:54,244 - root - INFO - Optimized program type: <class 'dspy.predict.predict.Predict'>\n",
            "2025-10-28 10:22:54,245 - root - INFO - Optimized program attributes: ['__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__subclasshook__', '__weakref__', '_base_init', 'batch', 'callbacks', 'candidate_programs', 'config', 'deepcopy', 'demos', 'dump_state', 'forward', 'get_config', 'get_lm', 'lm', 'load', 'load_state', 'map_named_predictors', 'mb_candidate_programs', 'model_family', 'named_parameters', 'named_predictors', 'named_sub_modules', 'parameters', 'predictors', 'prompt_model_total_calls', 'reset', 'reset_copy', 'save', 'score', 'set_lm', 'signature', 'stage', 'total_calls', 'traces', 'train', 'trial_logs', 'update_config']\n",
            "2025-10-28 10:22:54,245 | INFO    | [Running optimization strategy] completed in 168.98s\n",
            "2025-10-28 10:22:54,245 | INFO    | Optimized prompt:\n",
            "2025-10-28 10:22:54,245 | INFO    | ----------------------------------------\n",
            "2025-10-28 10:22:54,245 | INFO    | You are a skilled customer support analyst for ProCare Facility Solutions. Read the customer's message carefully and extract the relevant information to provide a response in the following format: {\"categories\": {\"emergency_repair_services\": false, \"routine_maintenance_requests\": false, \"quality_and_safety_concerns\": false, \"specialized_cleaning_services\": false, \"general_inquiries\": false, \"sustainability_and_environmental_practices\": false, \"training_and_support_requests\": false, \"cleaning_services_scheduling\": false, \"customer_feedback_and_complaints\": false, \"facility_management_issues\": false}, \"sentiment\": \"neutral\", \"urgency\": \"low\"} where the values for categories are set to true or false based on the content of the message, sentiment is one of \"negative\", \"neutral\", \"positive\", and urgency is one of \"high\", \"medium\", \"low\". Your response should be a valid json string.\n",
            "2025-10-28 10:22:54,245 | INFO    | ----------------------------------------\n",
            "2025-10-28 10:22:54,246 | INFO    | Saved optimized prompt to results/config_20251028_102005.json\n",
            "2025-10-28 10:22:54,247 | INFO    | Saved YAML prompt to results/config_20251028_102005.yaml\n",
            "2025-10-28 10:22:54,247 | INFO    | [Saving optimized prompt] completed in 0.00s\n",
            "\n",
            "=== Optimization Complete ===\n",
            "Results saved to: /content/my-cerebras-project/results/config_20251028_102005.json\n",
            "Results also saved to: /content/my-cerebras-project/results/config_20251028_102005.yaml\n",
            "\n",
            "Optimized prompt:\n",
            "================================================================================\n",
            "You are a skilled customer support analyst for ProCare Facility Solutions. Read the customer's message carefully and extract the relevant information to provide a response in the following format: {\"categories\": {\"emergency_repair_services\": false, \"routine_maintenance_requests\": false, \"quality_and_safety_concerns\": false, \"specialized_cleaning_services\": false, \"general_inquiries\": false, \"sustainability_and_environmental_practices\": false, \"training_and_support_requests\": false, \"cleaning_services_scheduling\": false, \"customer_feedback_and_complaints\": false, \"facility_management_issues\": false}, \"sentiment\": \"neutral\", \"urgency\": \"low\"} where the values for categories are set to true or false based on the content of the message, sentiment is one of \"negative\", \"neutral\", \"positive\", and urgency is one of \"high\", \"medium\", \"low\". Your response should be a valid json string.\n",
            "================================================================================\n",
            "2025-10-28 10:22:54,250 | INFO    | === Timings summary ===\n",
            "2025-10-28 10:22:54,250 | INFO    | Running optimization strategy 168.98s\n",
            "2025-10-28 10:22:54,250 | INFO    | Saving optimized prompt     0.00s\n"
          ]
        }
      ],
      "source": [
        "# Run prompt optimization with Cerebras\n",
        "!cd my-cerebras-project && prompt-ops migrate --config config.yaml --api-key-env CEREBRAS_API_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-irY9CxS470"
      },
      "source": [
        "The optimization process will take a few minutes to complete. It involves:\n",
        "1. Loading your system prompt and dataset\n",
        "2. Analyzing the prompt structure and content\n",
        "3. Applying optimization techniques specific to Llama models\n",
        "4. Evaluating the optimized prompt against the original prompt\n",
        "5. Saving the optimized prompt to the `results/` directory\n",
        "\n",
        "Let's check the results directory to see the optimized prompt. If the optimizer successfully found a better prompt, it will be saved in the `results/` directory. You may need to run the optimization process again with different parameters or a larger dataset if the prompt is the same as the original."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ev_rs8FBS471",
        "outputId": "66022d01-c4ee-4d94-f546-b1d72820760b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 32\n",
            "drwxr-xr-x 2 root root  4096 Oct 28 10:22 .\n",
            "drwxr-xr-x 6 root root  4096 Oct 28 10:20 ..\n",
            "-rw-r--r-- 1 root root 10901 Oct 28 10:22 config_20251028_102005.json\n",
            "-rw-r--r-- 1 root root 11583 Oct 28 10:22 config_20251028_102005.yaml\n"
          ]
        }
      ],
      "source": [
        "!ls -la my-cerebras-project/results/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB6ed5LoS471"
      },
      "source": [
        "The optimized prompt is saved as a YAML file with a timestamp. Let's examine the contents of the optimized prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d85h-i_gS471",
        "outputId": "de4d4b9a-08f4-4736-feb2-736de65a015a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <style>\n",
              "        .prompt-container {\n",
              "            margin: 20px 0;\n",
              "            font-family: 'Courier New', monospace;\n",
              "        }\n",
              "        .prompt-box {\n",
              "            background-color: #000000;\n",
              "            color: #ffffff;\n",
              "            border: 2px solid #444;\n",
              "            border-radius: 8px;\n",
              "            padding: 20px;\n",
              "            margin: 10px 0;\n",
              "            white-space: pre-wrap;\n",
              "            word-wrap: break-word;\n",
              "            max-width: 100%;\n",
              "            overflow-wrap: break-word;\n",
              "        }\n",
              "        .prompt-header {\n",
              "            font-size: 18px;\n",
              "            font-weight: bold;\n",
              "            color: #ffffff;\n",
              "            margin-bottom: 10px;\n",
              "            padding: 10px;\n",
              "            background-color: #1a1a1a;\n",
              "            border-radius: 5px;\n",
              "        }\n",
              "        .optimized {\n",
              "            border-left: 5px solid #4CAF50;\n",
              "        }\n",
              "    </style>\n",
              "    \n",
              "    <div class=\"prompt-container\">\n",
              "        <div class=\"prompt-header\">✨ Optimized System Prompt (from config_20251028_102005.yaml)</div>\n",
              "        <div class=\"prompt-box optimized\">You are a skilled customer support analyst for ProCare Facility Solutions. Read the customer's message carefully and extract the relevant information to provide a response in the following format: {\"categories\": {\"emergency_repair_services\": false, \"routine_maintenance_requests\": false, \"quality_and_safety_concerns\": false, \"specialized_cleaning_services\": false, \"general_inquiries\": false, \"sustainability_and_environmental_practices\": false, \"training_and_support_requests\": false, \"cleaning_services_scheduling\": false, \"customer_feedback_and_complaints\": false, \"facility_management_issues\": false}, \"sentiment\": \"neutral\", \"urgency\": \"low\"} where the values for categories are set to true or false based on the content of the message, sentiment is one of \"negative\", \"neutral\", \"positive\", and urgency is one of \"high\", \"medium\", \"low\". Your response should be a valid json string.\n",
              "\n",
              "        Few-shot examples:\n",
              "            \n",
              "                Example 1:\n",
              "                    Question: Subject: Request for Training and Support on Facility Management Best Practices\n",
              "        \n",
              "        Dear ProCare Support Team,\n",
              "                    [...truncated 16 more lines...]\n",
              "                    Answer: {\"categories\": {\"routine_maintenance_requests\": false, \"customer_feedback_and_complaints\": false, \"t... [truncated]\n",
              "            Example 2:\n",
              "                    Question: Subject: Inquiry on Sustainability Practices\n",
              "        \n",
              "        Dear ProCare Support Team,\n",
              "                    [...truncated 14 more lines...]\n",
              "                    Answer: {\"categories\": {\"routine_maintenance_requests\": false, \"customer_feedback_and_complaints\": false, \"t... [truncated]\n",
              "            Example 3:\n",
              "                    Question: Subject: Inquiry Regarding Facility Management Coordination\n",
              "        \n",
              "        Dear ProCare Facility Solutions Support Team,\n",
              "                    [...truncated 14 more lines...]\n",
              "                    Answer: {\"categories\": {\"routine_maintenance_requests\": false, \"customer_feedback_and_complaints\": false, \"t... [truncated]\n",
              "            Example 4:\n",
              "                    Question: Subject: Guidance Needed for Routine Plumbing Maintenance\n",
              "        \n",
              "        Dear ProCare Support Team,\n",
              "                    [...truncated 14 more lines...]\n",
              "                    Answer: {\"categories\": {\"routine_maintenance_requests\": true, \"customer_feedback_and_complaints\": false, \"tr... [truncated]\n",
              "            Example 5:\n",
              "                    Question: Subject: Scheduling Cleaning Services\n",
              "        \n",
              "        Hey ProCare Team,\n",
              "                    [...truncated 11 more lines...]\n",
              "                    Answer: {\"categories\": {\"routine_maintenance_requests\": false, \"customer_feedback_and_complaints\": false, \"t... [truncated]\n",
              "            </div>\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import glob\n",
        "import os\n",
        "import re\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def truncate_examples(text, max_question_lines=3, max_answer_chars=100):\n",
        "    \"\"\"\n",
        "    Truncates few-shot examples to make them more readable.\n",
        "    Shows only the first few lines of questions and truncates answers.\n",
        "    \"\"\"\n",
        "    def truncate_section(match):\n",
        "        full_text = match.group(0)\n",
        "\n",
        "        # Extract question part\n",
        "        question_match = re.search(r'Question:(.*?)Answer:', full_text, re.DOTALL)\n",
        "        answer_match = re.search(r'Answer:(.*?)(?=Example \\d+:|$)', full_text, re.DOTALL)\n",
        "\n",
        "        result = match.group(1)  # Keep \"Example N:\"\n",
        "\n",
        "        if question_match:\n",
        "            question = question_match.group(1).strip()\n",
        "            lines = question.split('\\n')\n",
        "            if len(lines) > max_question_lines:\n",
        "                truncated = '\\n'.join(lines[:max_question_lines])\n",
        "                result += f\"\\n                    Question: {truncated}\\n                    [...truncated {len(lines) - max_question_lines} more lines...]\"\n",
        "            else:\n",
        "                result += f\"\\n                    Question: {question}\"\n",
        "\n",
        "        if answer_match:\n",
        "            answer = answer_match.group(1).strip()\n",
        "            if len(answer) > max_answer_chars:\n",
        "                result += f\"\\n                    Answer: {answer[:max_answer_chars]}... [truncated]\"\n",
        "            else:\n",
        "                result += f\"\\n                    Answer: {answer}\"\n",
        "\n",
        "        return result + \"\\n            \"\n",
        "\n",
        "    # Find and truncate each example\n",
        "    pattern = r'(Example \\d+:)(.*?)(?=Example \\d+:|$)'\n",
        "    truncated = re.sub(pattern, truncate_section, text, flags=re.DOTALL)\n",
        "\n",
        "    return truncated\n",
        "\n",
        "\n",
        "result_files = glob.glob('my-cerebras-project/results/*.yaml')\n",
        "if result_files:\n",
        "    latest_result = max(result_files, key=os.path.getctime)\n",
        "\n",
        "    # Simple approach: just read and split\n",
        "    with open(latest_result, 'r') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    # Split on 'config:' and take the first part (the system prompt)\n",
        "    system_section = content.split('config:')[0]\n",
        "    system_prompt = system_section.replace('system: |-', '').strip()\n",
        "\n",
        "    # Truncate the examples\n",
        "    system_prompt_truncated = truncate_examples(system_prompt)\n",
        "\n",
        "    # Create HTML output with word wrapping (dark theme)\n",
        "    html_output = f\"\"\"\n",
        "    <style>\n",
        "        .prompt-container {{\n",
        "            margin: 20px 0;\n",
        "            font-family: 'Courier New', monospace;\n",
        "        }}\n",
        "        .prompt-box {{\n",
        "            background-color: #000000;\n",
        "            color: #ffffff;\n",
        "            border: 2px solid #444;\n",
        "            border-radius: 8px;\n",
        "            padding: 20px;\n",
        "            margin: 10px 0;\n",
        "            white-space: pre-wrap;\n",
        "            word-wrap: break-word;\n",
        "            max-width: 100%;\n",
        "            overflow-wrap: break-word;\n",
        "        }}\n",
        "        .prompt-header {{\n",
        "            font-size: 18px;\n",
        "            font-weight: bold;\n",
        "            color: #ffffff;\n",
        "            margin-bottom: 10px;\n",
        "            padding: 10px;\n",
        "            background-color: #1a1a1a;\n",
        "            border-radius: 5px;\n",
        "        }}\n",
        "        .optimized {{\n",
        "            border-left: 5px solid #4CAF50;\n",
        "        }}\n",
        "    </style>\n",
        "\n",
        "    <div class=\"prompt-container\">\n",
        "        <div class=\"prompt-header\">Optimized System Prompt (from {os.path.basename(latest_result)})</div>\n",
        "        <div class=\"prompt-box optimized\">{system_prompt_truncated.replace('<', '&lt;').replace('>', '&gt;')}</div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "    display(HTML(html_output))\n",
        "else:\n",
        "    print(\"No result files found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSQVtP-wS471"
      },
      "source": [
        "## 5. Analyzing the Results\n",
        "\n",
        "Let's compare the original prompt with the optimized prompt to understand the changes made during optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vSGmfivKS471",
        "outputId": "970a39bc-cbb5-4331-a4c2-884a6da60287"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <style>\n",
              "        .prompt-container {\n",
              "            margin: 20px 0;\n",
              "            font-family: 'Courier New', monospace;\n",
              "        }\n",
              "        .prompt-box {\n",
              "            background-color: #000000;\n",
              "            color: #ffffff;\n",
              "            border: 2px solid #444;\n",
              "            border-radius: 8px;\n",
              "            padding: 20px;\n",
              "            margin: 10px 0;\n",
              "            white-space: pre-wrap;\n",
              "            word-wrap: break-word;\n",
              "            max-width: 100%;\n",
              "            overflow-wrap: break-word;\n",
              "        }\n",
              "        .prompt-header {\n",
              "            font-size: 18px;\n",
              "            font-weight: bold;\n",
              "            color: #ffffff;\n",
              "            margin-bottom: 10px;\n",
              "            padding: 10px;\n",
              "            background-color: #1a1a1a;\n",
              "            border-radius: 5px;\n",
              "        }\n",
              "        .original {\n",
              "            border-left: 5px solid #2196F3;\n",
              "        }\n",
              "        .optimized {\n",
              "            border-left: 5px solid #4CAF50;\n",
              "        }\n",
              "    </style>\n",
              "    \n",
              "    <div class=\"prompt-container\">\n",
              "        <div class=\"prompt-header\">📝 Original Prompt</div>\n",
              "        <div class=\"prompt-box original\">You are a helpful assistant. Extract and return a json with the following keys and values:\n",
              "- \"urgency\" as one of `high`, `medium`, `low`\n",
              "- \"sentiment\" as one of `negative`, `neutral`, `positive`\n",
              "- \"categories\" Create a dictionary with categories as keys and boolean values (True/False), where the value indicates whether the category is one of the best matching support category tags from: `emergency_repair_services`, `routine_maintenance_requests`, `quality_and_safety_concerns`, `specialized_cleaning_services`, `general_inquiries`, `sustainability_and_environmental_practices`, `training_and_support_requests`, `cleaning_services_scheduling`, `customer_feedback_and_complaints`, `facility_management_issues`\n",
              "Your complete message should be a valid json string that can be read directly and only contain the keys mentioned in the list above. Never enclose it in ```json...```, no newlines, no unnessacary whitespaces.\n",
              "</div>\n",
              "        \n",
              "        <div class=\"prompt-header\">✨ Optimized Prompt</div>\n",
              "        <div class=\"prompt-box optimized\">You are a skilled customer support analyst for ProCare Facility Solutions. Read the customer's message carefully and extract the relevant information to provide a response in the following format: {\"categories\": {\"emergency_repair_services\": false, \"routine_maintenance_requests\": false, \"quality_and_safety_concerns\": false, \"specialized_cleaning_services\": false, \"general_inquiries\": false, \"sustainability_and_environmental_practices\": false, \"training_and_support_requests\": false, \"cleaning_services_scheduling\": false, \"customer_feedback_and_complaints\": false, \"facility_management_issues\": false}, \"sentiment\": \"neutral\", \"urgency\": \"low\"} where the values for categories are set to true or false based on the content of the message, sentiment is one of \"negative\", \"neutral\", \"positive\", and urgency is one of \"high\", \"medium\", \"low\". Your response should be a valid json string.\n",
              "\n",
              "        Few-shot examples:\n",
              "            \n",
              "                Example 1:\n",
              "                    Question: Subject: Request for Training and Support on Facility Management Best Practices\n",
              "        \n",
              "        Dear ProCare Support Team,\n",
              "                    [...truncated 16 more lines...]\n",
              "                    Answer: {\"categories\": {\"routine_maintenance_requests\": false, \"customer_feedback_and_complaints\": false, \"t... [truncated]\n",
              "            Example 2:\n",
              "                    Question: Subject: Inquiry on Sustainability Practices\n",
              "        \n",
              "        Dear ProCare Support Team,\n",
              "                    [...truncated 14 more lines...]\n",
              "                    Answer: {\"categories\": {\"routine_maintenance_requests\": false, \"customer_feedback_and_complaints\": false, \"t... [truncated]\n",
              "            Example 3:\n",
              "                    Question: Subject: Inquiry Regarding Facility Management Coordination\n",
              "        \n",
              "        Dear ProCare Facility Solutions Support Team,\n",
              "                    [...truncated 14 more lines...]\n",
              "                    Answer: {\"categories\": {\"routine_maintenance_requests\": false, \"customer_feedback_and_complaints\": false, \"t... [truncated]\n",
              "            Example 4:\n",
              "                    Question: Subject: Guidance Needed for Routine Plumbing Maintenance\n",
              "        \n",
              "        Dear ProCare Support Team,\n",
              "                    [...truncated 14 more lines...]\n",
              "                    Answer: {\"categories\": {\"routine_maintenance_requests\": true, \"customer_feedback_and_complaints\": false, \"tr... [truncated]\n",
              "            Example 5:\n",
              "                    Question: Subject: Scheduling Cleaning Services\n",
              "        \n",
              "        Hey ProCare Team,\n",
              "                    [...truncated 11 more lines...]\n",
              "                    Answer: {\"categories\": {\"routine_maintenance_requests\": false, \"customer_feedback_and_complaints\": false, \"t... [truncated]\n",
              "            </div>\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import glob\n",
        "import os\n",
        "import re\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def truncate_examples(text, max_question_lines=3, max_answer_chars=100):\n",
        "    \"\"\"\n",
        "    Truncates few-shot examples to make them more readable.\n",
        "    Shows only the first few lines of questions and truncates answers.\n",
        "    \"\"\"\n",
        "    def truncate_section(match):\n",
        "        full_text = match.group(0)\n",
        "\n",
        "        # Extract question part\n",
        "        question_match = re.search(r'Question:(.*?)Answer:', full_text, re.DOTALL)\n",
        "        answer_match = re.search(r'Answer:(.*?)(?=Example \\d+:|$)', full_text, re.DOTALL)\n",
        "\n",
        "        result = match.group(1)  # Keep \"Example N:\"\n",
        "\n",
        "        if question_match:\n",
        "            question = question_match.group(1).strip()\n",
        "            lines = question.split('\\n')\n",
        "            if len(lines) > max_question_lines:\n",
        "                truncated = '\\n'.join(lines[:max_question_lines])\n",
        "                result += f\"\\n                    Question: {truncated}\\n                    [...truncated {len(lines) - max_question_lines} more lines...]\"\n",
        "            else:\n",
        "                result += f\"\\n                    Question: {question}\"\n",
        "\n",
        "        if answer_match:\n",
        "            answer = answer_match.group(1).strip()\n",
        "            if len(answer) > max_answer_chars:\n",
        "                result += f\"\\n                    Answer: {answer[:max_answer_chars]}... [truncated]\"\n",
        "            else:\n",
        "                result += f\"\\n                    Answer: {answer}\"\n",
        "\n",
        "        return result + \"\\n            \"\n",
        "\n",
        "    # Find and truncate each example\n",
        "    pattern = r'(Example \\d+:)(.*?)(?=Example \\d+:|$)'\n",
        "    truncated = re.sub(pattern, truncate_section, text, flags=re.DOTALL)\n",
        "\n",
        "    return truncated\n",
        "\n",
        "\n",
        "# Load the original prompt\n",
        "with open('my-cerebras-project/prompts/prompt.txt', 'r') as f:\n",
        "    original_prompt = f.read()\n",
        "\n",
        "# Find the most recent result file\n",
        "result_files = glob.glob('my-cerebras-project/results/*.yaml')\n",
        "if result_files:\n",
        "    latest_result = max(result_files, key=os.path.getctime)\n",
        "\n",
        "    # Load the optimized prompt using simple text parsing\n",
        "    with open(latest_result, 'r') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    # Split on 'config:' and take the first part (the system prompt)\n",
        "    system_section = content.split('config:')[0]\n",
        "    optimized_prompt = system_section.replace('system: |-', '').strip()\n",
        "\n",
        "    # Truncate examples in both prompts\n",
        "    original_truncated = truncate_examples(original_prompt)\n",
        "    optimized_truncated = truncate_examples(optimized_prompt)\n",
        "\n",
        "    # Create HTML output with word wrapping (dark theme)\n",
        "    html_output = f\"\"\"\n",
        "    <style>\n",
        "        .prompt-container {{\n",
        "            margin: 20px 0;\n",
        "            font-family: 'Courier New', monospace;\n",
        "        }}\n",
        "        .prompt-box {{\n",
        "            background-color: #000000;\n",
        "            color: #ffffff;\n",
        "            border: 2px solid #444;\n",
        "            border-radius: 8px;\n",
        "            padding: 20px;\n",
        "            margin: 10px 0;\n",
        "            white-space: pre-wrap;\n",
        "            word-wrap: break-word;\n",
        "            max-width: 100%;\n",
        "            overflow-wrap: break-word;\n",
        "        }}\n",
        "        .prompt-header {{\n",
        "            font-size: 18px;\n",
        "            font-weight: bold;\n",
        "            color: #ffffff;\n",
        "            margin-bottom: 10px;\n",
        "            padding: 10px;\n",
        "            background-color: #1a1a1a;\n",
        "            border-radius: 5px;\n",
        "        }}\n",
        "        .original {{\n",
        "            border-left: 5px solid #2196F3;\n",
        "        }}\n",
        "        .optimized {{\n",
        "            border-left: 5px solid #4CAF50;\n",
        "        }}\n",
        "    </style>\n",
        "\n",
        "    <div class=\"prompt-container\">\n",
        "        <div class=\"prompt-header\">Original Prompt</div>\n",
        "        <div class=\"prompt-box original\">{original_truncated.replace('<', '&lt;').replace('>', '&gt;')}</div>\n",
        "\n",
        "        <div class=\"prompt-header\">Optimized Prompt</div>\n",
        "        <div class=\"prompt-box optimized\">{optimized_truncated.replace('<', '&lt;').replace('>', '&gt;')}</div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "    display(HTML(html_output))\n",
        "else:\n",
        "    print(\"No result files found. Make sure the optimization process completed successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk_Ay97PS471"
      },
      "source": [
        "### Key Differences in the Optimized Prompt\n",
        "\n",
        "The optimized prompt typically includes several improvements:\n",
        "\n",
        "1. **Better Structure**: Llama models respond better to clear, structured instructions\n",
        "2. **Llama-Specific Formatting**: Formatting that works better with Llama's training patterns\n",
        "3. **Few-Shot Examples**: Examples that help the model understand the expected output format\n",
        "4. **Clear Output Expectations**: More explicit instructions about what the output should look like\n",
        "\n",
        "These changes can significantly improve the model's performance on your specific task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fh2ZarMS471"
      },
      "source": [
        "## 6. Advanced Usage and Customization\n",
        "\n",
        "### Using Your Own Data\n",
        "\n",
        "To use your own data with Prompt ops, you'll need to:\n",
        "\n",
        "1. Prepare your dataset in JSON format\n",
        "2. Create a system prompt file\n",
        "3. Create a configuration file\n",
        "\n",
        "Check out the comprehensive guide [here](https://github.com/meta-llama/prompt-ops/tree/main/docs) to learn more.\n",
        "\n",
        "Now, let's see how to create a custom configuration file:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T__RuigGS471",
        "outputId": "92574af1-bf55-4782-8810-9dcb04c8c027"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing custom_config.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile custom_config.yaml\n",
        "system_prompt:\n",
        "  file: \"path/to/your/prompt.txt\"\n",
        "  inputs: [\"question\"]\n",
        "  outputs: [\"answer\"]\n",
        "\n",
        "# Dataset configuration\n",
        "dataset:\n",
        "  path: \"path/to/your/dataset.json\"\n",
        "  input_field: \"question\"  # or [\"fields\", \"input\"] for nested fields\n",
        "  golden_output_field: \"answer\"\n",
        "\n",
        "# Model configuration\n",
        "model:\n",
        "  # Cerebras Inference with Llama 3.3 70B\n",
        "  task_model: \"cerebras/llama3.3-70b-instruct\"\n",
        "  proposer_model: \"cerebras/llama3.3-70b-instruct\"\n",
        "\n",
        "  # Cerebras API endpoint\n",
        "  api_base: \"https://api.cerebras.ai/v1\"\n",
        "\n",
        "  # Generation settings\n",
        "  temperature: 0.0\n",
        "  max_tokens: 4096\n",
        "\n",
        "# Metric configuration\n",
        "metric:\n",
        "  class: \"prompt_ops.core.metrics.StandardJSONMetric\"\n",
        "  strict_json: false\n",
        "  output_field: \"answer\"\n",
        "\n",
        "# Optimization settings\n",
        "optimization:\n",
        "  strategy: \"llama\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PanA0BdES471"
      },
      "source": [
        "### Using Different Metrics\n",
        "\n",
        "Prompt ops supports different metrics for evaluating prompt performance. The default is `StandardJSONMetric`, but you can use other metrics like `FacilityMetric` for specific use cases.\n",
        "\n",
        "Here's an example of using the `FacilityMetric` for the facility support analyzer use case:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aX43LY7S471",
        "outputId": "312e311c-b27f-4523-a57a-0a21348b3172"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing facility_config.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile facility_config.yaml\n",
        "system_prompt:\n",
        "  file: \"prompts/facility_prompt.txt\"\n",
        "  inputs: [\"question\"]\n",
        "  outputs: [\"answer\"]\n",
        "\n",
        "# Dataset configuration\n",
        "dataset:\n",
        "  path: \"data/facility_dataset.json\"\n",
        "  input_field: [\"fields\", \"input\"]\n",
        "  golden_output_field: \"answer\"\n",
        "\n",
        "# Model configuration\n",
        "model:\n",
        "  # Cerebras Inference with Llama 3.3 70B\n",
        "  task_model: \"cerebras/llama3.3-70b-instruct\"\n",
        "  proposer_model: \"cerebras/llama3.3-70b-instruct\"\n",
        "\n",
        "  # Cerebras API endpoint\n",
        "  api_base: \"https://api.cerebras.ai/v1\"\n",
        "\n",
        "  # Generation settings\n",
        "  temperature: 0.0\n",
        "  max_tokens: 4096\n",
        "\n",
        "# Metric configuration\n",
        "metric:\n",
        "  class: \"prompt_ops.core.metrics.FacilityMetric\"\n",
        "  strict_json: false\n",
        "  output_field: \"answer\"\n",
        "\n",
        "# Optimization settings\n",
        "optimization:\n",
        "  strategy: \"llama\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPahjhjwS471"
      },
      "source": [
        "### Using Different Models\n",
        "\n",
        "Prompt ops supports different models through various inference providers. You can use Cerebras, OpenRouter, vLLM, or NVIDIA NIMs depending on your infrastructure needs.\n",
        "\n",
        "With Cerebras, you have access to ultra-fast Llama models. Here's an example configuration using a different Llama model size:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSwow1SqS471",
        "outputId": "bb6f8a3d-43de-4b01-a2ed-64bd17e00a89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing different_model_config.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile different_model_config.yaml\n",
        "system_prompt:\n",
        "  file: \"prompts/prompt.txt\"\n",
        "  inputs: [\"question\"]\n",
        "  outputs: [\"answer\"]\n",
        "\n",
        "# Dataset configuration\n",
        "dataset:\n",
        "  path: \"data/dataset.json\"\n",
        "  input_field: \"question\"\n",
        "  golden_output_field: \"answer\"\n",
        "\n",
        "# Model configuration\n",
        "model:\n",
        "  # Using Llama 3.1 8B for faster, cost-effective optimization\n",
        "  task_model: \"cerebras/llama3.1-8b-instruct\"\n",
        "  proposer_model: \"cerebras/llama3.1-8b-instruct\"\n",
        "\n",
        "  # Cerebras API endpoint\n",
        "  api_base: \"https://api.cerebras.ai/v1\"\n",
        "\n",
        "  # Generation settings\n",
        "  temperature: 0.0\n",
        "  max_tokens: 4096\n",
        "\n",
        "# Metric configuration\n",
        "metric:\n",
        "  class: \"prompt_ops.core.metrics.StandardJSONMetric\"\n",
        "  strict_json: false\n",
        "  output_field: \"answer\"\n",
        "\n",
        "# Optimization settings\n",
        "optimization:\n",
        "  strategy: \"llama\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnsTBBrDS471"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we've covered:\n",
        "\n",
        "1. Introduction to Prompt ops and its benefits\n",
        "2. Creating a sample project\n",
        "3. Setting up your environment and Cerebras API key\n",
        "4. Running prompt optimization with Cerebras Inference\n",
        "5. Analyzing the results\n",
        "6. Advanced usage and customization options\n",
        "\n",
        "Prompt ops provides a powerful way to optimize your prompts for Llama models, improving performance and reliability. By using Cerebras Inference API, you get ultra-fast inference speeds that make prompt optimization rapid and efficient.\n",
        "\n",
        "### Key Takeaways with Cerebras\n",
        "\n",
        "- **Ultra-Fast Inference**: Cerebras provides exceptional inference speed for Llama models\n",
        "- **Easy Integration**: Simply configure the API endpoint and model name\n",
        "- **Production Ready**: Cerebras scales from development to production seamlessly\n",
        "\n",
        "By following the steps in this notebook, you can start optimizing your own prompts with Cerebras and building more effective LLM applications.\n",
        "\n",
        "For more information:\n",
        "- [prompt-ops documentation](https://github.com/meta-llama/prompt-ops/tree/main/docs)\n",
        "- [Cerebras Inference API](https://cerebras.ai/)\n",
        "- Explore example use cases in the prompt-ops repository"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llama-prompt-ops",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
