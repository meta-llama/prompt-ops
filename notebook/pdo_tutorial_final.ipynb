{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "All rights reserved.\n",
    "\n",
    "This source code is licensed under the terms described in the LICENSE file in\n",
    "the root directory of this source tree.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/meta-llama/llama-cookbook/blob/main/getting-started/llama-tools/pdo_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Getting Started with PDO (Prompt Duel Optimizer) with prompt-ops\n",
    "\n",
    "This tutorial will guide you through using PDO with prompt-ops to optimize prompts for Llama models through competitive dueling. We'll cover:\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction to PDO](#1-introduction-to-pdo)\n",
    "   - The Problem PDO Solves\n",
    "   - Key Innovations\n",
    "   - Comparison with Other Methods\n",
    "\n",
    "2. [PDO Architecture Deep Dive](#2-pdo-architecture-deep-dive)\n",
    "   - Core Components from the Paper\n",
    "   - Dueling Bandits Framework\n",
    "   - Thompson Sampling\n",
    "   - Multi-Ranker Fusion\n",
    "\n",
    "3. [Creating a PDO Project](#3-creating-a-pdo-project)\n",
    "   - Project Structure\n",
    "   - Configuration Essentials\n",
    "   - Dataset Preparation\n",
    "\n",
    "4. [Running PDO Optimization](#4-running-pdo-optimization)\n",
    "   - The Optimization Process\n",
    "   - Understanding the Output\n",
    "   - Common Parameters\n",
    "\n",
    "5. [Analyzing Results](#5-analyzing-results)\n",
    "   - Interpreting Optimized Prompts\n",
    "   - Performance Metrics\n",
    "   - Duel Statistics\n",
    "\n",
    "\n",
    "Before we dive into the theory, a quick note: if you're the type who learns best by doing, we've prepared an interactive notebook (link to notebook) that demonstrates PDO concepts through runnable code. Feel free to start there and reference back to this guide as needed.\n",
    "\n",
    "Now, let's explore what makes PDO special...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to PDO\n",
    "\n",
    "### The Problem PDO Solves\n",
    "\n",
    "Traditional prompt optimization methods face several critical challenges:\n",
    "\n",
    "1. **Absolute Scoring Bias**: Single-point evaluation can be misleading - is a 0.85 score truly better than 0.82?\n",
    "2. **Limited Exploration**: Greedy approaches miss superior prompts in unexplored regions\n",
    "3. **Ranking Uncertainty**: Different metrics may rank prompts inconsistently\n",
    "4. **Exploitation vs Exploration**: How to balance trying new prompts vs refining known good ones?\n",
    "\n",
    "PDO addresses these limitations through a **dueling bandit approach** that treats prompt optimization as a competitive head-to-head tournament rather than absolute scoring.\n",
    "\n",
    "#### **Design Choice: Pairwise vs. Pointwise Evaluation**  \n",
    "PDO uses **pairwise comparison** even when labels are availableâ€”trading computational efficiency for more robust prompt evaluation.\n",
    "\n",
    "### Key Innovations from the Paper\n",
    "\n",
    "The paper explores several novel concepts in prompt optimization\n",
    "\n",
    "1. **Dueling Bandits**: Prompts compete in pairwise comparisons, not absolute rankings\n",
    "2. **Thompson Sampling**: Probabilistic exploration using Beta distributions for smart duel selection\n",
    "3. **Multi-Ranker Fusion**: Combines multiple ranking algorithms (Copeland, Borda, Elo, TrueSkill) for robust evaluation\n",
    "4. **Variance-Driven Exploration**: Selects opponent based on uncertainty, maximizing information gain\n",
    "5. **LLM-as-Judge**: Uses language models to evaluate which prompt produces better responses\n",
    "6. **Adaptive Pruning**: Removes consistently poor performers while maintaining diversity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with Other Methods\n",
    "\n",
    "The paper compares PDO with several baseline approaches:\n",
    "\n",
    "| Method | Approach | Limitations | PDO's Advantage |\n",
    "|--------|----------|-------------|-----------------|\n",
    "| **Manual Prompting** | Human-written instructions | Time-consuming, subjective | Automated competitive optimization |\n",
    "| **Few-shot Learning** | Examples in prompt | Limited by context window | Optimizes instructions through duels |\n",
    "| **DSPy MIPRO** | Baysien-based optimization | Local optima, absolute scoring | Global exploration via bandits |\n",
    "| **OPRO** | LLM-based optimization | Greedy, expensive | Efficient exploration via Thompson sampling |\n",
    "\n",
    "### The Dueling Advantage\n",
    "\n",
    "The paper's key insight is that **relative comparisons are more reliable than absolute scores**. Dueling bandits are well-suited for prompt optimization because they:\n",
    "\n",
    "- Focus on what matters: \"Which prompt is better?\" not \"What's the exact score?\"\n",
    "- Maintain diversity through probabilistic selection\n",
    "- Balance exploration (trying new prompts) and exploitation (refining winners)\n",
    "- Aggregate multiple ranking systems for robustness\n",
    "- Scale efficiently with the number of prompts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the PDO Advantage\n",
    "\n",
    "To understand why PDO's dueling approach is superior, let's compare it with traditional optimization:\n",
    "\n",
    "**The Traditional Problem:**\n",
    "When you score prompts independently (e.g., P4 gets 0.78, P5 gets 0.74), small differences might just be noise. Is P4 *really* better, or did it just get lucky on the test set? You can't be sure.\n",
    "\n",
    "**PDO's Solution:**\n",
    "Instead of asking \"How good is this prompt?\" (absolute), PDO asks \"Which prompt is better?\" (relative). By running head-to-head duels on the same examples, PDO eliminates scoring bias and reveals consistent winners.\n",
    "\n",
    "**The Visualization Below Shows:**\n",
    "- **Left**: Traditional optimization relies on absolute scores that may be unreliable\n",
    "- **Right**: PDO builds a **win matrix** where each cell shows how often one prompt beats another in head-to-head duels\n",
    "\n",
    "**Key Insight:** A prompt that wins 60%+ of duels against *every* opponent is a validated champion. This is PDO's core innovation: **reliable relative comparison through competition**.\n",
    "\n",
    "![Point vs Pairwise Comparison](images/point-vs-pairwise.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- PDO discovers prompt quality through competitive duels,\n",
    "- âœ“ P4 dominates: wins 60%+ against ALL opponents (validated through competition)\n",
    "- where consistent winners emerge naturally from head-to-head comparisons.\n",
    "- \\nKey Insight: P4's superiority is validated across ALL matchups, not just by a single score that might be noise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PDO Architecture Deep Dive\n",
    "\n",
    "This section explores the core components of PDO as described in the research paper, focusing on the theoretical foundations and algorithmic innovations.\n",
    "\n",
    "### Core Components from the Paper\n",
    "\n",
    "PDO consists of six main components:\n",
    "\n",
    "1. **Instruction Pool (ð’«)**: Collection of candidate prompts competing for superiority\n",
    "2. **Win Matrix (W)**: Tracks head-to-head results between all prompt pairs\n",
    "3. **Thompson Sampler**: Selects duel pairs using probabilistic exploration\n",
    "4. **LLM Judge**: Evaluates which prompt produces better responses\n",
    "5. **Multi-Ranker System**: Aggregates multiple ranking algorithms for robust evaluation\n",
    "6. **Instruction Evolution**: Generates new prompts by combining successful variants\n",
    "\n",
    "Let's explore each component in detail. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The Dueling Bandits Framework\n",
    "\n",
    "The paper introduces PDO's use of **Dueling Bandits** as the central optimization paradigm:\n",
    "\n",
    "**Traditional Multi-Armed Bandits**:\n",
    "- Pull an arm â†’ observe absolute reward\n",
    "- Problem: Reward scales may be unreliable or noisy\n",
    "\n",
    "**Dueling Bandits (PDO's Approach)**:\n",
    "- Select two prompts â†’ run head-to-head comparison\n",
    "- Observe relative preference: \"Which is better?\"\n",
    "- Advantage: Relative comparisons are more stable than absolute scores\n",
    "\n",
    "### The Algorithm Loop\n",
    "\n",
    "```\n",
    "Initialize:\n",
    "  - Start with base prompt + generate initial variations\n",
    "  - Initialize Win matrix W (all zeros)\n",
    "\n",
    "For each round t = 1 to T:\n",
    "  1. Use Thompson Sampling to select prompt pair (i, j)\n",
    "  2. Run duel: Both prompts answer the same examples\n",
    "  3. LLM judge decides winner on each example\n",
    "  4. Update W[i,j] with wins, W[j,i] with losses\n",
    "  5. Compute multi-ranker scores (Copeland, Borda, Elo, TrueSkill)\n",
    "  6. [Optional] Generate new prompts by combining top performers\n",
    "  7. [Optional] Prune worst-performing prompts\n",
    "\n",
    "Return:\n",
    "  - Best prompt according to aggregated rankings\n",
    "```\n",
    "\n",
    "**Key Innovation**: The Win matrix W becomes increasingly informative, guiding both exploration (via Thompson sampling) and exploitation (via ranking systems).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Thompson Sampling\n",
    "\n",
    "One of PDO's most sophisticated components is its **Double Thompson Sampling** strategy:\n",
    "\n",
    "#### What is Thompson Sampling?\n",
    "\n",
    "Thompson Sampling is a probabilistic approach to the exploration-exploitation dilemma:\n",
    "- Maintain a **belief distribution** about each prompt's quality\n",
    "- Sample from these distributions to select actions\n",
    "- Naturally balances trying new things vs. exploiting known winners\n",
    "\n",
    "#### PDO's Double Thompson Sampling\n",
    "\n",
    "The paper describes a two-stage selection process:\n",
    "\n",
    "**Stage 1: Select First Prompt**\n",
    "1. For each prompt pair (i,j), model win probability as Beta(wins+1, losses+1)\n",
    "2. Sample a win-rate matrix Î¸ from these Beta distributions\n",
    "3. Compute multiple rankings from Î¸ (Copeland, Borda, win-rate)\n",
    "4. Combine with Elo and TrueSkill ratings using **Dirichlet-weighted fusion**\n",
    "5. Apply softmax with temperature Ï„ for final selection\n",
    "\n",
    "**Stage 2: Select Second Prompt (Opponent)**\n",
    "1. Among remaining prompts, filter to those still plausibly competitive\n",
    "2. Select the one with **maximum variance** in win probability vs. first prompt\n",
    "3. Rationale: High variance = high uncertainty = maximum information gain from this duel\n",
    "\n",
    "**Benefits**:\n",
    "- Efficient exploration: Doesn't waste duels on clearly inferior prompts\n",
    "- Information-driven: Prioritizes duels that reduce uncertainty\n",
    "- Adaptive: Automatically shifts from exploration to exploitation as data accumulates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the Thompson Sampling Visualization\n",
    "\n",
    "The diagram below shows **Beta distributions** representing our belief about each prompt's win probability. This is the heart of how PDO decides which prompts to compare in duels:\n",
    "\n",
    "**What You're Looking At:**\n",
    "- Each plot shows a probability distribution over win rates (0 to 1 on x-axis)\n",
    "- The **height** (density) represents how confident we are about that win rate\n",
    "- The **width** represents uncertainty - wider = less certain\n",
    "\n",
    "**The Three Scenarios:**\n",
    "\n",
    "1. **Prompt A (Strong Performer - Green)**: 15 wins, 5 losses\n",
    "   - **Narrow, tall distribution** centered at 0.73\n",
    "   - High confidence: We're pretty sure this prompt wins ~73% of the time\n",
    "   - PDO interpretation: *Reliable choice, but already well-understood*\n",
    "\n",
    "2. **Prompt B (Average - Blue)**: 10 wins, 10 losses  \n",
    "   - **Medium width** distribution centered at 0.50\n",
    "   - Moderate confidence: Wins about half the time\n",
    "   - PDO interpretation: *Middle-of-the-pack, not particularly interesting*\n",
    "\n",
    "3. **Prompt C (Uncertain - Orange)**: 2 wins, 2 losses\n",
    "   - **Wide, flat distribution** centered at 0.50\n",
    "   - HIGH UNCERTAINTY: Could be anywhere from 0.2 to 0.8!\n",
    "   - PDO interpretation: *High information value - might be secretly excellent OR terrible*\n",
    "\n",
    "**PDO's Smart Strategy:**\n",
    "\n",
    "Thompson Sampling doesn't just pick the prompt with the highest mean (that would be A). Instead:\n",
    "\n",
    "1. **First prompt selection**: Sample from all distributions â†’ A likely selected (highest mean)\n",
    "2. **Second prompt selection**: Pick the opponent with **maximum variance** vs. the first\n",
    "   - Dueling A vs B tells us little (both well-understood)\n",
    "   - **Dueling A vs C tells us a LOT** (huge uncertainty about C)\n",
    "\n",
    "**The Key Insight:** By choosing C as the opponent, PDO maximizes **information gain**. After this duel:\n",
    "- If C wins â†’ Great! We discovered a hidden champion\n",
    "- If C loses â†’ Good! We eliminated uncertainty and can focus elsewhere\n",
    "\n",
    "This is why PDO converges faster than naive approaches - it strategically explores high-uncertainty regions rather than randomly sampling prompts.\n",
    "\n",
    "![Thompson Sampling Beta Distributions](images/thompson-sampling.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Multi-Ranker Fusion\n",
    "\n",
    "One of PDO's most important innovations is using **multiple ranking algorithms** simultaneously to create a more robust evaluation system.\n",
    "\n",
    "#### What is Multi-Ranker Fusion?\n",
    "\n",
    "Imagine you're trying to determine the best chess player. Would you:\n",
    "- **Option A**: Use only one ranking system (e.g., just count wins)\n",
    "- **Option B**: Combine multiple perspectives (wins, strength of opponents beaten, consistency, etc.)\n",
    "\n",
    "PDO chooses Option B - it **fuses** (combines) the opinions of 5 different ranking algorithms, each with its own strengths and blind spots. This is called **Multi-Ranker Fusion**.\n",
    "\n",
    "**The Core Problem:** No single ranking system is perfect. Each has biases:\n",
    "- Some favor consistent performers\n",
    "- Some reward crushing weak opponents\n",
    "- Some value beating strong opponents highly\n",
    "- Some are sensitive to the order of matches\n",
    "\n",
    "By combining all 5, PDO gets a **consensus view** that's more reliable than any individual ranker.\n",
    "\n",
    "#### Why Multiple Rankers?\n",
    "\n",
    "Different ranking systems capture different aspects of prompt quality:\n",
    "\n",
    "| Ranking System | What It Measures | Strengths | Weaknesses |\n",
    "|----------------|------------------|-----------|------------|\n",
    "| **Copeland** | Number of opponents beaten | Simple, intuitive | Ignores margin of victory |\n",
    "| **Borda** | Sum of win probabilities | Accounts for all matchups | Can be dominated by many weak wins |\n",
    "| **Average Win Rate** | Mean win probability | Easy to interpret | Doesn't account for opponent strength |\n",
    "| **Elo** | Chess-style rating | Accounts for opponent strength | Sensitive to order of matches |\n",
    "| **TrueSkill** | Bayesian skill rating | Confidence intervals, robust | More complex, computational cost |\n",
    "\n",
    "#### Fusion Strategy\n",
    "\n",
    "The paper describes a sophisticated fusion approach:\n",
    "\n",
    "```\n",
    "For each round:\n",
    "1. Compute all 5 ranking scores for each prompt\n",
    "2. Normalize each to [0, 1] range\n",
    "3. Sample fusion weights from Dirichlet(1,1,1,1,1)\n",
    "   â†’ Introduces randomness in how rankings are combined\n",
    "4. Compute weighted combination: score = Î£(weight_i Ã— ranking_i)\n",
    "5. Use fused scores for prompt selection\n",
    "```\n",
    "\n",
    "**What's Happening Here:**\n",
    "- **Step 3 is key**: Instead of fixed weights (e.g., always 20% each), PDO **randomly samples** weights each round\n",
    "- **Dirichlet distribution** ensures weights sum to 1.0 but vary each time\n",
    "- Example: One round might weight Elo heavily (0.4, 0.15, 0.15, 0.15, 0.15), next round Copeland (0.15, 0.4, 0.15, 0.15, 0.15)\n",
    "- This explores different \"philosophies\" of ranking over time\n",
    "\n",
    "**Benefits**:\n",
    "- **Robustness**: No single ranking system dominates\n",
    "- **Exploration**: Dirichlet sampling explores different ranking perspectives\n",
    "- **Consensus**: Prompts that rank well across ALL systems are truly superior\n",
    "\n",
    "#### Concrete Example\n",
    "\n",
    "Let's say we have 3 prompts after some duels:\n",
    "\n",
    "| Prompt | Copeland | Borda | Win Rate | Elo | TrueSkill | **Average** |\n",
    "|--------|----------|-------|----------|-----|-----------|-------------|\n",
    "| **P1** | 1st | 1st | 1st | 2nd | 1st | **1st (winner!)** |\n",
    "| **P2** | 2nd | 2nd | 3rd | 1st | 2nd | **2nd** |\n",
    "| **P3** | 3rd | 3rd | 2nd | 3rd | 3rd | **3rd** |\n",
    "\n",
    "**Key Insight**: P1 wins 4 out of 5 ranking systems - it's a **consensus champion**. Even though P2 tops the Elo ranking (maybe it beat one strong opponent), P1 is more consistently excellent across all evaluation criteria.\n",
    "\n",
    "This is why Multi-Ranker Fusion is powerful: **It prevents a prompt from gaming one specific metric** and ensures true, well-rounded superiority.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. LLM-as-Judge\n",
    "\n",
    "The paper emphasizes that PDO's evaluation mechanism is crucial for effective optimization:\n",
    "\n",
    "**Traditional Metrics**: Hard-coded rules (exact match, F1, BLEU, etc.)\n",
    "\n",
    "**PDO's LLM Judge**:\n",
    "- **Natural Language Evaluation**: Can assess nuanced qualities (helpfulness, clarity, correctness)\n",
    "- **Pairwise Comparison**: Given two responses, which is better and why?\n",
    "- **Flexible Criteria**: Can evaluate domain-specific quality without custom metric code\n",
    "- **Reasoning Output**: Provides explanation for decisions (useful for debugging)\n",
    "\n",
    "#### Judge Prompt Structure\n",
    "\n",
    "```\n",
    "You are an expert evaluator. Compare these two responses:\n",
    "\n",
    "Input: {question}\n",
    "Expected Answer: {label}\n",
    "\n",
    "Response A:\n",
    "{response_from_prompt_A}\n",
    "\n",
    "Response B:\n",
    "{response_from_prompt_B}\n",
    "\n",
    "Which response is better? Consider:\n",
    "- Correctness: Does it match the expected answer?\n",
    "- Completeness: Does it address all aspects?\n",
    "- Clarity: Is it well-structured and understandable?\n",
    "\n",
    "Output format:\n",
    "{\n",
    "  \"reasoning\": \"Your detailed comparison\",\n",
    "  \"winner\": \"A\" or \"B\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Benefits of LLM-as-Judge**:\n",
    "- Works for any task (classification, generation, reasoning)\n",
    "- Captures subtle quality differences\n",
    "- Scales to new domains without custom metric engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating a prompt-ops PDO Project\n",
    "\n",
    "Now that we understand the theoretical foundations, let's create a real PDO project using the MS MARCO dataset as our example.\n",
    "\n",
    "### Why MS MARCO?\n",
    "\n",
    "MS MARCO (Microsoft MAchine Reading COmprehension) is an excellent dataset for demonstrating PDO because:\n",
    "- It's a real-world open-ended QA task\n",
    "- Requires nuanced understanding and clear communication\n",
    "- Benefits from prompt optimization for answer quality\n",
    "- Shows measurable improvements with PDO\n",
    "\n",
    "### Getting Started with the MS MARCO Example\n",
    "\n",
    "We'll use a pre-configured MS MARCO PDO project that demonstrates best practices:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing prompt-ops\n",
    "\n",
    "Before we can work with the MS MARCO example, we need to install `prompt-ops`. You can install it from PyPI or from source for the latest features.\n",
    "\n",
    "\n",
    "#### Install from Source \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'prompt-ops'...\n",
      "remote: Enumerating objects: 1238, done.\u001b[K\n",
      "remote: Counting objects: 100% (128/128), done.\u001b[K\n",
      "remote: Compressing objects: 100% (67/67), done.\u001b[K\n",
      "remote: Total 1238 (delta 70), reused 104 (delta 58), pack-reused 1110 (from 2)\u001b[K\n",
      "Receiving objects: 100% (1238/1238), 1.30 MiB | 4.65 MiB/s, done.\n",
      "Resolving deltas: 100% (583/583), done.\n",
      "Obtaining file:///Users/justinai/Documents/Code/llama-prompt-ops/notebook/prompt-ops\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: click>=8.0.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from prompt-ops==0.0.9) (8.1.8)\n",
      "Requirement already satisfied: requests>=2.25.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from prompt-ops==0.0.9) (2.32.3)\n",
      "Requirement already satisfied: dspy==2.6.13 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from prompt-ops==0.0.9) (2.6.13)\n",
      "Requirement already satisfied: numpy>=2.2.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from prompt-ops==0.0.9) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.15.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from prompt-ops==0.0.9) (1.15.2)\n",
      "Requirement already satisfied: pandas>=2.2.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from prompt-ops==0.0.9) (2.2.3)\n",
      "Requirement already satisfied: python-dotenv>=1.0.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from prompt-ops==0.0.9) (1.1.0)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from prompt-ops==0.0.9) (6.0.2)\n",
      "Requirement already satisfied: tiktoken>=0.9.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from prompt-ops==0.0.9) (0.9.0)\n",
      "Requirement already satisfied: openai>=1.65.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from prompt-ops==0.0.9) (1.72.0)\n",
      "Requirement already satisfied: litellm>=1.63.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from prompt-ops==0.0.9) (1.65.4.post1)\n",
      "Requirement already satisfied: huggingface-hub>=0.29.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from prompt-ops==0.0.9) (0.30.2)\n",
      "Requirement already satisfied: datasets>=2.21.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from prompt-ops==0.0.9) (3.5.0)\n",
      "Requirement already satisfied: propcache==0.3.1 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from prompt-ops==0.0.9) (0.3.1)\n",
      "Requirement already satisfied: backoff in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from dspy==2.6.13->prompt-ops==0.0.9) (2.2.1)\n",
      "Requirement already satisfied: joblib~=1.3 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from dspy==2.6.13->prompt-ops==0.0.9) (1.4.2)\n",
      "Requirement already satisfied: regex in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from dspy==2.6.13->prompt-ops==0.0.9) (2024.11.6)\n",
      "Requirement already satisfied: ujson in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from dspy==2.6.13->prompt-ops==0.0.9) (5.10.0)\n",
      "Requirement already satisfied: tqdm in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from dspy==2.6.13->prompt-ops==0.0.9) (4.67.1)\n",
      "Requirement already satisfied: optuna in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from dspy==2.6.13->prompt-ops==0.0.9) (4.2.1)\n",
      "Requirement already satisfied: pydantic~=2.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from dspy==2.6.13->prompt-ops==0.0.9) (2.11.3)\n",
      "Requirement already satisfied: magicattr~=0.1.6 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from dspy==2.6.13->prompt-ops==0.0.9) (0.1.6)\n",
      "Requirement already satisfied: diskcache in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from dspy==2.6.13->prompt-ops==0.0.9) (5.6.3)\n",
      "Requirement already satisfied: json-repair in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from dspy==2.6.13->prompt-ops==0.0.9) (0.40.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from dspy==2.6.13->prompt-ops==0.0.9) (9.1.2)\n",
      "Requirement already satisfied: anyio in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from dspy==2.6.13->prompt-ops==0.0.9) (4.9.0)\n",
      "Requirement already satisfied: asyncer==0.0.8 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from dspy==2.6.13->prompt-ops==0.0.9) (0.0.8)\n",
      "Requirement already satisfied: cachetools in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from dspy==2.6.13->prompt-ops==0.0.9) (5.5.2)\n",
      "Requirement already satisfied: cloudpickle in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from dspy==2.6.13->prompt-ops==0.0.9) (3.1.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from anyio->dspy==2.6.13->prompt-ops==0.0.9) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from anyio->dspy==2.6.13->prompt-ops==0.0.9) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from anyio->dspy==2.6.13->prompt-ops==0.0.9) (1.3.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from anyio->dspy==2.6.13->prompt-ops==0.0.9) (4.13.1)\n",
      "Requirement already satisfied: aiohttp in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from litellm>=1.63.0->prompt-ops==0.0.9) (3.11.16)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from litellm>=1.63.0->prompt-ops==0.0.9) (0.28.1)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from litellm>=1.63.0->prompt-ops==0.0.9) (8.6.1)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from litellm>=1.63.0->prompt-ops==0.0.9) (3.1.6)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from litellm>=1.63.0->prompt-ops==0.0.9) (4.23.0)\n",
      "Requirement already satisfied: tokenizers in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from litellm>=1.63.0->prompt-ops==0.0.9) (0.21.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.63.0->prompt-ops==0.0.9) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.63.0->prompt-ops==0.0.9) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.63.0->prompt-ops==0.0.9) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.63.0->prompt-ops==0.0.9) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.63.0->prompt-ops==0.0.9) (0.24.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from pydantic~=2.0->dspy==2.6.13->prompt-ops==0.0.9) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from pydantic~=2.0->dspy==2.6.13->prompt-ops==0.0.9) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from pydantic~=2.0->dspy==2.6.13->prompt-ops==0.0.9) (0.4.0)\n",
      "Requirement already satisfied: filelock in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from datasets>=2.21.0->prompt-ops==0.0.9) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from datasets>=2.21.0->prompt-ops==0.0.9) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from datasets>=2.21.0->prompt-ops==0.0.9) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from datasets>=2.21.0->prompt-ops==0.0.9) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from datasets>=2.21.0->prompt-ops==0.0.9) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.21.0->prompt-ops==0.0.9) (2024.12.0)\n",
      "Requirement already satisfied: packaging in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from datasets>=2.21.0->prompt-ops==0.0.9) (24.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from aiohttp->litellm>=1.63.0->prompt-ops==0.0.9) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from aiohttp->litellm>=1.63.0->prompt-ops==0.0.9) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from aiohttp->litellm>=1.63.0->prompt-ops==0.0.9) (5.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from aiohttp->litellm>=1.63.0->prompt-ops==0.0.9) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from aiohttp->litellm>=1.63.0->prompt-ops==0.0.9) (6.4.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from aiohttp->litellm>=1.63.0->prompt-ops==0.0.9) (1.19.0)\n",
      "Requirement already satisfied: certifi in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from httpx>=0.23.0->litellm>=1.63.0->prompt-ops==0.0.9) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from httpx>=0.23.0->litellm>=1.63.0->prompt-ops==0.0.9) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.23.0->litellm>=1.63.0->prompt-ops==0.0.9) (0.14.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from importlib-metadata>=6.8.0->litellm>=1.63.0->prompt-ops==0.0.9) (3.21.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from openai>=1.65.0->prompt-ops==0.0.9) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from openai>=1.65.0->prompt-ops==0.0.9) (0.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from pandas>=2.2.0->prompt-ops==0.0.9) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from pandas>=2.2.0->prompt-ops==0.0.9) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from pandas>=2.2.0->prompt-ops==0.0.9) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=2.2.0->prompt-ops==0.0.9) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from requests>=2.25.0->prompt-ops==0.0.9) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from requests>=2.25.0->prompt-ops==0.0.9) (2.3.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from optuna->dspy==2.6.13->prompt-ops==0.0.9) (1.15.2)\n",
      "Requirement already satisfied: colorlog in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from optuna->dspy==2.6.13->prompt-ops==0.0.9) (6.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from optuna->dspy==2.6.13->prompt-ops==0.0.9) (2.0.40)\n",
      "Requirement already satisfied: Mako in /Users/justinai/anaconda3/envs/llama-prompt-ops/lib/python3.10/site-packages (from alembic>=1.5.0->optuna->dspy==2.6.13->prompt-ops==0.0.9) (1.3.9)\n",
      "Building wheels for collected packages: prompt-ops\n",
      "  Building editable for prompt-ops (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for prompt-ops: filename=prompt_ops-0.0.9-0.editable-py3-none-any.whl size=6961 sha256=58b22c9804f3954c3ec86adbc7d9001363e0557ce7c5ac9fd7cd66075e7c8f43\n",
      "  Stored in directory: /private/var/folders/s9/5_z22sq92_gf508gtrm9njm40000gn/T/pip-ephem-wheel-cache-tx4__ezg/wheels/b7/fd/aa/6502e47f19ba47b5b17ae61f12b254576eb80c406a0a88f118\n",
      "Successfully built prompt-ops\n",
      "Installing collected packages: prompt-ops\n",
      "  Attempting uninstall: prompt-ops\n",
      "    Found existing installation: prompt-ops 0.0.9\n",
      "    Uninstalling prompt-ops-0.0.9:\n",
      "      Successfully uninstalled prompt-ops-0.0.9\n",
      "Successfully installed prompt-ops-0.0.9\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install from source\n",
    "\n",
    "# Clone and install in development mode (editable install)\n",
    "!git clone -b pdo-notebook https://github.com/meta-llama/prompt-ops.git\n",
    "!cd prompt-ops && pip install -e .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's get the MS MARCO Example\n",
    "\n",
    "With prompt-ops installed, we can now clone and explore the MS MARCO PDO example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 120\n",
      "drwxr-xr-x@ 7 justinai  staff    224 Nov 11 16:17 \u001b[34m.\u001b[m\u001b[m\n",
      "drwxr-xr-x@ 6 justinai  staff    192 Nov 11 16:17 \u001b[34m..\u001b[m\u001b[m\n",
      "-rw-r--r--@ 1 justinai  staff  52794 Nov 11 16:17 MSMARCO_PDO_eval.ipynb\n",
      "-rw-r--r--@ 1 justinai  staff   2890 Nov 11 16:17 README.md\n",
      "-rw-r--r--@ 1 justinai  staff   1257 Nov 11 16:17 config.yaml\n",
      "drwxr-xr-x@ 3 justinai  staff     96 Nov 11 16:17 \u001b[34mdataset\u001b[m\u001b[m\n",
      "drwxr-xr-x@ 3 justinai  staff     96 Nov 11 16:17 \u001b[34mprompts\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# Clone the pre-configured MS MARCO PDO example\n",
    "!cd prompt-ops/use-cases/ms-marco-pdo && ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the MS MARCO Configuration\n",
    "\n",
    "Let's examine the key configuration elements for this open-ended QA task:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MS MARCO PDO Configuration\n",
    "\n",
    "```yaml\n",
    "# PDO Configuration for MS MARCO Open-Ended QA\n",
    "\n",
    "name: ms-marco-qa\n",
    "\n",
    "models:\n",
    "  task_model: \"openrouter/meta-llama/llama-3.3-70b-instruct\"      # Model to optimize\n",
    "  proposer_model: \"openrouter/meta-llama/llama-3.3-70b-instruct\" # For evolution & judging\n",
    "  provider: \"openrouter\"\n",
    "  temperature: 0.0                                                # Zero temp for consistency\n",
    "\n",
    "dataset:\n",
    "  path: \"dataset/ms_marco_description.json\"\n",
    "  input_field: \"question\"                                        # Direct field (not nested)\n",
    "  golden_output_field: \"answer\"                                  # Ground truth answers\n",
    "\n",
    "optimization:\n",
    "  strategy: \"pdo\"                                               # PDO strategy\n",
    "  task_type: \"open_ended\"                                       # Open-ended QA task\n",
    "  \n",
    "  # Judge rubric aligned with final evaluation criteria\n",
    "  judge_requirement: |\n",
    "    - Accuracy: Correct and contextually faithful.\n",
    "    - Relevance: Stays on-topic.\n",
    "    - Clarity: Clear and well-structured.\n",
    "    - Conciseness: Uses only necessary information; avoids unnecessary detail.\n",
    "  \n",
    "  # Core dueling parameters\n",
    "  total_rounds: 30                                              # Number of optimization rounds\n",
    "  num_duels_per_round: 3                                        # Duels per round\n",
    "  num_eval_examples_per_duel: 10                                # Examples per duel\n",
    "  num_initial_instructions: 2                                   # Start with 2 prompts\n",
    "  \n",
    "  # Instruction evolution\n",
    "  num_top_prompts_to_combine: 2                                 # Top K for combination\n",
    "  num_new_prompts_to_generate: 1                                # New prompts per gen round\n",
    "  max_prompts_to_generate: 10                                   # Max total prompt pool size\n",
    "  gen_new_prompt_round_frequency: 2                             # Generate every 2 rounds\n",
    "  \n",
    "  # Execution parameters\n",
    "  max_concurrent_threads: 10                                    # Parallel API calls\n",
    "  verbose: true                                                 # Detailed logging\n",
    "\n",
    "prompts:\n",
    "  system: \"prompts/prompt.txt\"                                  # Initial prompt file\n",
    "\n",
    "metric:\n",
    "  class: \"llama_prompt_ops.core.metrics.LLMJudgeStandardMetric\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Key Differences for Open-Ended Tasks\n",
    "\n",
    "â€¢ **task_type: 'open_ended'** - Enables LLM-based evaluation  \n",
    "â€¢ **judge_requirement** - Custom rubric for answer quality  \n",
    "â€¢ **No strict_json** - Answers are free-form text  \n",
    "â€¢ **Smaller num_eval_examples_per_duel** - Quality over quantity  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support both close-ended and open-ended task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For close-ended (classification) tasks, set `optimization.task_type` to `close_ended` in your configuration, and specify `optimization.answer_choices` (such as `[\"Yes\", \"No\"]`), though this defaults to yes/no if omitted. In this mode, the model returns its answer as a JSON object that includes both reasoning and the chosen answer, and judging uses a fixed rubricâ€”evaluating both correctness and reasoning quality.\n",
    "```yaml\n",
    "optimization:\n",
    "  strategy: \"pdo\"\n",
    "  task_type: close_ended\n",
    "  answer_choices: [\"Yes\", \"No\"]\n",
    "```\n",
    "\n",
    "For open-ended tasks (meaning any task that isnâ€™t simple classification), set `optimization.task_type` to `open_ended` in your configuration. You can also provide a custom evaluation rubric by specifying `optimization.judge_requirement`. If you omit this, PDO will automatically generate appropriate judging criteria based on your dataset summary and sample questions. In open-ended mode, the model generates free-form answers, and the judge selects the better response using either your custom rubric or the automatically generated one, providing both an explanation and a winner. For example, if you are working on a summarization task, your YAML might look like this with a specific customized requirement:\n",
    "```yaml\n",
    "optimization:\n",
    "  strategy: \"pdo\"\n",
    "  task_type: open_ended\n",
    "  judge_requirement: |\n",
    "    - Key points coverage (40%): Captures major decisions, actions, owners, dates.\n",
    "    - Faithfulness (30%): No invented details or attributions.\n",
    "    - Structure (15%): Logical, grouped bullets or sections.\n",
    "    - Brevity & clarity (10%): Minimal fluff; readable.\n",
    "    - Tie-Breaker (5%): Better prioritization of actionable items.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation\n",
    "\n",
    "The MS MARCO dataset included in our example contains curated question-answer pairs. PDO works effectively with:\n",
    "\n",
    "1. **Moderate dataset sizes** (50-200 examples): Our subset has 100 QA pairs\n",
    "2. **Clear input/output pairs**: Questions with human-written answers\n",
    "3. **Diverse examples**: Covers various topics and question types\n",
    "\n",
    "Let's look at the MS MARCO data structure:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MS MARCO Dataset Structure\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"question\": \"what is the purpose of an mri\",\n",
    "  \"answer\": \"Magnetic resonance imaging (MRI) is a medical imaging technique used to visualize internal structures of the body in detail. MRI makes use of the property of nuclear magnetic resonance (NMR) to image nuclei of atoms inside the body. An MRI scan can be used as an extremely accurate method of disease detection throughout the body and is most often used after other testing fails to provide sufficient information to confirm a patient's diagnosis.\"\n",
    "}\n",
    "```\n",
    "\n",
    "### Dataset Characteristics:\n",
    "â€¢ Direct question-answer pairs  \n",
    "â€¢ Questions are natural language queries  \n",
    "â€¢ Answers are comprehensive, factual explanations  \n",
    "â€¢ No additional context needed (unlike reading comprehension)  \n",
    "\n",
    "### This structure is perfect for PDO because:\n",
    "â€¢ Clear evaluation criteria (answer quality)  \n",
    "â€¢ Natural variation in question types  \n",
    "â€¢ Human-written gold standard answers  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Prompt Template\n",
    "\n",
    "For MS MARCO, we start with a simple, neutral prompt that PDO will optimize:\n",
    "\n",
    "**Design principles**:\n",
    "1. **Keep it simple** - Let PDO discover complexity\n",
    "2. **Domain-neutral** - Avoid biasing toward specific answer styles\n",
    "3. **Clear task definition** - The model knows it should answer questions\n",
    "\n",
    "PDO will evolve this through competitive testing to discover:\n",
    "- Optimal answer length and detail level\n",
    "- Best communication style\n",
    "- Most effective structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MS MARCO Initial Prompt\n",
    "\n",
    "**prompts/prompt.txt:**\n",
    "```\n",
    "initial_prompt = \"\"\"You are an expert answerer. \n",
    "Read the question and the provided context (if any).\n",
    "Write a concise, accurate answer in your own words.\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_prompt = \"\"\"You are an expert answerer. \n",
    "Read the question and the provided context (if any).\n",
    "Write a concise, accurate answer in your own words.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Start Simple?\n",
    "â€¢ Establishes baseline performance  \n",
    "â€¢ No assumptions about optimal style  \n",
    "â€¢ PDO will discover what works through duels  \n",
    "\n",
    "### What PDO Might Discover:\n",
    "â€¢ Whether step-by-step explanations help  \n",
    "â€¢ Optimal level of technical detail  \n",
    "â€¢ Best way to structure complex answers  \n",
    "â€¢ Tone and communication style  \n",
    "\n",
    "### Example Evolution:\n",
    "**Round 1:** Basic prompt (above)  \n",
    "**Round 5:** Adds 'provide clear explanations'  \n",
    "**Round 10:** Discovers 'step-by-step' improves clarity  \n",
    "**Round 20:** Adds role specification ('knowledgeable researcher')  \n",
    "**Round 30:** Converges on optimal prompt through competition  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Judge requirement and final metric alignment\n",
    "\n",
    "For the evaluation of open-ended tasks, an increasing number of benchmarks now rely on LLM judges due to their flexibility and the availability of more capable frontier or fine-tuned models. In MS MARCO, we also use an LLM judge to evaluate the outputs generated by the model.\n",
    "\n",
    "The final evaluation of a prompt involves comparing the LLM-generated response, produced in response to the instruction prompt, with the ground-truth answer using an LLM judge template as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE_PROMPT = \"\"\"Begin your evaluation by carefully comparing the AI-generated answer with the reference solution. Identify any outputs that aren't true, as well as omissions, or deviations, and simulate human judgments in explaining how these impact the overall quality of the response. Ensure that your assessment is objective and consistent.\n",
    "\n",
    "At the end of your evaluation, assign a score from 1 to 5 based on the following scale:\n",
    "\n",
    "- 1: Very poorâ€”does not meet the requirements or is significantly incorrect.\n",
    "- 2: Poorâ€”contains major errors or omissions.\n",
    "- 3: Fairâ€”adequate but with notable flaws.\n",
    "- 4: Goodâ€”meets the requirements with minor errors.\n",
    "- 5: Excellentâ€”fully accurate and well-articulated.\n",
    "\n",
    "[User Question]\n",
    "\n",
    "{question}\n",
    "\n",
    "[Reference Solution]\n",
    "\n",
    "{ground_truth}\n",
    "\n",
    "[AI-Generated Answer]\n",
    "\n",
    "{prediction}\n",
    "\n",
    "Your response should be a valid json string (no backticks) following this schema:\n",
    "\n",
    "{{\n",
    "    \"explanation\": \"{{Detailed reasoning based on the comparison}}\",\n",
    "    \"score\": {{1-5}}\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running PDO Optimization\n",
    "\n",
    "Now let's run PDO on the MS MARCO dataset and watch the optimization process in action.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Before running PDO, ensure you have:\n",
    "1. **API Key**: Set your OpenRouter API key\n",
    "2. **Project Setup**: Cloned the MS MARCO example\n",
    "3. **Dependencies**: Installed prompt-ops\n",
    "\n",
    "```bash\n",
    "# Set your API key (or add to .env file)\n",
    "export OPENROUTER_API_KEY=\"your-key-here\"\n",
    "\n",
    "# Or if using a .env file\n",
    "echo \"OPENROUTER_API_KEY=your-key-here\" > .env\n",
    "```\n",
    "\n",
    "### Understanding the Judge\n",
    "\n",
    "For open-ended tasks like MS MARCO, PDO uses an LLM judge to compare answers. The judge evaluates based on the criteria in `judge_requirement`:\n",
    "\n",
    "- **Accuracy**: Is the answer factually correct?\n",
    "- **Relevance**: Does it address the question?\n",
    "- **Clarity**: Is it well-structured and understandable?\n",
    "- **Conciseness**: Does it avoid unnecessary detail?\n",
    "\n",
    "This aligns with how we'll evaluate the final optimized prompt.\n",
    "\n",
    "### Running PDO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration from config.yaml\n",
      "2025-11-11 16:18:05,982 | INFO    |  Using model with LiteLLM: openrouter/meta-llama/llama-3.3-70b-instruct\n",
      "Using the same model for task and proposer: openrouter/meta-llama/llama-3.3-70b-instruct\n",
      "Using metric: StandardJSONMetric\n",
      "Resolved relative dataset path to: /Users/justinai/Documents/Code/llama-prompt-ops/notebook/prompt-ops/use-cases/ms-marco-pdo/dataset/ms_marco_description.json\n",
      "Using dataset adapter: ConfigurableJSONAdapter\n",
      "Using PDOStrategy from config for model: llama-3.3-70b-instruct\n",
      "Task model: openrouter/meta-llama/llama-3.3-70b-instruct, Judge model: openrouter/meta-llama/llama-3.3-70b-instruct\n",
      "2025-11-11 16:18:05,988 - root - INFO - Loaded 200 examples from /Users/justinai/Documents/Code/llama-prompt-ops/notebook/prompt-ops/use-cases/ms-marco-pdo/dataset/ms_marco_description.json\n",
      "2025-11-11 16:18:05,988 - root - INFO - Created dataset splits:\n",
      "2025-11-11 16:18:05,988 - root - INFO -   - Training:   50 examples (25.0% of total)\n",
      "2025-11-11 16:18:05,988 - root - INFO -   - Validation: 50 examples (25.0% of total)\n",
      "2025-11-11 16:18:05,988 - root - INFO -   - Testing:    100 examples (50.0% of total)\n",
      "Loaded prompt from file: /Users/justinai/Documents/Code/llama-prompt-ops/notebook/prompt-ops/use-cases/ms-marco-pdo/prompts/prompt.txt\n",
      "Using 'system_prompt' from config\n",
      "Using config filename as output prefix: config\n",
      "Starting prompt optimization...\n",
      "2025-11-11 16:18:05,990 | INFO    | Applying PDOStrategy to optimize prompt\n",
      "2025-11-11 16:18:05,990 | INFO    | Training set size: 50\n",
      "2025-11-11 16:18:05,990 | INFO    | Validation set size: 50\n",
      "2025-11-11 16:18:05,990 | INFO    | Test set size: 100\n",
      "2025-11-11 16:18:05,990 | INFO    | === Pre-Optimization Summary ===\n",
      "    Task Model       : Unknown\n",
      "    Proposer Model   : Unknown\n",
      "    Metric           : <prompt_ops.core.metrics.StandardJSONMetric object at 0x12e8c8820>\n",
      "    Train / Val size : 50 / 50\n",
      "    MIPRO Params     : {\"auto_user\":\"basic\",\"auto_dspy\":\"light\",\"max_labeled_demos\":5,\"max_bootstrapped_demos\":4,\"num_candidates\":10,\"num_threads\":18,\"init_temperature\":0.5,\"seed\":9}\n",
      "Debug: PDO config: {'total_rounds': 20, 'num_duels_per_round': 25, 'num_eval_examples_per_duel': 1, 'num_initial_instructions': 20, 'use_labels': False, 'thompson_alpha': 1.2, 'num_top_prompts_to_combine': 3, 'num_new_prompts_to_generate': 10, 'num_to_prune_each_round': 10, 'gen_new_prompt_round_frequency': 20, 'max_concurrent_threads': 10, 'answer_choices': ['Yes', 'No'], 'ranking_method': 'copeland', 'task_type': 'open_ended', 'judge_requirement': '- Accuracy: Correct and contextually faithful.\\n- Relevance: Stays on-topic.\\n- Clarity: Clear and well-structured.\\n- Conciseness: Uses only necessary information; avoids unnecessary detail.\\n'}\n",
      "Debug: Task model type: <class 'prompt_ops.core.model.LiteLLMModelAdapter'>\n",
      "Debug: Prompt model type: <class 'prompt_ops.core.model.LiteLLMModelAdapter'>\n",
      "Starting PDO optimization with 50 training examples...\n",
      "Task model: openrouter/meta-llama/llama-3.3-70b-instruct\n",
      "Judge model: openrouter/meta-llama/llama-3.3-70b-instruct\n",
      "Generating 20 initial instructions...\n",
      "Debug: examples type: <class 'list'>, length: 50\n",
      "Debug: labels type: <class 'NoneType'>\n",
      "Debug: judge_model type: <class 'prompt_ops.core.model.LiteLLMModelAdapter'>\n",
      "Generating 20 new instructions...\n",
      "âœ“ Initialized with 21 instructions\n",
      "\n",
      "Starting PDO optimization:\n",
      "- Total rounds: 20\n",
      "- Duels per round: 25\n",
      "- Examples per duel: 1\n",
      "- Initial instructions: 21\n",
      "\n",
      "=== Round 1/20 ===\n",
      "Executing 50 prompts...\n",
      "Evaluating duel responses...\n",
      "Getting judge evaluations for 25 comparisons...\n",
      "\n",
      "=== Round 2/20 ===\n",
      "Executing 50 prompts...\n",
      "Evaluating duel responses...\n",
      "Getting judge evaluations for 25 comparisons...\n",
      "\n",
      "=== Round 3/20 ===\n",
      "Executing 50 prompts...\n",
      "Evaluating duel responses...\n",
      "Getting judge evaluations for 25 comparisons...\n",
      "\n",
      "=== Round 4/20 ===\n",
      "Executing 50 prompts...\n",
      "Evaluating duel responses...\n",
      "Getting judge evaluations for 25 comparisons...\n",
      "\n",
      "=== Round 5/20 ===\n",
      "Executing 50 prompts...\n",
      "Evaluating duel responses...\n",
      "Getting judge evaluations for 25 comparisons...\n",
      "\n",
      "=== Round 6/20 ===\n",
      "Executing 50 prompts...\n",
      "Evaluating duel responses...\n",
      "Getting judge evaluations for 25 comparisons...\n",
      "\n",
      "=== Round 7/20 ===\n",
      "Executing 50 prompts...\n",
      "Evaluating duel responses...\n",
      "Getting judge evaluations for 25 comparisons...\n",
      "\n",
      "=== Round 8/20 ===\n",
      "/Users/justinai/Documents/Code/llama-prompt-ops/src/prompt_ops/core/pdo/ranking_systems.py:277: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1.0 + np.exp(-x))\n",
      "Executing 50 prompts...\n",
      "Evaluating duel responses...\n",
      "Getting judge evaluations for 25 comparisons...\n",
      "\n",
      "=== Round 9/20 ===\n",
      "Executing 50 prompts...\n",
      "Evaluating duel responses...\n",
      "Getting judge evaluations for 25 comparisons...\n",
      "\n",
      "=== Round 10/20 ===\n",
      "Executing 50 prompts...\n",
      "Evaluating duel responses...\n",
      "Getting judge evaluations for 25 comparisons...\n",
      "\n",
      "=== Round 11/20 ===\n",
      "Executing 50 prompts...\n",
      "Evaluating duel responses...\n",
      "Getting judge evaluations for 25 comparisons...\n",
      "\n",
      "=== Round 12/20 ===\n",
      "Executing 50 prompts...\n",
      "Evaluating duel responses...\n",
      "Getting judge evaluations for 25 comparisons...\n",
      "\n",
      "=== Round 13/20 ===\n",
      "Executing 50 prompts...\n",
      "Evaluating duel responses...\n",
      "Getting judge evaluations for 25 comparisons...\n",
      "\n",
      "=== Round 14/20 ===\n",
      "Executing 50 prompts...\n",
      "Evaluating duel responses...\n",
      "Getting judge evaluations for 25 comparisons...\n",
      "\n",
      "=== Round 15/20 ===\n",
      "Executing 50 prompts...\n",
      "Evaluating duel responses...\n",
      "Getting judge evaluations for 25 comparisons...\n",
      "\n",
      "=== Round 16/20 ===\n",
      "Executing 50 prompts...\n",
      "Evaluating duel responses...\n",
      "Getting judge evaluations for 25 comparisons...\n",
      "\n",
      "=== Round 17/20 ===\n",
      "Executing 50 prompts...\n",
      "Evaluating duel responses...\n",
      "Getting judge evaluations for 25 comparisons...\n",
      "\n",
      "=== Round 18/20 ===\n",
      "Executing 50 prompts...\n",
      "Evaluating duel responses...\n",
      "Getting judge evaluations for 25 comparisons...\n",
      "\n",
      "=== Round 19/20 ===\n",
      "Executing 50 prompts...\n",
      "Evaluating duel responses...\n",
      "Getting judge evaluations for 25 comparisons...\n",
      "\n",
      "=== Round 20/20 ===\n",
      "Executing 50 prompts...\n",
      "Evaluating duel responses...\n",
      "Getting judge evaluations for 25 comparisons...\n",
      "\n",
      "ðŸŽ‰ PDO optimization complete!\n",
      "Best instruction:\n",
      "================================================================================\n",
      "Analyze the provided query and context passages to identify key terms and concepts, and generate a comprehensive answer that addresses all aspects of the question, considering multiple perspectives and possible interpretations, to provide a clear and concise response that fully resolves the inquiry.\n",
      "================================================================================\n",
      "âœ… PDO optimization completed successfully!\n",
      "Generated 21 total instructions\n",
      "Conducted 500 total duels\n",
      "2025-11-11 16:29:53,956 | INFO    | [Running optimization strategy] completed in 707.96s\n",
      "2025-11-11 16:29:53,957 | INFO    | Optimized prompt:\n",
      "2025-11-11 16:29:53,957 | INFO    | ----------------------------------------\n",
      "2025-11-11 16:29:53,957 | INFO    | Analyze the provided query and context passages to identify key terms and concepts, and generate a comprehensive answer that addresses all aspects of the question, considering multiple perspectives and possible interpretations, to provide a clear and concise response that fully resolves the inquiry.\n",
      "2025-11-11 16:29:53,957 | INFO    | ----------------------------------------\n",
      "2025-11-11 16:29:53,958 | INFO    | Saved optimized prompt to results/config_20251111_161805.json\n",
      "2025-11-11 16:29:53,962 | INFO    | Saved YAML prompt to results/config_20251111_161805.yaml\n",
      "2025-11-11 16:29:53,962 | INFO    | [Saving optimized prompt] completed in 0.01s\n",
      "\n",
      "=== Optimization Complete ===\n",
      "Results saved to: /Users/justinai/Documents/Code/llama-prompt-ops/notebook/prompt-ops/use-cases/ms-marco-pdo/results/config_20251111_161805.json\n",
      "Results also saved to: /Users/justinai/Documents/Code/llama-prompt-ops/notebook/prompt-ops/use-cases/ms-marco-pdo/results/config_20251111_161805.yaml\n",
      "\n",
      "Optimized prompt:\n",
      "================================================================================\n",
      "Analyze the provided query and context passages to identify key terms and concepts, and generate a comprehensive answer that addresses all aspects of the question, considering multiple perspectives and possible interpretations, to provide a clear and concise response that fully resolves the inquiry.\n",
      "================================================================================\n",
      "2025-11-11 16:29:53,964 | INFO    | === Timings summary ===\n",
      "2025-11-11 16:29:53,964 | INFO    | Running optimization strategy 707.96s\n",
      "2025-11-11 16:29:53,964 | INFO    | Saving optimized prompt     0.01s\n"
     ]
    }
   ],
   "source": [
    "!cd prompt-ops/use-cases/ms-marco-pdo && prompt-ops migrate --config config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running PDO, we store the optimized prompt as follows, ready for final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZED_PROMPT = (\n",
    "    \"\"\"Embody the role of a knowledgeable researcher and provide a clear, \n",
    "    step-by-step explanation to answer the user's question, utilizing the provided \n",
    "    context and avoiding technical jargon unless absolutely necessary, to ensure a \n",
    "    concise and accurate response.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate the optimized prompt with LLM judge\n",
    "\n",
    "In this section, we evaluate the performance of the optimized prompt after running PDO against the initial prompt. We compare generated answers against ground truth using the judge prompt from Section 1 (returns JSON with explanation + 1â€“5 score).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_scores(preds: list[str]):\n",
    "    \"\"\"Judge with guided JSON decoding (fallback to free-form if unsupported).\"\"\"\n",
    "    import json as _json\n",
    "    import numpy as _np\n",
    "\n",
    "    scores = []\n",
    "    explanations = []\n",
    "    judge_prompts = []\n",
    "    for pred, r in zip(preds, rows):\n",
    "        judge_prompts.append(\n",
    "            JUDGE_PROMPT.format(\n",
    "                question=r[\"question\"],\n",
    "                ground_truth=r[\"answer\"],\n",
    "                prediction=pred,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    response_format = {\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\n",
    "            \"name\": \"judge_score\",\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"explanation\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Detailed reasoning based on the comparison\",\n",
    "                    },\n",
    "                    \"score\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"Score from 1 to 5\",\n",
    "                        \"enum\": [1, 2, 3, 4, 5],\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"explanation\", \"score\"],\n",
    "                \"additionalProperties\": False,\n",
    "            },\n",
    "            \"strict\": True,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        outs = model.generate_batch(\n",
    "            judge_prompts, max_threads=10, temperature=0.0, response_format=response_format\n",
    "        )\n",
    "    except Exception:\n",
    "        outs = model.generate_batch(\n",
    "            judge_prompts, max_threads=10, temperature=0.0\n",
    "        )\n",
    "\n",
    "    for o in outs:\n",
    "        obj = _try_json_parse(o)\n",
    "        score = obj.get(\"score\", 0)\n",
    "        exp = obj.get(\"explanation\", \"\")\n",
    "        try:\n",
    "            score = float(score)\n",
    "        except Exception:\n",
    "            score = 0.0\n",
    "        scores.append(score)\n",
    "        explanations.append(exp)\n",
    "    return _np.array(scores), explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 16:29:54,489 | INFO    |  Using model with LiteLLM: openrouter/meta-llama/llama-3.3-70b-instruct\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating... Initial Prompt\n",
      "evaluating... PDO\n",
      "\n",
      "=== LLM Judge Avg Score (1â€“5) on MS MARCO Description ===\n",
      "Initial Prompt: 4.11\n",
      "PDO: 4.60\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"../../src\"))\n",
    "\n",
    "from prompt_ops.core.model import setup_model\n",
    "\n",
    "# Load dataset\n",
    "DATA_PATH = \"prompt-ops/use-cases/ms-marco-pdo/dataset/ms_marco_description.json\"\n",
    "rows = json.load(open(DATA_PATH, \"r\"))\n",
    "\n",
    "# Model for answering and for judging (same adapter; different prompts)\n",
    "model = setup_model(\n",
    "    adapter_type=\"litellm\",\n",
    "    model_name=\"openrouter/meta-llama/llama-3.3-70b-instruct\",\n",
    "    api_base=os.environ.get(\"OPENROUTER_API_BASE\", \"https://openrouter.ai/api/v1\"),\n",
    "    api_key=os.environ.get(\"OPENROUTER_API_KEY\"),\n",
    "    max_tokens=4096,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "# Helper\n",
    "def _try_json_parse(s: str):\n",
    "    try:\n",
    "        start = s.find(\"{\"); end = s.rfind(\"}\") + 1\n",
    "        blob = s[start:end] if (start >= 0 and end > start) else s\n",
    "        return json.loads(blob)\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "# Baselines and PDO instruction\n",
    "BASELINES = {\n",
    "    \"Initial Prompt\": \"\"\"You are an expert answerer. \n",
    "Read the question and the provided context (if any).\n",
    "Write a concise, accurate answer in your own words.\n",
    "\"\"\",\n",
    "}\n",
    "\n",
    "\n",
    "def predict_answers(instruction: str):\n",
    "    prompts = []\n",
    "    for r in rows:\n",
    "        q = r[\"question\"].strip()\n",
    "        prompts.append(f\"{instruction}\\n\\nQuestion:\\n{q}\")\n",
    "    return model.generate_batch(prompts, max_threads=10, temperature=0.0)\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Evaluate baselines\n",
    "for name, instr in BASELINES.items():\n",
    "    print(f\"evaluating... {name}\")\n",
    "    preds = predict_answers(instr)\n",
    "    s, _ = judge_scores(preds)\n",
    "    results[name] = float(np.mean(s)) if len(s) else 0.0\n",
    "\n",
    "# Evaluate PDO (if you paste the optimized instruction)\n",
    "if OPTIMIZED_PROMPT:\n",
    "    print(\"evaluating... PDO\")\n",
    "    preds = predict_answers(OPTIMIZED_PROMPT)\n",
    "    s, _ = judge_scores(preds)\n",
    "    results[\"PDO\"] = float(np.mean(s)) if len(s) else 0.0\n",
    "\n",
    "print(\"\\n=== LLM Judge Avg Score (1â€“5) on MS MARCO Description ===\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k}: {v:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDO achieves a **higher average LLM judge score (4.64)** than the Initial Prompt (4.48) on the MS MARCO Description, indicating a measurable improvement in answer quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m4.5\u001b[39m))\n\u001b[1;32m      4\u001b[0m bars \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mbar(results\u001b[38;5;241m.\u001b[39mkeys(), results\u001b[38;5;241m.\u001b[39mvalues(), color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#4e79a7\u001b[39m\u001b[38;5;124m\"\u001b[39m, edgecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#2e4a62\u001b[39m\u001b[38;5;124m\"\u001b[39m, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.5\u001b[39m, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.55\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 4.5))\n",
    "bars = plt.bar(results.keys(), results.values(), color=\"#4e79a7\", edgecolor=\"#2e4a62\", linewidth=1.5, width=0.55)\n",
    "plt.title(\"LLM Judge Avg Score (1â€“5) on MS MARCO Description\", fontsize=15, fontweight=\"bold\", pad=14)\n",
    "plt.ylabel(\"Avg Score (1â€“5)\", fontsize=13)\n",
    "plt.ylim(4, 5.1)\n",
    "plt.xticks(rotation=15, fontsize=11)\n",
    "plt.yticks(fontsize=11)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.25)\n",
    "\n",
    "for bar, v in zip(bars, results.values()):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, v + 0.10, f\"{v:.2f}\", ha=\"center\", va=\"bottom\", fontsize=12, fontweight=\"bold\", color=\"#34495e\")\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Key Takeaways\n",
    "\n",
    "### Summary of PDO's Innovations\n",
    "\n",
    "Based on our exploration of the PDO paper and practical implementation:\n",
    "\n",
    "1. **Dueling Bandits Framework**: PDO treats optimization as a tournament where prompts compete head-to-head, making relative comparisons more reliable than absolute scoring.\n",
    "\n",
    "2. **Thompson Sampling**: Smart exploration strategy that balances trying new prompts (exploration) with refining known winners (exploitation) through probabilistic sampling.\n",
    "\n",
    "3. **Multi-Ranker Fusion**: Combines 5 different ranking algorithms (Copeland, Borda, Win Rate, Elo, TrueSkill) with Dirichlet-weighted fusion for robust evaluation.\n",
    "\n",
    "4. **LLM-as-Judge**: Uses language models to evaluate pairwise preferences, enabling nuanced quality assessment beyond hard metrics.\n",
    "\n",
    "5. **Instruction Evolution**: Generates new prompts by combining top performers, allowing discovery of superior variants through competitive pressure.\n",
    "\n",
    "6. **Adaptive Pruning**: Removes consistently poor performers while maintaining diversity in the prompt pool.\n",
    "\n",
    "### When to Use PDO\n",
    "\n",
    "The paper suggests PDO is particularly effective for:\n",
    "\n",
    "- **High-stakes applications** where prompt quality is critical\n",
    "- **Complex tasks** with nuanced quality criteria\n",
    "- **Scenarios where absolute scoring is unreliable** (subjective tasks)\n",
    "- **Budget available for thorough exploration** (more LLM calls than simpler methods)\n",
    "\n",
    "### Paper's Key Results\n",
    "\n",
    "The paper demonstrates PDO's effectiveness:\n",
    "- **Outperforms gradient-based methods**: More thorough exploration avoids local optima\n",
    "- **Robust rankings**: Multi-ranker consensus prevents over-reliance on single metric\n",
    "- **Efficient evolution**: Thompson sampling focuses computational budget on informative duels\n",
    "- **Interpretable results**: Win matrices and Elo ratings provide clear understanding of prompt quality\n",
    "\n",
    "\n",
    "### Final Thoughts\n",
    "\n",
    "PDO represents an important advance in prompt optimization by bringing dueling bandit algorithms to LLM prompt engineering. Its competitive approach, combined with sophisticated exploration strategies and multi-ranker fusion, creates a powerful framework for discovering high-quality prompts through natural selection in a tournament setting.\n",
    "\n",
    "The key insight is that **asking \"which is better?\" is often more reliable than asking \"how good is this?\"** - PDO leverages this principle throughout its design.\n",
    "\n",
    "For more details, refer to the full paper and implementation documentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "### References\n",
    "\n",
    " 1. **PDO Paper**: \"Prompt Dueling Optimization: Tournament-Driven Prompt Discovery and Evaluation\" ([arXiv:2510.13907](https://arxiv.org/abs/2510.13907))\n",
    "\n",
    "### Quick Reference Commands\n",
    "\n",
    "```bash\n",
    "# Install prompt-ops\n",
    "pip install prompt-ops\n",
    "\n",
    "# Create a new PDO project\n",
    "prompt-ops create my-pdo-project\n",
    "\n",
    "# Run PDO optimization\n",
    "prompt-ops migrate --config config.yaml --log-level INFO\n",
    "\n",
    "# View results\n",
    "ls results/*.yaml\n",
    "```\n",
    "\n",
    "### Tips for Success\n",
    "\n",
    "1. **Start small**: Use 10-30 training examples as recommended by the paper\n",
    "2. **Design good feedback**: Specific, actionable feedback functions are crucial\n",
    "3. **Monitor validation**: Track validation performance to ensure generalization\n",
    "4. **Experiment with budgets**: Start with 500-1000 metric calls\n",
    "5. **Enable merging**: Set `use_merge: true` for best results\n",
    "\n",
    "---\n",
    "\n",
    "*This tutorial covered the theoretical foundations and practical implementation of PDO based on dueling bandits the research paper. For hands-on practice, try the facility support example or adapt PDO to your own classification or generation tasks.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-prompt-ops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
