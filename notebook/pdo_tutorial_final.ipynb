{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "All rights reserved.\n",
    "\n",
    "This source code is licensed under the terms described in the LICENSE file in\n",
    "the root directory of this source tree.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/meta-llama/llama-cookbook/blob/main/getting-started/llama-tools/pdo_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Getting Started with PDO (Prompt Duel Optimizer) with prompt-ops\n",
    "\n",
    "This tutorial will guide you through using PDO with prompt-ops to optimize prompts for Llama models through competitive dueling. We'll cover:\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction to PDO](#1-introduction-to-pdo)\n",
    "   - The Problem PDO Solves\n",
    "   - Key Innovations\n",
    "   - Comparison with Other Methods\n",
    "\n",
    "2. [PDO Architecture Deep Dive](#2-pdo-architecture-deep-dive)\n",
    "   - Core Components from the Paper\n",
    "   - Dueling Bandits Framework\n",
    "   - Thompson Sampling\n",
    "   - Multi-Ranker Fusion\n",
    "\n",
    "3. [Creating a PDO Project](#3-creating-a-pdo-project)\n",
    "   - Project Structure\n",
    "   - Configuration Essentials\n",
    "   - Dataset Preparation\n",
    "\n",
    "4. [Running PDO Optimization](#4-running-pdo-optimization)\n",
    "   - The Optimization Process\n",
    "   - Understanding the Output\n",
    "   - Common Parameters\n",
    "\n",
    "5. [Analyzing Results](#5-analyzing-results)\n",
    "   - Interpreting Optimized Prompts\n",
    "   - Performance Metrics\n",
    "   - Duel Statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to PDO\n",
    "\n",
    "### The Problem PDO Solves\n",
    "\n",
    "Traditional prompt optimization methods face several critical challenges:\n",
    "\n",
    "1. **Absolute Scoring Bias**: Single-point evaluation can be misleading - is a 0.85 score truly better than 0.82?\n",
    "2. **Limited Exploration**: Greedy approaches miss superior prompts in unexplored regions\n",
    "3. **Ranking Uncertainty**: Different metrics may rank prompts inconsistently\n",
    "4. **Exploitation vs Exploration**: How to balance trying new prompts vs refining known good ones?\n",
    "\n",
    "PDO addresses these limitations through a **dueling bandit approach** that treats prompt optimization as a competitive head-to-head tournament rather than absolute scoring.\n",
    "\n",
    "#### **Design Choice: Pairwise vs. Pointwise Evaluation**  \n",
    "PDO uses **pairwise comparison** even when labels are available‚Äîtrading computational efficiency for more robust prompt evaluation.\n",
    "\n",
    "### Key Innovations from the Paper\n",
    "\n",
    "The paper explores several novel concepts in prompt optimization\n",
    "\n",
    "1. **Dueling Bandits**: Prompts compete in pairwise comparisons, not absolute rankings\n",
    "2. **Thompson Sampling**: Probabilistic exploration using Beta distributions for smart duel selection\n",
    "3. **Multi-Ranker Fusion**: Combines multiple ranking algorithms (Copeland, Borda, Elo, TrueSkill) for robust evaluation\n",
    "4. **Variance-Driven Exploration**: Selects opponent based on uncertainty, maximizing information gain\n",
    "5. **LLM-as-Judge**: Uses language models to evaluate which prompt produces better responses\n",
    "6. **Adaptive Pruning**: Removes consistently poor performers while maintaining diversity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with Other Methods\n",
    "\n",
    "The paper compares PDO with several baseline approaches:\n",
    "\n",
    "| Method | Approach | Limitations | PDO's Advantage |\n",
    "|--------|----------|-------------|-----------------|\n",
    "| **Manual Prompting** | Human-written instructions | Time-consuming, subjective | Automated competitive optimization |\n",
    "| **Few-shot Learning** | Examples in prompt | Limited by context window | Optimizes instructions through duels |\n",
    "| **DSPy MIPRO** | Baysien-based optimization | Local optima, absolute scoring | Global exploration via bandits |\n",
    "| **OPRO** | LLM-based optimization | Greedy, expensive | Efficient exploration via Thompson sampling |\n",
    "\n",
    "### The Dueling Advantage\n",
    "\n",
    "The paper's key insight is that **relative comparisons are more reliable than absolute scores**. Dueling bandits are well-suited for prompt optimization because they:\n",
    "\n",
    "- Focus on what matters: \"Which prompt is better?\" not \"What's the exact score?\"\n",
    "- Maintain diversity through probabilistic selection\n",
    "- Balance exploration (trying new prompts) and exploitation (refining winners)\n",
    "- Aggregate multiple ranking systems for robustness\n",
    "- Scale efficiently with the number of prompts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the PDO Advantage\n",
    "\n",
    "To understand why PDO's dueling approach is superior, let's compare it with traditional optimization:\n",
    "\n",
    "**The Traditional Problem:**\n",
    "When you score prompts independently (e.g., P4 gets 0.78, P5 gets 0.74), small differences might just be noise. Is P4 *really* better, or did it just get lucky on the test set? You can't be sure.\n",
    "\n",
    "**PDO's Solution:**\n",
    "Instead of asking \"How good is this prompt?\" (absolute), PDO asks \"Which prompt is better?\" (relative). By running head-to-head duels on the same examples, PDO eliminates scoring bias and reveals consistent winners.\n",
    "\n",
    "**The Visualization Below Shows:**\n",
    "- **Left**: Traditional optimization relies on absolute scores that may be unreliable\n",
    "- **Right**: PDO builds a **win matrix** where each cell shows how often one prompt beats another in head-to-head duels\n",
    "\n",
    "**Key Insight:** A prompt that wins 60%+ of duels against *every* opponent is a validated champion. This is PDO's core innovation: **reliable relative comparison through competition**.\n",
    "\n",
    "![Point vs Pairwise Comparison](images/point-vs-pairwise.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- PDO discovers prompt quality through competitive duels,\n",
    "- ‚úì P4 dominates: wins 60%+ against ALL opponents (validated through competition)\n",
    "- where consistent winners emerge naturally from head-to-head comparisons.\n",
    "- \\nKey Insight: P4's superiority is validated across ALL matchups, not just by a single score that might be noise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PDO Architecture Deep Dive\n",
    "\n",
    "This section explores the core components of PDO as described in the research paper, focusing on the theoretical foundations and algorithmic innovations.\n",
    "\n",
    "### Core Components from the Paper\n",
    "\n",
    "PDO consists of six main components:\n",
    "\n",
    "1. **Instruction Pool (ùí´)**: Collection of candidate prompts competing for superiority\n",
    "2. **Win Matrix (W)**: Tracks head-to-head results between all prompt pairs\n",
    "3. **Thompson Sampler**: Selects duel pairs using probabilistic exploration\n",
    "4. **LLM Judge**: Evaluates which prompt produces better responses\n",
    "5. **Multi-Ranker System**: Aggregates multiple ranking algorithms for robust evaluation\n",
    "6. **Instruction Evolution**: Generates new prompts by combining successful variants\n",
    "\n",
    "Let's explore each component in detail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The Dueling Bandits Framework\n",
    "\n",
    "The paper introduces PDO's use of **Dueling Bandits** as the central optimization paradigm:\n",
    "\n",
    "**Traditional Multi-Armed Bandits**:\n",
    "- Pull an arm ‚Üí observe absolute reward\n",
    "- Problem: Reward scales may be unreliable or noisy\n",
    "\n",
    "**Dueling Bandits (PDO's Approach)**:\n",
    "- Select two prompts ‚Üí run head-to-head comparison\n",
    "- Observe relative preference: \"Which is better?\"\n",
    "- Advantage: Relative comparisons are more stable than absolute scores\n",
    "\n",
    "### The Algorithm Loop\n",
    "\n",
    "```\n",
    "Initialize:\n",
    "  - Start with base prompt + generate initial variations\n",
    "  - Initialize Win matrix W (all zeros)\n",
    "\n",
    "For each round t = 1 to T:\n",
    "  1. Use Thompson Sampling to select prompt pair (i, j)\n",
    "  2. Run duel: Both prompts answer the same examples\n",
    "  3. LLM judge decides winner on each example\n",
    "  4. Update W[i,j] with wins, W[j,i] with losses\n",
    "  5. Compute multi-ranker scores (Copeland, Borda, Elo, TrueSkill)\n",
    "  6. [Optional] Generate new prompts by combining top performers\n",
    "  7. [Optional] Prune worst-performing prompts\n",
    "\n",
    "Return:\n",
    "  - Best prompt according to aggregated rankings\n",
    "```\n",
    "\n",
    "**Key Innovation**: The Win matrix W becomes increasingly informative, guiding both exploration (via Thompson sampling) and exploitation (via ranking systems).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Thompson Sampling\n",
    "\n",
    "One of PDO's most sophisticated components is its **Double Thompson Sampling** strategy:\n",
    "\n",
    "#### What is Thompson Sampling?\n",
    "\n",
    "Thompson Sampling is a probabilistic approach to the exploration-exploitation dilemma:\n",
    "- Maintain a **belief distribution** about each prompt's quality\n",
    "- Sample from these distributions to select actions\n",
    "- Naturally balances trying new things vs. exploiting known winners\n",
    "\n",
    "#### PDO's Double Thompson Sampling\n",
    "\n",
    "The paper describes a two-stage selection process:\n",
    "\n",
    "**Stage 1: Select First Prompt**\n",
    "1. For each prompt pair (i,j), model win probability as Beta(wins+1, losses+1)\n",
    "2. Sample a win-rate matrix Œ∏ from these Beta distributions\n",
    "3. Compute multiple rankings from Œ∏ (Copeland, Borda, win-rate)\n",
    "4. Combine with Elo and TrueSkill ratings using **Dirichlet-weighted fusion**\n",
    "5. Apply softmax with temperature œÑ for final selection\n",
    "\n",
    "**Stage 2: Select Second Prompt (Opponent)**\n",
    "1. Among remaining prompts, filter to those still plausibly competitive\n",
    "2. Select the one with **maximum variance** in win probability vs. first prompt\n",
    "3. Rationale: High variance = high uncertainty = maximum information gain from this duel\n",
    "\n",
    "**Benefits**:\n",
    "- Efficient exploration: Doesn't waste duels on clearly inferior prompts\n",
    "- Information-driven: Prioritizes duels that reduce uncertainty\n",
    "- Adaptive: Automatically shifts from exploration to exploitation as data accumulates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the Thompson Sampling Visualization\n",
    "\n",
    "The diagram below shows **Beta distributions** representing our belief about each prompt's win probability. This is the heart of how PDO decides which prompts to compare in duels:\n",
    "\n",
    "**What You're Looking At:**\n",
    "- Each plot shows a probability distribution over win rates (0 to 1 on x-axis)\n",
    "- The **height** (density) represents how confident we are about that win rate\n",
    "- The **width** represents uncertainty - wider = less certain\n",
    "\n",
    "**The Three Scenarios:**\n",
    "\n",
    "1. **Prompt A (Strong Performer - Green)**: 15 wins, 5 losses\n",
    "   - **Narrow, tall distribution** centered at 0.73\n",
    "   - High confidence: We're pretty sure this prompt wins ~73% of the time\n",
    "   - PDO interpretation: *Reliable choice, but already well-understood*\n",
    "\n",
    "2. **Prompt B (Average - Blue)**: 10 wins, 10 losses  \n",
    "   - **Medium width** distribution centered at 0.50\n",
    "   - Moderate confidence: Wins about half the time\n",
    "   - PDO interpretation: *Middle-of-the-pack, not particularly interesting*\n",
    "\n",
    "3. **Prompt C (Uncertain - Orange)**: 2 wins, 2 losses\n",
    "   - **Wide, flat distribution** centered at 0.50\n",
    "   - HIGH UNCERTAINTY: Could be anywhere from 0.2 to 0.8!\n",
    "   - PDO interpretation: *High information value - might be secretly excellent OR terrible*\n",
    "\n",
    "**PDO's Smart Strategy:**\n",
    "\n",
    "Thompson Sampling doesn't just pick the prompt with the highest mean (that would be A). Instead:\n",
    "\n",
    "1. **First prompt selection**: Sample from all distributions ‚Üí A likely selected (highest mean)\n",
    "2. **Second prompt selection**: Pick the opponent with **maximum variance** vs. the first\n",
    "   - Dueling A vs B tells us little (both well-understood)\n",
    "   - **Dueling A vs C tells us a LOT** (huge uncertainty about C)\n",
    "\n",
    "**The Key Insight:** By choosing C as the opponent, PDO maximizes **information gain**. After this duel:\n",
    "- If C wins ‚Üí Great! We discovered a hidden champion\n",
    "- If C loses ‚Üí Good! We eliminated uncertainty and can focus elsewhere\n",
    "\n",
    "This is why PDO converges faster than naive approaches - it strategically explores high-uncertainty regions rather than randomly sampling prompts.\n",
    "\n",
    "![Thompson Sampling Beta Distributions](images/thompson-sampling.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Multi-Ranker Fusion\n",
    "\n",
    "One of PDO's most important innovations is using **multiple ranking algorithms** simultaneously to create a more robust evaluation system.\n",
    "\n",
    "#### What is Multi-Ranker Fusion?\n",
    "\n",
    "Imagine you're trying to determine the best chess player. Would you:\n",
    "- **Option A**: Use only one ranking system (e.g., just count wins)\n",
    "- **Option B**: Combine multiple perspectives (wins, strength of opponents beaten, consistency, etc.)\n",
    "\n",
    "PDO chooses Option B - it **fuses** (combines) the opinions of 5 different ranking algorithms, each with its own strengths and blind spots. This is called **Multi-Ranker Fusion**.\n",
    "\n",
    "**The Core Problem:** No single ranking system is perfect. Each has biases:\n",
    "- Some favor consistent performers\n",
    "- Some reward crushing weak opponents\n",
    "- Some value beating strong opponents highly\n",
    "- Some are sensitive to the order of matches\n",
    "\n",
    "By combining all 5, PDO gets a **consensus view** that's more reliable than any individual ranker.\n",
    "\n",
    "#### Why Multiple Rankers?\n",
    "\n",
    "Different ranking systems capture different aspects of prompt quality:\n",
    "\n",
    "| Ranking System | What It Measures | Strengths | Weaknesses |\n",
    "|----------------|------------------|-----------|------------|\n",
    "| **Copeland** | Number of opponents beaten | Simple, intuitive | Ignores margin of victory |\n",
    "| **Borda** | Sum of win probabilities | Accounts for all matchups | Can be dominated by many weak wins |\n",
    "| **Average Win Rate** | Mean win probability | Easy to interpret | Doesn't account for opponent strength |\n",
    "| **Elo** | Chess-style rating | Accounts for opponent strength | Sensitive to order of matches |\n",
    "| **TrueSkill** | Bayesian skill rating | Confidence intervals, robust | More complex, computational cost |\n",
    "\n",
    "#### Fusion Strategy\n",
    "\n",
    "The paper describes a sophisticated fusion approach:\n",
    "\n",
    "```\n",
    "For each round:\n",
    "1. Compute all 5 ranking scores for each prompt\n",
    "2. Normalize each to [0, 1] range\n",
    "3. Sample fusion weights from Dirichlet(1,1,1,1,1)\n",
    "   ‚Üí Introduces randomness in how rankings are combined\n",
    "4. Compute weighted combination: score = Œ£(weight_i √ó ranking_i)\n",
    "5. Use fused scores for prompt selection\n",
    "```\n",
    "\n",
    "**What's Happening Here:**\n",
    "- **Step 3 is key**: Instead of fixed weights (e.g., always 20% each), PDO **randomly samples** weights each round\n",
    "- **Dirichlet distribution** ensures weights sum to 1.0 but vary each time\n",
    "- Example: One round might weight Elo heavily (0.4, 0.15, 0.15, 0.15, 0.15), next round Copeland (0.15, 0.4, 0.15, 0.15, 0.15)\n",
    "- This explores different \"philosophies\" of ranking over time\n",
    "\n",
    "**Benefits**:\n",
    "- **Robustness**: No single ranking system dominates\n",
    "- **Exploration**: Dirichlet sampling explores different ranking perspectives\n",
    "- **Consensus**: Prompts that rank well across ALL systems are truly superior\n",
    "\n",
    "#### Concrete Example\n",
    "\n",
    "Let's say we have 3 prompts after some duels:\n",
    "\n",
    "| Prompt | Copeland | Borda | Win Rate | Elo | TrueSkill | **Average** |\n",
    "|--------|----------|-------|----------|-----|-----------|-------------|\n",
    "| **P1** | 1st | 1st | 1st | 2nd | 1st | **1st (winner!)** |\n",
    "| **P2** | 2nd | 2nd | 3rd | 1st | 2nd | **2nd** |\n",
    "| **P3** | 3rd | 3rd | 2nd | 3rd | 3rd | **3rd** |\n",
    "\n",
    "**Key Insight**: P1 wins 4 out of 5 ranking systems - it's a **consensus champion**. Even though P2 tops the Elo ranking (maybe it beat one strong opponent), P1 is more consistently excellent across all evaluation criteria.\n",
    "\n",
    "This is why Multi-Ranker Fusion is powerful: **It prevents a prompt from gaming one specific metric** and ensures true, well-rounded superiority.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. LLM-as-Judge\n",
    "\n",
    "The paper emphasizes that PDO's evaluation mechanism is crucial for effective optimization:\n",
    "\n",
    "**Traditional Metrics**: Hard-coded rules (exact match, F1, BLEU, etc.)\n",
    "\n",
    "**PDO's LLM Judge**:\n",
    "- **Natural Language Evaluation**: Can assess nuanced qualities (helpfulness, clarity, correctness)\n",
    "- **Pairwise Comparison**: Given two responses, which is better and why?\n",
    "- **Flexible Criteria**: Can evaluate domain-specific quality without custom metric code\n",
    "- **Reasoning Output**: Provides explanation for decisions (useful for debugging)\n",
    "\n",
    "#### Judge Prompt Structure\n",
    "\n",
    "```\n",
    "You are an expert evaluator. Compare these two responses:\n",
    "\n",
    "Input: {question}\n",
    "Expected Answer: {label}\n",
    "\n",
    "Response A:\n",
    "{response_from_prompt_A}\n",
    "\n",
    "Response B:\n",
    "{response_from_prompt_B}\n",
    "\n",
    "Which response is better? Consider:\n",
    "- Correctness: Does it match the expected answer?\n",
    "- Completeness: Does it address all aspects?\n",
    "- Clarity: Is it well-structured and understandable?\n",
    "\n",
    "Output format:\n",
    "{\n",
    "  \"reasoning\": \"Your detailed comparison\",\n",
    "  \"winner\": \"A\" or \"B\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Benefits of LLM-as-Judge**:\n",
    "- Works for any task (classification, generation, reasoning)\n",
    "- Captures subtle quality differences\n",
    "- Scales to new domains without custom metric engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating a PDO Project\n",
    "\n",
    "Now that we understand the theoretical foundations, let's see how to create a project that leverages PDO's capabilities.\n",
    "\n",
    "### Project Structure\n",
    "\n",
    "A PDO-enabled project requires several components:\n",
    "\n",
    "```\n",
    "my-pdo-project/\n",
    "‚îú‚îÄ‚îÄ config.yaml          # PDO configuration\n",
    "‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ dataset.json     # Training/test data\n",
    "‚îú‚îÄ‚îÄ prompts/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ prompt.txt       # Initial prompt template\n",
    "‚îú‚îÄ‚îÄ results/             # Optimization outputs\n",
    "‚îî‚îÄ‚îÄ logs/                # Detailed execution logs\n",
    "```\n",
    "\n",
    "### Key Configuration Elements\n",
    "\n",
    "Here are the essential configuration parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example PDO Configuration (config.yaml)\n",
    "config_example = \"\"\"\n",
    "# PDO Configuration based on paper recommendations\n",
    "\n",
    "name: customer-support-classifier\n",
    "\n",
    "models:\n",
    "  task_model: \"openrouter/meta-llama/llama-3.3-70b-instruct\"      # Model to optimize\n",
    "  proposer_model: \"openrouter/meta-llama/llama-3.3-70b-instruct\" # For evolution & judging\n",
    "  provider: \"openrouter\"\n",
    "  temperature: 0.1                                                # Low temp for consistency\n",
    "\n",
    "dataset:\n",
    "  path: \"data/dataset.json\"\n",
    "  input_field: [\"fields\", \"input\"]                               # Path to input in JSON\n",
    "  golden_output_field: \"answer\"                                  # Expected output field\n",
    "\n",
    "optimization:\n",
    "  strategy: \"qpdo\"                                               # Enable PDO strategy\n",
    "  \n",
    "  # Core dueling parameters\n",
    "  total_rounds: 50                                               # Number of rounds\n",
    "  num_duels_per_round: 3                                         # Duels per round\n",
    "  num_eval_examples_per_duel: 25                                 # Examples per duel\n",
    "  num_initial_instructions: 3                                    # Initial prompt pool size\n",
    "  \n",
    "  # Thompson sampling parameters\n",
    "  thompson_alpha: 2.0                                            # Confidence bound parameter\n",
    "  \n",
    "  # Instruction evolution\n",
    "  num_top_prompts_to_combine: 3                                  # Top K for combination\n",
    "  num_new_prompts_to_generate: 1                                 # New prompts per gen round\n",
    "  max_new_prompts_to_generate: 20                                # Max total prompt pool size\n",
    "  num_to_prune_each_round: 1                                     # Adaptive pruning\n",
    "  gen_new_prompt_round_frequency: 3                              # Generate every N rounds\n",
    "  \n",
    "  # Execution parameters\n",
    "  max_concurrent_threads: 4                                      # Parallel threads\n",
    "  use_labels: true                                               # Enable supervised evolution\n",
    "  verbose: true                                                  # Detailed logging\n",
    "\n",
    "prompts:\n",
    "  system: \"prompts/prompt.txt\"                                   # Initial prompt file\n",
    "\n",
    "metric:\n",
    "  class: \"llama_prompt_ops.core.metrics.StandardJSONMetric\"      # JSON matching metric\n",
    "  strict_json: false                                             # Allow flexible parsing\n",
    "  output_field: \"answer\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"PDO Configuration Structure:\")\n",
    "print(\"=\" * 50)\n",
    "print(config_example)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"\\nKey Parameters Explained:\")\n",
    "print(\"‚Ä¢ total_rounds √ó num_duels_per_round = total number of prompt comparisons\")\n",
    "print(\"‚Ä¢ thompson_alpha controls exploration: higher = more exploration\")\n",
    "print(\"‚Ä¢ gen_new_prompt_round_frequency: Generate new variants every N rounds\")\n",
    "print(\"‚Ä¢ num_to_prune_each_round: Remove worst performers to maintain efficiency\")\n",
    "print(\"\\nCost Considerations:\")\n",
    "print(\"Total LLM calls ‚âà rounds √ó duels √ó examples √ó 3\")\n",
    "print(\"  (√ó3 = both prompts + judge evaluation)\")\n",
    "print(\"Example: 50 √ó 3 √ó 25 √ó 3 = ~11,250 LLM calls\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation\n",
    "\n",
    "According to the paper, PDO works effectively with:\n",
    "\n",
    "1. **Moderate dataset sizes** (50-200 examples): Enough for reliable duels\n",
    "2. **Clear input/output pairs**: Enables supervised evolution\n",
    "3. **Diverse examples**: Cover different aspects of the task\n",
    "\n",
    "Example dataset structure:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Prompt Template\n",
    "\n",
    "The starting prompt doesn't need to be perfect - PDO will evolve it through duels:\n",
    "\n",
    "**Key principles**:\n",
    "1. Start with a **clear task description**\n",
    "2. Include **basic structure** for the output\n",
    "3. PDO will discover improvements through **competitive testing**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example initial prompt (prompts/prompt.txt)\n",
    "initial_prompt = \"\"\"\n",
    "You are a customer support message classifier.\n",
    "\n",
    "Analyze the customer message and classify it according to:\n",
    "- Urgency: low, medium, high, or critical\n",
    "- Sentiment: positive, neutral, or negative\n",
    "- Category: account_access, billing, technical_issue, feedback, or general_inquiry\n",
    "- Escalation needed: true or false\n",
    "\n",
    "Return your classification as a JSON object with these exact fields:\n",
    "{\n",
    "  \"urgency\": \"<urgency_level>\",\n",
    "  \"sentiment\": \"<sentiment>\",\n",
    "  \"category\": \"<category>\",\n",
    "  \"requires_escalation\": <boolean>\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "print(\"Initial Prompt Template:\")\n",
    "print(\"=\" * 50)\n",
    "print(initial_prompt)\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"\\nWhat PDO Will Optimize:\")\n",
    "print(\"‚Ä¢ Clarity of classification criteria\")\n",
    "print(\"‚Ä¢ Examples of edge cases (e.g., 'What makes something critical?')\")\n",
    "print(\"‚Ä¢ Ordering and emphasis of instructions\")\n",
    "print(\"‚Ä¢ Additional guidance for ambiguous cases\")\n",
    "print(\"\\nEvolution Process:\")\n",
    "print(\"1. PDO tests this prompt against others in duels\")\n",
    "print(\"2. Identifies where it wins/loses\")\n",
    "print(\"3. Generates improved variants\")\n",
    "print(\"4. New variants compete in subsequent rounds\")\n",
    "print(\"5. Best performers naturally emerge through competition\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running PDO Optimization\n",
    "\n",
    "### The Optimization Process\n",
    "\n",
    "Based on the paper's methodology, PDO optimization follows a structured tournament process:\n",
    "\n",
    "1. **Initialization Phase**\n",
    "   - Load base prompt\n",
    "   - Generate initial prompt variations\n",
    "   - Initialize Win matrix (all zeros)\n",
    "\n",
    "2. **Competition Phase** (main loop)\n",
    "   - Use Thompson sampling to select duel pair\n",
    "   - Run head-to-head comparison on examples\n",
    "   - LLM judge decides winners\n",
    "   - Update Win matrix\n",
    "   - Compute multi-ranker scores\n",
    "   - Periodically generate new prompts\n",
    "   - Prune worst performers\n",
    "\n",
    "3. **Selection Phase**\n",
    "   - Aggregate rankings (Copeland, Borda, Elo, TrueSkill)\n",
    "   - Select highest-ranked prompt\n",
    "   - Save optimized prompt and metadata\n",
    "\n",
    "### Running PDO with prompt-ops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Budget Considerations from the Paper\n",
    "\n",
    "The paper provides important insights on setting optimization budgets:\n",
    "\n",
    "| Budget Type | Recommended Range | Use Case |\n",
    "|------------|-------------------|----------|\n",
    "| **max_metric_calls** | 500-2000 | General optimization |\n",
    "| **max_iterations** | 20-100 | When you want fixed rounds |\n",
    "| **max_evals_per_trainval_instance** | 10-50 | Per-example budget control |\n",
    "\n",
    "**Paper's Findings**:\n",
    "- PDO discovers winners through ~50-200 rounds of duels\n",
    "- Larger budgets help with complex multi-objective tasks\n",
    "- Small training sets (10-30 examples) work best to avoid overfitting\n",
    "- Validation performance is the key metric to track\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyzing Results\n",
    "\n",
    "### Understanding the Optimized Prompts\n",
    "\n",
    "After PDO completes, you'll have access to detailed results that showcase the paper's innovations in action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Performance Evolution\n",
    "\n",
    "The paper emphasizes tracking performance over the optimization process to understand PDO's dueling dynamics:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Before and After\n",
    "\n",
    "The paper shows significant improvements across various benchmarks. Here's how to interpret your results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Key Takeaways\n",
    "\n",
    "### Summary of PDO's Innovations\n",
    "\n",
    "Based on our exploration of the PDO paper and practical implementation:\n",
    "\n",
    "1. **Dueling Bandits Framework**: PDO treats optimization as a tournament where prompts compete head-to-head, making relative comparisons more reliable than absolute scoring.\n",
    "\n",
    "2. **Thompson Sampling**: Smart exploration strategy that balances trying new prompts (exploration) with refining known winners (exploitation) through probabilistic sampling.\n",
    "\n",
    "3. **Multi-Ranker Fusion**: Combines 5 different ranking algorithms (Copeland, Borda, Win Rate, Elo, TrueSkill) with Dirichlet-weighted fusion for robust evaluation.\n",
    "\n",
    "4. **LLM-as-Judge**: Uses language models to evaluate pairwise preferences, enabling nuanced quality assessment beyond hard metrics.\n",
    "\n",
    "5. **Instruction Evolution**: Generates new prompts by combining top performers, allowing discovery of superior variants through competitive pressure.\n",
    "\n",
    "6. **Adaptive Pruning**: Removes consistently poor performers while maintaining diversity in the prompt pool.\n",
    "\n",
    "### When to Use PDO\n",
    "\n",
    "The paper suggests PDO is particularly effective for:\n",
    "\n",
    "- **High-stakes applications** where prompt quality is critical\n",
    "- **Complex tasks** with nuanced quality criteria\n",
    "- **Scenarios where absolute scoring is unreliable** (subjective tasks)\n",
    "- **Budget available for thorough exploration** (more LLM calls than simpler methods)\n",
    "\n",
    "### Paper's Key Results\n",
    "\n",
    "The paper demonstrates PDO's effectiveness:\n",
    "- **Outperforms gradient-based methods**: More thorough exploration avoids local optima\n",
    "- **Robust rankings**: Multi-ranker consensus prevents over-reliance on single metric\n",
    "- **Efficient evolution**: Thompson sampling focuses computational budget on informative duels\n",
    "- **Interpretable results**: Win matrices and Elo ratings provide clear understanding of prompt quality\n",
    "\n",
    "\n",
    "### Final Thoughts\n",
    "\n",
    "PDO represents an important advance in prompt optimization by bringing dueling bandit algorithms to LLM prompt engineering. Its competitive approach, combined with sophisticated exploration strategies and multi-ranker fusion, creates a powerful framework for discovering high-quality prompts through natural selection in a tournament setting.\n",
    "\n",
    "The key insight is that **asking \"which is better?\" is often more reliable than asking \"how good is this?\"** - PDO leverages this principle throughout its design.\n",
    "\n",
    "For more details, refer to the full paper and implementation documentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "### References\n",
    "\n",
    " 1. **PDO Paper**: \"Prompt Dueling Optimization: Tournament-Driven Prompt Discovery and Evaluation\" ([arXiv:2510.13907](https://arxiv.org/abs/2510.13907))\n",
    "\n",
    "### Quick Reference Commands\n",
    "\n",
    "```bash\n",
    "# Install prompt-ops\n",
    "pip install prompt-ops\n",
    "\n",
    "# Create a new PDO project\n",
    "prompt-ops create my-pdo-project\n",
    "\n",
    "# Run PDO optimization\n",
    "prompt-ops migrate --config config.yaml --log-level INFO\n",
    "\n",
    "# View results\n",
    "ls results/*.yaml\n",
    "```\n",
    "\n",
    "### Tips for Success\n",
    "\n",
    "1. **Start small**: Use 10-30 training examples as recommended by the paper\n",
    "2. **Design good feedback**: Specific, actionable feedback functions are crucial\n",
    "3. **Monitor validation**: Track validation performance to ensure generalization\n",
    "4. **Experiment with budgets**: Start with 500-1000 metric calls\n",
    "5. **Enable merging**: Set `use_merge: true` for best results\n",
    "\n",
    "---\n",
    "\n",
    "*This tutorial covered the theoretical foundations and practical implementation of PDO based on dueling bandits the research paper. For hands-on practice, try the facility support example or adapt PDO to your own classification or generation tasks.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sap-prompt-opt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
